"""
title: ğŸš€ Advanced Multimodal Context Manager
author: JiangNanGenius
version: 1.5.8
license: MIT
required_open_webui_version: 0.5.17
description: æ™ºèƒ½é•¿ä¸Šä¸‹æ–‡å’Œå¤šæ¨¡æ€å†…å®¹å¤„ç†å™¨ï¼Œæ”¯æŒå‘é‡åŒ–æ£€ç´¢ã€è¯­ä¹‰é‡æ’åºã€æ™ºèƒ½åˆ†ç‰‡ç­‰åŠŸèƒ½ - ä¿®å¤æ–°æ¶ˆæ¯åˆ¤æ–­å’Œå‰ç«¯æ˜¾ç¤º
"""

import json
import hashlib
import asyncio
import re
import base64
import math
import time
from typing import Optional, List, Dict, Callable, Any, Tuple, Union
from pydantic import BaseModel, Field
from enum import Enum

try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    tiktoken = None

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None

class VectorStrategy(str, Enum):
    AUTO = "auto"
    MULTIMODAL_FIRST = "multimodal_first"
    TEXT_FIRST = "text_first"
    MIXED = "mixed"
    FALLBACK = "fallback"
    VISION_TO_TEXT = "vision_to_text"

class MultimodalStrategy(str, Enum):
    ALL_MODELS = "all_models"
    NON_MULTIMODAL_ONLY = "non_multimodal_only"
    CUSTOM_LIST = "custom_list"
    SMART_ADAPTIVE = "smart_adaptive"

class ProcessingStrategy(str, Enum):
    ITERATIVE = "iterative"  # è¿­ä»£å¤„ç†ï¼ˆæ¨èï¼‰
    CHUNK_FIRST = "chunk_first"  # åˆ†ç‰‡ä¼˜å…ˆ
    SUMMARY_FIRST = "summary_first"  # æ‘˜è¦ä¼˜å…ˆ
    MIXED = "mixed"  # æ··åˆç­–ç•¥

class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    def __init__(self):
        self.original_tokens = 0
        self.original_messages = 0
        self.final_tokens = 0
        self.final_messages = 0
        self.token_limit = 0
        self.iterations = 0
        self.chunked_messages = 0
        self.summarized_messages = 0
        self.vector_retrievals = 0
        self.rerank_operations = 0
        self.multimodal_processed = 0
        self.processing_time = 0.0
        self.current_user_message_preserved = False
        
    def calculate_retention_ratio(self) -> float:
        """è®¡ç®—å†…å®¹ä¿ç•™æ¯”ä¾‹"""
        if self.original_tokens == 0:
            return 0.0
        return self.final_tokens / self.original_tokens
    
    def calculate_window_usage_ratio(self) -> float:
        """è®¡ç®—å¯¹è¯çª—å£ä½¿ç”¨ç‡"""
        if self.token_limit == 0:
            return 0.0
        return self.final_tokens / self.token_limit
    
    def calculate_compression_ratio(self) -> float:
        """è®¡ç®—å‹ç¼©æ¯”ä¾‹"""
        if self.original_tokens == 0:
            return 0.0
        return (self.original_tokens - self.final_tokens) / self.original_tokens
    
    def calculate_processing_efficiency(self) -> float:
        """è®¡ç®—å¤„ç†æ•ˆç‡ (ä¿ç•™çš„æœ‰ç”¨ä¿¡æ¯ / å¤„ç†æ—¶é—´)"""
        if self.processing_time == 0:
            return 0.0
        return self.final_tokens / self.processing_time
    
    def get_summary(self) -> str:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        retention = self.calculate_retention_ratio()
        window_usage = self.calculate_window_usage_ratio()
        compression = self.calculate_compression_ratio()
        efficiency = self.calculate_processing_efficiency()
        
        return f"""
ğŸ“Š å¤„ç†ç»Ÿè®¡æŠ¥å‘Š:
â”œâ”€ è¾“å…¥: {self.original_messages}æ¡æ¶ˆæ¯, {self.original_tokens:,}tokens
â”œâ”€ è¾“å‡º: {self.final_messages}æ¡æ¶ˆæ¯, {self.final_tokens:,}tokens
â”œâ”€ æ¨¡å‹é™åˆ¶: {self.token_limit:,}tokens
â”œâ”€ ğŸ“ˆ å†…å®¹ä¿ç•™ç‡: {retention:.2%}
â”œâ”€ ğŸªŸ çª—å£ä½¿ç”¨ç‡: {window_usage:.2%}
â”œâ”€ ğŸ“‰ å‹ç¼©æ¯”ä¾‹: {compression:.2%}
â”œâ”€ âš¡ å¤„ç†æ•ˆç‡: {efficiency:.0f}tokens/s
â”œâ”€ ğŸ”„ è¿­ä»£æ¬¡æ•°: {self.iterations}
â”œâ”€ ğŸ§© åˆ†ç‰‡æ¶ˆæ¯: {self.chunked_messages}æ¡
â”œâ”€ ğŸ“ æ‘˜è¦æ¶ˆæ¯: {self.summarized_messages}æ¡
â”œâ”€ ğŸ” å‘é‡æ£€ç´¢: {self.vector_retrievals}æ¬¡
â”œâ”€ ğŸ”„ é‡æ’åº: {self.rerank_operations}æ¬¡
â”œâ”€ ğŸ–¼ï¸ å¤šæ¨¡æ€å¤„ç†: {self.multimodal_processed}å¼ å›¾ç‰‡
â”œâ”€ ğŸ’¬ å½“å‰ç”¨æˆ·æ¶ˆæ¯: {'å·²ä¿ç•™' if self.current_user_message_preserved else 'æœªä¿ç•™'}
â””â”€ â±ï¸ å¤„ç†æ—¶é—´: {self.processing_time:.2f}ç§’"""

class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨"""
    def __init__(self, __event_emitter__):
        self.event_emitter = __event_emitter__
        self.current_step = 0
        self.total_steps = 0
        self.current_phase = ""
        self.phase_progress = 0
        self.phase_total = 0
        
    async def start_phase(self, phase_name: str, total_items: int = 0):
        """å¼€å§‹æ–°é˜¶æ®µ"""
        self.current_phase = phase_name
        self.phase_progress = 0
        self.phase_total = total_items
        await self.update_status(f"å¼€å§‹ {phase_name}")
        
    async def update_progress(self, completed: int, total: int = None, detail: str = ""):
        """æ›´æ–°è¿›åº¦"""
        if total is None:
            total = self.phase_total
        
        self.phase_progress = completed
        if total > 0:
            percentage = (completed / total) * 100
            progress_bar = "â–ˆ" * int(percentage // 10) + "â–‘" * (10 - int(percentage // 10))
            status = f"{self.current_phase} [{progress_bar}] {percentage:.1f}% ({completed}/{total})"
            if detail:
                status += f" - {detail}"
        else:
            status = f"{self.current_phase} - {detail}" if detail else self.current_phase
            
        await self.update_status(status, False)
        
    async def complete_phase(self, message: str = ""):
        """å®Œæˆå½“å‰é˜¶æ®µ"""
        final_message = f"{self.current_phase} å®Œæˆ"
        if message:
            final_message += f" - {message}"
        await self.update_status(final_message, True)
        
    async def update_status(self, message: str, done: bool = False):
        """æ›´æ–°çŠ¶æ€"""
        if self.event_emitter:
            try:
                await self.event_emitter({
                    "type": "status",
                    "data": {"description": f"ğŸ”„ {message}", "done": done},
                })
            except:
                pass

class Filter:
    class Valves(BaseModel):
        # åŸºç¡€æ§åˆ¶
        enable_processing: bool = Field(default=True, description="ğŸ”„ å¯ç”¨æ‰€æœ‰å¤„ç†åŠŸèƒ½")
        excluded_models: str = Field(
            default="", description="ğŸš« æ’é™¤æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)"
        )
        
        # æ ¸å¿ƒå¤„ç†ç­–ç•¥
        processing_strategy: str = Field(
            default="iterative", 
            description="ğŸ¯ å¤„ç†ç­–ç•¥ (iterative|chunk_first|summary_first|mixed) - æ¨èiterative"
        )
        
        # å¤šæ¨¡æ€æ¨¡å‹é…ç½®
        multimodal_models: str = Field(
            default="gpt-4o,gpt-4o-mini,gpt-4-vision-preview,doubao-1.5-vision-pro,doubao-1.5-vision-lite,claude-3,gemini-pro-vision,qwen-vl",
            description="ğŸ–¼ï¸ å¤šæ¨¡æ€æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)",
        )
        
        # æ¨¡å‹Tokené™åˆ¶é…ç½®
        model_token_limits: str = Field(
            default="gpt-4o:128000,gpt-4o-mini:128000,gpt-4:8192,gpt-3.5-turbo:16385,doubao-1.5-thinking-pro:128000,doubao-1.5-vision-pro:128000,doubao-seed:50000,doubao:50000,claude-3:200000,gemini-pro:128000,doubao-1-5-pro-256k:200000,doubao-seed-1-6-250615:50000",
            description="âš–ï¸ æ¨¡å‹Tokené™åˆ¶é…ç½®(model:limitæ ¼å¼ï¼Œé€—å·åˆ†éš”)",
        )
        
        # å¤šæ¨¡æ€å¤„ç†ç­–ç•¥
        multimodal_processing_strategy: str = Field(
            default="smart_adaptive",
            description="ğŸ–¼ï¸ å¤šæ¨¡æ€å¤„ç†ç­–ç•¥ (all_models|non_multimodal_only|custom_list|smart_adaptive)",
        )
        force_vision_processing_models: str = Field(
            default="gpt-4,gpt-3.5-turbo,doubao-1.5-thinking-pro",
            description="ğŸ” å¼ºåˆ¶è¿›è¡Œè§†è§‰å¤„ç†çš„æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)",
        )
        preserve_images_in_multimodal: bool = Field(
            default=True, description="ğŸ“¸ å¤šæ¨¡æ€æ¨¡å‹æ˜¯å¦ä¿ç•™åŸå§‹å›¾ç‰‡"
        )
        always_process_images_before_summary: bool = Field(
            default=True, description="ğŸ“ æ‘˜è¦å‰æ€»æ˜¯å…ˆå¤„ç†å›¾ç‰‡"
        )
        
        # åŠŸèƒ½å¼€å…³
        enable_multimodal: bool = Field(default=True, description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å¤„ç†")
        enable_vision_preprocessing: bool = Field(
            default=True, description="ğŸ‘ï¸ å¯ç”¨å›¾ç‰‡é¢„å¤„ç†"
        )
        enable_smart_truncation: bool = Field(
            default=True, description="âœ‚ï¸ å¯ç”¨æ™ºèƒ½æˆªæ–­"
        )
        enable_vector_retrieval: bool = Field(
            default=True, description="ğŸ” å¯ç”¨å‘é‡æ£€ç´¢"
        )
        enable_content_maximization: bool = Field(
            default=True, description="ğŸ“ˆ å¯ç”¨å†…å®¹æœ€å¤§åŒ–ä¿ç•™"
        )
        enable_intelligent_chunking: bool = Field(
            default=True, description="ğŸ§© å¯ç”¨æ™ºèƒ½åˆ†ç‰‡"
        )
        
        # ç»Ÿè®¡å’Œè°ƒè¯•
        enable_detailed_stats: bool = Field(
            default=True, description="ğŸ“Š å¯ç”¨è¯¦ç»†ç»Ÿè®¡"
        )
        enable_detailed_progress: bool = Field(
            default=True, description="ğŸ“± å¯ç”¨è¯¦ç»†è¿›åº¦æ˜¾ç¤º"
        )
        debug_level: int = Field(default=2, description="ğŸ› è°ƒè¯•çº§åˆ« 0-3")
        show_frontend_progress: bool = Field(
            default=True, description="ğŸ“± æ˜¾ç¤ºå¤„ç†è¿›åº¦"
        )
        api_error_retry_times: int = Field(
            default=2, description="ğŸ”„ APIé”™è¯¯é‡è¯•æ¬¡æ•°"
        )
        api_error_retry_delay: float = Field(
            default=1.0, description="â±ï¸ APIé”™è¯¯é‡è¯•å»¶è¿Ÿ(ç§’)"
        )
        
        # Tokenç®¡ç† - åŠ¨æ€è°ƒæ•´ç­–ç•¥
        default_token_limit: int = Field(default=100000, description="âš–ï¸ é»˜è®¤tokené™åˆ¶")
        token_safety_ratio: float = Field(
            default=0.88, description="ğŸ›¡ï¸ Tokenå®‰å…¨æ¯”ä¾‹"
        )
        target_window_usage: float = Field(
            default=0.85, description="ğŸªŸ ç›®æ ‡çª—å£ä½¿ç”¨ç‡(85%)"
        )
        min_window_usage: float = Field(
            default=0.70, description="ğŸªŸ æœ€å°çª—å£ä½¿ç”¨ç‡(70%)"
        )
        max_processing_iterations: int = Field(
            default=8, description="ğŸ”„ æœ€å¤§å¤„ç†è¿­ä»£æ¬¡æ•°"
        )
        min_reduction_threshold: int = Field(
            default=2000, description="ğŸ“‰ æœ€å°å‡å°‘é˜ˆå€¼"
        )
        
        # åŠ¨æ€å†…å®¹ä¸¢å¤±æ¯”ä¾‹ - æ ¹æ®å‹ç¼©ç‡å’Œçª—å£ä½¿ç”¨ç‡è°ƒæ•´
        base_content_loss_ratio: float = Field(
            default=0.20, description="ğŸ“‰ åŸºç¡€å†…å®¹ä¸¢å¤±æ¯”ä¾‹(20%)"
        )
        high_compression_loss_ratio: float = Field(
            default=0.08, description="ğŸ“‰ é«˜å‹ç¼©åœºæ™¯å†…å®¹ä¸¢å¤±æ¯”ä¾‹(8%)"
        )
        compression_threshold: float = Field(
            default=0.15, description="ğŸ“‰ é«˜å‹ç¼©åœºæ™¯é˜ˆå€¼(ç›®æ ‡/åŸå§‹<15%æ—¶è®¤ä¸ºæ˜¯é«˜å‹ç¼©)"
        )
        
        # ä¿æŠ¤ç­–ç•¥ - ä¿®å¤å½“å‰æ¶ˆæ¯åˆ¤æ–­
        force_preserve_current_user_message: bool = Field(
            default=True, description="ğŸ”’ å¼ºåˆ¶ä¿ç•™å½“å‰ç”¨æˆ·æ¶ˆæ¯(æœ€æ–°çš„ç”¨æˆ·è¾“å…¥)"
        )
        preserve_recent_exchanges: int = Field(
            default=4, description="ğŸ’¬ ä¿æŠ¤æœ€è¿‘å®Œæ•´å¯¹è¯è½®æ¬¡"
        )
        max_preserve_ratio: float = Field(
            default=0.3, description="ğŸ”’ ä¿æŠ¤æ¶ˆæ¯æœ€å¤§tokenæ¯”ä¾‹"
        )
        max_single_message_tokens: int = Field(
            default=20000, description="ğŸ“ å•æ¡æ¶ˆæ¯æœ€å¤§token"
        )
        
        # æ™ºèƒ½åˆ†ç‰‡é…ç½®
        chunk_target_tokens: int = Field(
            default=4000, description="ğŸ§© åˆ†ç‰‡ç›®æ ‡tokenæ•°"
        )
        chunk_overlap_tokens: int = Field(
            default=400, description="ğŸ”— åˆ†ç‰‡é‡å tokenæ•°"
        )
        chunk_min_tokens: int = Field(
            default=1000, description="ğŸ“ åˆ†ç‰‡æœ€å°tokenæ•°"
        )
        chunk_max_tokens: int = Field(
            default=8000, description="ğŸ“ åˆ†ç‰‡æœ€å¤§tokenæ•°"
        )
        preserve_paragraph_integrity: bool = Field(
            default=True, description="ğŸ“ ä¿æŒæ®µè½å®Œæ•´æ€§"
        )
        preserve_sentence_integrity: bool = Field(
            default=True, description="ğŸ“ ä¿æŒå¥å­å®Œæ•´æ€§"
        )
        preserve_code_blocks: bool = Field(
            default=True, description="ğŸ’» ä¿æŒä»£ç å—å®Œæ•´æ€§"
        )
        
        # è¿­ä»£å¤„ç†é…ç½®
        chunk_selection_ratio: float = Field(
            default=0.8, description="ğŸ§© åˆ†ç‰‡é€‰æ‹©æ¯”ä¾‹(80%)"
        )
        enable_chunk_vector_retrieval: bool = Field(
            default=True, description="ğŸ” å¯ç”¨åˆ†ç‰‡å‘é‡æ£€ç´¢"
        )
        enable_chunk_reranking: bool = Field(
            default=True, description="ğŸ”„ å¯ç”¨åˆ†ç‰‡é‡æ’åº"
        )
        enable_progressive_summarization: bool = Field(
            default=True, description="ğŸ“ å¯ç”¨æ¸è¿›å¼æ‘˜è¦"
        )
        
        # å†…å®¹ä¼˜å…ˆçº§è®¾ç½®
        high_priority_content: str = Field(
            default="ä»£ç ,é…ç½®,å‚æ•°,æ•°æ®,é”™è¯¯,è§£å†³æ–¹æ¡ˆ,æ­¥éª¤,æ–¹æ³•,æŠ€æœ¯ç»†èŠ‚,API,å‡½æ•°,ç±»,å˜é‡,é—®é¢˜,bug,ä¿®å¤,å®ç°,ç®—æ³•,æ¶æ„",
            description="ğŸ¯ é«˜ä¼˜å…ˆçº§å†…å®¹å…³é”®è¯(é€—å·åˆ†éš”)"
        )
        
        # Visioné…ç½®
        vision_api_base: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸ‘ï¸ Vision APIåœ°å€",
        )
        vision_api_key: str = Field(default="", description="ğŸ”‘ Vision APIå¯†é’¥")
        vision_model: str = Field(
            default="doubao-1.5-vision-pro-250328", description="ğŸ§  Visionæ¨¡å‹"
        )
        vision_prompt_template: str = Field(
            default="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€æ–‡å­—ã€é¢œè‰²ã€å¸ƒå±€ç­‰æ‰€æœ‰å¯è§ä¿¡æ¯ã€‚ç‰¹åˆ«æ³¨æ„ä»£ç ã€é…ç½®ã€æ•°æ®ç­‰æŠ€æœ¯ä¿¡æ¯ã€‚ä¿æŒå®¢è§‚å‡†ç¡®ï¼Œé‡ç‚¹çªå‡ºå…³é”®ä¿¡æ¯ã€‚",
            description="ğŸ‘ï¸ Visionæç¤ºè¯",
        )
        vision_max_tokens: int = Field(
            default=2000, description="ğŸ‘ï¸ Visionæœ€å¤§è¾“å‡ºtokens"
        )
        
        # å¤šæ¨¡æ€å‘é‡
        enable_multimodal_vector: bool = Field(
            default=True, description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å‘é‡"
        )
        multimodal_vector_api_base: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸ”— å¤šæ¨¡æ€å‘é‡API",
        )
        multimodal_vector_api_key: str = Field(
            default="", description="ğŸ”‘ å¤šæ¨¡æ€å‘é‡å¯†é’¥"
        )
        multimodal_vector_model: str = Field(
            default="doubao-embedding-vision-250615", description="ğŸ§  å¤šæ¨¡æ€å‘é‡æ¨¡å‹"
        )
        
        # æ–‡æœ¬å‘é‡
        enable_text_vector: bool = Field(default=True, description="ğŸ“ å¯ç”¨æ–‡æœ¬å‘é‡")
        text_vector_api_base: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸ”— æ–‡æœ¬å‘é‡API",
        )
        text_vector_api_key: str = Field(default="", description="ğŸ”‘ æ–‡æœ¬å‘é‡å¯†é’¥")
        text_vector_model: str = Field(
            default="doubao-embedding-large-text-250515", description="ğŸ§  æ–‡æœ¬å‘é‡æ¨¡å‹"
        )
        
        # å‘é‡ç­–ç•¥
        vector_strategy: str = Field(
            default="auto",
            description="ğŸ¯ å‘é‡åŒ–ç­–ç•¥ (auto|multimodal_first|text_first|mixed|fallback|vision_to_text)",
        )
        vector_similarity_threshold: float = Field(
            default=0.25, description="ğŸ¯ åŸºç¡€ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        multimodal_similarity_threshold: float = Field(
            default=0.2, description="ğŸ–¼ï¸ å¤šæ¨¡æ€ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        text_similarity_threshold: float = Field(
            default=0.3, description="ğŸ“ æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        vector_top_k: int = Field(default=50, description="ğŸ” å‘é‡æ£€ç´¢Top-Kæ•°é‡")
        
        # é‡æ’åº
        enable_reranking: bool = Field(default=True, description="ğŸ”„ å¯ç”¨é‡æ’åº")
        rerank_api_base: str = Field(
            default="https://api.bochaai.com", description="ğŸ”„ é‡æ’åºAPI"
        )
        rerank_api_key: str = Field(default="", description="ğŸ”‘ é‡æ’åºå¯†é’¥")
        rerank_model: str = Field(default="gte-rerank", description="ğŸ§  é‡æ’åºæ¨¡å‹")
        rerank_top_k: int = Field(default=40, description="ğŸ” é‡æ’åºè¿”å›æ•°é‡")
        
        # æ‘˜è¦é…ç½® - åŠ¨æ€è®¡ç®—æœ€å°é•¿åº¦
        summary_api_base: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3", description="ğŸ“ æ‘˜è¦API"
        )
        summary_api_key: str = Field(default="", description="ğŸ”‘ æ‘˜è¦å¯†é’¥")
        summary_model: str = Field(
            default="doubao-1.5-thinking-pro-250415", description="ğŸ§  æ‘˜è¦æ¨¡å‹"
        )
        max_summary_length: int = Field(
            default=10000, description="ğŸ“ æ‘˜è¦æœ€å¤§é•¿åº¦"
        )
        min_summary_ratio: float = Field(
            default=0.3, description="ğŸ“ æ‘˜è¦æœ€å°é•¿åº¦æ¯”ä¾‹(ç›¸å¯¹äºchunkå¤§å°çš„30%)"
        )
        summary_compression_ratio: float = Field(
            default=0.5, description="ğŸ“Š æ‘˜è¦å‹ç¼©æ¯”ä¾‹"
        )
        max_recursion_depth: int = Field(
            default=3, description="ğŸ”„ æœ€å¤§é€’å½’æ·±åº¦"
        )
        
        # æ€§èƒ½é…ç½®
        max_concurrent_requests: int = Field(default=3, description="âš¡ æœ€å¤§å¹¶å‘æ•°")
        request_timeout: int = Field(default=60, description="â±ï¸ è¯·æ±‚è¶…æ—¶(ç§’)")
        chunk_size: int = Field(default=1500, description="ğŸ“„ åˆ†ç‰‡å¤§å°")
        overlap_size: int = Field(default=150, description="ğŸ”— é‡å å¤§å°")

    def __init__(self):
        print("\n" + "=" * 60)
        print("ğŸš€ Advanced Multimodal Context Manager v1.5.8")
        print("ğŸ“ æ’ä»¶æ­£åœ¨åˆå§‹åŒ–...")
        print("ğŸ”§ ä¿®å¤æ–°æ¶ˆæ¯åˆ¤æ–­é€»è¾‘ï¼Œä¼˜åŒ–å‰ç«¯æ˜¾ç¤º...")
        self.valves = self.Valves()
        self._vision_client = None
        self._text_vector_client = None
        self._multimodal_vector_client = None
        self._rerank_client = None
        self._encoding = None
        self.vision_cache = {}
        self.vector_cache = {}
        self.processing_cache = {}
        self.api_error_cache = {}
        
        # å¤„ç†ç»Ÿè®¡
        self.stats = ProcessingStats()
        
        # è§£æå¤šæ¨¡æ€æ¨¡å‹é…ç½®
        self.multimodal_models = set()
        if self.valves.multimodal_models:
            self.multimodal_models = {
                model.strip().lower()
                for model in self.valves.multimodal_models.split(",")
                if model.strip()
            }
        
        # è§£ææ¨¡å‹Tokené™åˆ¶é…ç½®
        self.model_token_limits = {}
        if self.valves.model_token_limits:
            for limit_config in self.valves.model_token_limits.split(","):
                if ":" in limit_config:
                    model, limit = limit_config.split(":", 1)
                    try:
                        self.model_token_limits[model.strip().lower()] = int(
                            limit.strip()
                        )
                    except ValueError:
                        pass
        
        # è§£æé«˜ä¼˜å…ˆçº§å†…å®¹å…³é”®è¯
        self.high_priority_keywords = set()
        if self.valves.high_priority_content:
            self.high_priority_keywords = {
                keyword.strip().lower()
                for keyword in self.valves.high_priority_content.split(",")
                if keyword.strip()
            }
        
        print(f"âœ… æ’ä»¶åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ¯ å¤„ç†ç­–ç•¥: {self.valves.processing_strategy}")
        print(f"ğŸ“Š è¯¦ç»†ç»Ÿè®¡: {self.valves.enable_detailed_stats}")
        print(f"ğŸ“± è¯¦ç»†è¿›åº¦: {self.valves.enable_detailed_progress}")
        print(f"ğŸ”’ ä¿ç•™å½“å‰ç”¨æˆ·æ¶ˆæ¯: {self.valves.force_preserve_current_user_message}")
        print(f"ğŸªŸ ç›®æ ‡çª—å£ä½¿ç”¨ç‡: {self.valves.target_window_usage:.1%}")
        print("=" * 60 + "\n")

    def calculate_min_summary_length(self) -> int:
        """åŠ¨æ€è®¡ç®—æœ€å°æ‘˜è¦é•¿åº¦"""
        return max(500, int(self.valves.chunk_target_tokens * self.valves.min_summary_ratio))

    def debug_log(self, level: int, message: str, emoji: str = "ğŸ”§"):
        if self.valves.debug_level >= level:
            prefix = ["", "ğŸ›[DEBUG]", "ğŸ”[DETAIL]", "ğŸ“‹[VERBOSE]"][min(level, 3)]
            print(f"{prefix} {emoji} {message}")

    def is_model_excluded(self, model_name: str) -> bool:
        if not self.valves.excluded_models or not model_name:
            return False
        excluded_list = [
            model.strip().lower()
            for model in self.valves.excluded_models.split(",")
            if model.strip()
        ]
        if not excluded_list:
            return False
        model_lower = model_name.lower()
        for excluded_model in excluded_list:
            if excluded_model in model_lower:
                self.debug_log(1, f"æ¨¡å‹ {model_name} åœ¨æ’é™¤åˆ—è¡¨ä¸­", "ğŸš«")
                return True
        return False

    def get_encoding(self):
        if not TIKTOKEN_AVAILABLE:
            return None
        if self._encoding is None:
            try:
                self._encoding = tiktoken.get_encoding("cl100k_base")
                self.debug_log(3, "tiktokenç¼–ç å™¨å·²åˆå§‹åŒ–", "ğŸ”§")
            except Exception as e:
                self.debug_log(1, f"tiktokenåˆå§‹åŒ–å¤±è´¥: {e}", "âš ï¸")
        return self._encoding

    def count_tokens(self, text: str) -> int:
        if not text:
            return 0
        text = str(text)
        encoding = self.get_encoding()
        if encoding:
            try:
                return len(encoding.encode(text))
            except Exception as e:
                self.debug_log(2, f"tokenè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ä¼°ç®—: {e}", "âš ï¸")
        return max(len(text) // 3, len(text.encode("utf-8")) // 4)

    def count_message_tokens(self, message: dict) -> int:
        if not message:
            return 0
        content = message.get("content", "")
        role = message.get("role", "")
        total_tokens = 0
        if isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    total_tokens += self.count_tokens(item.get("text", ""))
                elif item.get("type") == "image_url":
                    total_tokens += 1500  # å›¾ç‰‡tokenä¼°ç®—
        else:
            total_tokens = self.count_tokens(str(content))
        # è§’è‰²å’Œæ ¼å¼å¼€é”€
        total_tokens += self.count_tokens(role) + 10
        return total_tokens

    def count_messages_tokens(self, messages: List[dict]) -> int:
        if not messages:
            return 0
        return sum(self.count_message_tokens(msg) for msg in messages)

    def get_model_token_limit(self, model_name: str) -> int:
        model_lower = model_name.lower()
        # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„é™åˆ¶
        for model_key, limit in self.model_token_limits.items():
            if model_key in model_lower:
                safe_limit = int(limit * self.valves.token_safety_ratio)
                self.debug_log(
                    2, f"æ¨¡å‹ {model_name} é™åˆ¶: {limit} -> {safe_limit}", "âš–ï¸"
                )
                return safe_limit
        # ä½¿ç”¨é»˜è®¤é™åˆ¶
        safe_limit = int(
            self.valves.default_token_limit * self.valves.token_safety_ratio
        )
        self.debug_log(1, f"æœªçŸ¥æ¨¡å‹ {model_name}, ä½¿ç”¨é»˜è®¤é™åˆ¶: {safe_limit}", "âš ï¸")
        return safe_limit

    def find_current_user_message(self, messages: List[dict]) -> Optional[dict]:
        """æŸ¥æ‰¾å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€æ–°çš„ç”¨æˆ·è¾“å…¥ï¼‰"""
        if not messages:
            return None
        
        # ä»æœ€åä¸€æ¡æ¶ˆæ¯å¼€å§‹æŸ¥æ‰¾ï¼Œæ‰¾åˆ°æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        # è¿™åº”è¯¥æ˜¯ç”¨æˆ·åˆšåˆšå‘é€çš„æ¶ˆæ¯
        for msg in reversed(messages):
            if msg.get("role") == "user":
                self.debug_log(2, f"æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯: {len(self.extract_text_from_content(msg.get('content', '')))}å­—ç¬¦", "ğŸ’¬")
                return msg
        
        return None

    def separate_current_and_history_messages(self, messages: List[dict]) -> Tuple[Optional[dict], List[dict]]:
        """åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯"""
        if not messages:
            return None, []
        
        # æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€æ–°çš„ç”¨æˆ·è¾“å…¥ï¼‰
        current_user_message = self.find_current_user_message(messages)
        
        if not current_user_message:
            return None, messages
        
        # åˆ†ç¦»å†å²æ¶ˆæ¯ï¼ˆé™¤äº†å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯ï¼‰
        history_messages = []
        current_found = False
        
        for msg in reversed(messages):
            if msg is current_user_message and not current_found:
                # è·³è¿‡å½“å‰ç”¨æˆ·æ¶ˆæ¯
                current_found = True
                continue
            else:
                history_messages.insert(0, msg)
        
        self.debug_log(
            1, 
            f"æ¶ˆæ¯åˆ†ç¦»: å½“å‰ç”¨æˆ·æ¶ˆæ¯1æ¡({self.count_message_tokens(current_user_message)}tokens), å†å²æ¶ˆæ¯{len(history_messages)}æ¡", 
            "ğŸ“‹"
        )
        
        return current_user_message, history_messages

    def calculate_dynamic_loss_ratio(self, original_tokens: int, target_tokens: int) -> float:
        """åŠ¨æ€è®¡ç®—å†…å®¹ä¸¢å¤±æ¯”ä¾‹é˜ˆå€¼"""
        if original_tokens <= 0:
            return self.valves.base_content_loss_ratio
        
        # è®¡ç®—å‹ç¼©ç‡
        compression_ratio = target_tokens / original_tokens
        
        # å¦‚æœæ˜¯é«˜å‹ç¼©åœºæ™¯
        if compression_ratio < self.valves.compression_threshold:
            max_loss_ratio = self.valves.high_compression_loss_ratio
            self.debug_log(
                2, 
                f"ğŸ“‰ é«˜å‹ç¼©åœºæ™¯(å‹ç¼©ç‡{compression_ratio:.2%})ï¼Œå…è®¸æœ€å¤§ä¸¢å¤±{max_loss_ratio:.1%}", 
                "ğŸ“‰"
            )
        else:
            max_loss_ratio = self.valves.base_content_loss_ratio
            self.debug_log(
                2, 
                f"ğŸ“‰ æ­£å¸¸å‹ç¼©åœºæ™¯(å‹ç¼©ç‡{compression_ratio:.2%})ï¼Œå…è®¸æœ€å¤§ä¸¢å¤±{max_loss_ratio:.1%}", 
                "ğŸ“‰"
            )
        
        return max_loss_ratio

    def should_continue_processing(self, current_tokens: int, target_tokens: int, original_tokens: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­å¤„ç†"""
        # 1. é¦–è¦æ¡ä»¶ï¼šæ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if current_tokens <= target_tokens:
            return False
        
        # 2. è®¡ç®—å½“å‰çª—å£ä½¿ç”¨ç‡
        current_usage = current_tokens / target_tokens if target_tokens > 0 else 0
        
        # 3. å¦‚æœä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¿…é¡»ç»§ç»­å¤„ç†
        if current_usage > 1.0:
            return True
        
        # 4. æ£€æŸ¥å†…å®¹ä¿ç•™ç‡
        retention_ratio = current_tokens / original_tokens if original_tokens > 0 else 0
        max_loss_ratio = self.calculate_dynamic_loss_ratio(original_tokens, target_tokens)
        
        # 5. å¦‚æœå†…å®¹ä¸¢å¤±è¿‡å¤šï¼Œè°¨æ…å¤„ç†
        if retention_ratio < (1 - max_loss_ratio):
            self.debug_log(
                2,
                f"ğŸ“Š å†…å®¹ä¿ç•™ç‡({retention_ratio:.2%}) < é˜ˆå€¼({1-max_loss_ratio:.2%})ï¼Œè°¨æ…å¤„ç†",
                "âš ï¸"
            )
            # ä½†å¦‚æœçª—å£ä½¿ç”¨ç‡ä»ç„¶è¿‡é«˜ï¼Œè¿˜æ˜¯è¦ç»§ç»­
            if current_usage > 1.1:  # è¶…è¿‡110%å¿…é¡»å¤„ç†
                return True
            else:
                return False
        
        return True

    def is_multimodal_model(self, model_name: str) -> bool:
        model_lower = model_name.lower()
        return any(mm in model_lower for mm in self.multimodal_models)

    def should_process_images_for_model(self, model_name: str) -> bool:
        if not self.valves.enable_multimodal:
            return False
        model_lower = model_name.lower()
        # æ£€æŸ¥å¼ºåˆ¶å¤„ç†åˆ—è¡¨
        force_list = [
            m.strip().lower()
            for m in self.valves.force_vision_processing_models.split(",")
            if m.strip()
        ]
        if any(force_model in model_lower for force_model in force_list):
            self.debug_log(2, f"æ¨¡å‹ {model_name} åœ¨å¼ºåˆ¶å¤„ç†åˆ—è¡¨ä¸­", "ğŸ”")
            return True
        # æ ¹æ®ç­–ç•¥åˆ¤æ–­
        is_multimodal = self.is_multimodal_model(model_name)
        strategy = self.valves.multimodal_processing_strategy.lower()
        if strategy == "all_models":
            return True
        elif strategy == "non_multimodal_only":
            return not is_multimodal
        elif strategy == "smart_adaptive":
            return True
        else:
            return not is_multimodal

    def has_images_in_content(self, content) -> bool:
        if isinstance(content, list):
            return any(item.get("type") == "image_url" for item in content)
        return False

    def has_images_in_messages(self, messages: List[dict]) -> bool:
        return any(self.has_images_in_content(msg.get("content")) for msg in messages)

    # ========== æ™ºèƒ½åˆ†ç‰‡åŠŸèƒ½ ==========
    def find_code_blocks(self, text: str) -> List[Tuple[int, int]]:
        """æŸ¥æ‰¾ä»£ç å—è¾¹ç•Œ"""
        code_blocks = []
        # æŸ¥æ‰¾```ä»£ç å—
        pattern = r'```[\s\S]*?```'
        for match in re.finditer(pattern, text):
            code_blocks.append((match.start(), match.end()))
        
        # æŸ¥æ‰¾`ä»£ç `
        pattern = r'`[^`\n]+`'
        for match in re.finditer(pattern, text):
            code_blocks.append((match.start(), match.end()))
        
        return code_blocks

    def find_sentence_boundaries(self, text: str) -> List[int]:
        """æŸ¥æ‰¾å¥å­è¾¹ç•Œ"""
        boundaries = []
        # ä¸­æ–‡å¥å­ç»“æŸæ ‡ç‚¹
        chinese_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'â€¦']
        # è‹±æ–‡å¥å­ç»“æŸæ ‡ç‚¹
        english_endings = ['.', '!', '?', ';']
        
        for i, char in enumerate(text):
            if char in chinese_endings or char in english_endings:
                # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„å¥å­ç»“æŸï¼ˆé¿å…å°æ•°ç‚¹ã€ç¼©å†™ç­‰ï¼‰
                if i + 1 < len(text):
                    next_char = text[i + 1]
                    if next_char in [' ', '\n', '\t'] or next_char.isupper():
                        boundaries.append(i + 1)
                else:
                    boundaries.append(i + 1)
        return boundaries

    def find_paragraph_boundaries(self, text: str) -> List[int]:
        """æŸ¥æ‰¾æ®µè½è¾¹ç•Œ"""
        boundaries = []
        lines = text.split('\n')
        current_pos = 0
        
        for line in lines:
            current_pos += len(line) + 1  # +1 for '\n'
            if line.strip() == '':  # ç©ºè¡Œè¡¨ç¤ºæ®µè½ç»“æŸ
                boundaries.append(current_pos)
        
        return boundaries

    def is_high_priority_content(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜ä¼˜å…ˆçº§å†…å®¹"""
        if not text or not self.high_priority_keywords:
            return False
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.high_priority_keywords)

    def intelligent_chunk_text_v3(self, text: str, target_tokens: int) -> List[str]:
        """æ™ºèƒ½åˆ†ç‰‡æ–‡æœ¬ v3 - ä¸ä¸¢å¼ƒå†…å®¹"""
        if not text:
            return []
        
        current_tokens = self.count_tokens(text)
        if current_tokens <= target_tokens:
            return [text]
        
        self.debug_log(2, f"ğŸ§© å¼€å§‹æ™ºèƒ½åˆ†ç‰‡: {current_tokens}tokens -> ç›®æ ‡{target_tokens}tokens", "ğŸ§©")
        
        chunks = []
        
        # 1. ä¿æŠ¤ä»£ç å—
        if self.valves.preserve_code_blocks:
            code_blocks = self.find_code_blocks(text)
            if code_blocks:
                # æŒ‰ä»£ç å—åˆ†å‰²
                last_end = 0
                for start, end in code_blocks:
                    # æ·»åŠ ä»£ç å—å‰çš„å†…å®¹
                    if start > last_end:
                        before_text = text[last_end:start].strip()
                        if before_text:
                            chunks.extend(self.chunk_text_by_paragraphs(before_text, target_tokens))
                    
                    # æ·»åŠ ä»£ç å—ï¼ˆä¿æŒå®Œæ•´ï¼‰
                    code_text = text[start:end]
                    code_tokens = self.count_tokens(code_text)
                    if code_tokens <= self.valves.chunk_max_tokens:
                        chunks.append(code_text)
                    else:
                        # ä»£ç å—å¤ªå¤§ï¼Œè°¨æ…åˆ†å‰²
                        chunks.extend(self.chunk_large_code_block(code_text, target_tokens))
                    
                    last_end = end
                
                # æ·»åŠ æœ€åçš„å†…å®¹
                if last_end < len(text):
                    after_text = text[last_end:].strip()
                    if after_text:
                        chunks.extend(self.chunk_text_by_paragraphs(after_text, target_tokens))
                
                self.debug_log(2, f"ğŸ§© ä»£ç å—åˆ†ç‰‡å®Œæˆ: {len(chunks)}ç‰‡", "ğŸ§©")
                return chunks
        
        # 2. æŒ‰æ®µè½åˆ†ç‰‡
        if self.valves.preserve_paragraph_integrity:
            chunks = self.chunk_text_by_paragraphs(text, target_tokens)
        # 3. æŒ‰å¥å­åˆ†ç‰‡
        elif self.valves.preserve_sentence_integrity:
            chunks = self.chunk_text_by_sentences(text, target_tokens)
        # 4. ç®€å•åˆ†ç‰‡
        else:
            chunks = self.simple_chunk_text(text, target_tokens)
        
        self.debug_log(2, f"ğŸ§© æ™ºèƒ½åˆ†ç‰‡å®Œæˆ: {current_tokens}tokens -> {len(chunks)}ç‰‡", "ğŸ§©")
        return chunks

    def chunk_text_by_paragraphs(self, text: str, target_tokens: int) -> List[str]:
        """æŒ‰æ®µè½åˆ†ç‰‡"""
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            para_tokens = self.count_tokens(paragraph)
            current_chunk_tokens = self.count_tokens(current_chunk)
            
            # å•ä¸ªæ®µè½å°±è¶…è¿‡ç›®æ ‡
            if para_tokens > target_tokens:
                # ä¿å­˜å½“å‰chunk
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # åˆ†å‰²é•¿æ®µè½
                if para_tokens > self.valves.chunk_max_tokens:
                    chunks.extend(self.chunk_long_paragraph(paragraph, target_tokens))
                else:
                    chunks.append(paragraph)
            
            # åŠ å…¥è¿™ä¸ªæ®µè½ä¼šè¶…è¿‡ç›®æ ‡
            elif current_chunk_tokens + para_tokens > target_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
        
        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

    def chunk_text_by_sentences(self, text: str, target_tokens: int) -> List[str]:
        """æŒ‰å¥å­åˆ†ç‰‡"""
        sentences = []
        sentence_boundaries = self.find_sentence_boundaries(text)
        
        start = 0
        for boundary in sentence_boundaries:
            sentence = text[start:boundary].strip()
            if sentence:
                sentences.append(sentence)
            start = boundary
        
        # å‰©ä½™éƒ¨åˆ†
        if start < len(text):
            remaining = text[start:].strip()
            if remaining:
                sentences.append(remaining)
        
        # ç»„åˆå¥å­æˆchunk
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence_tokens = self.count_tokens(sentence)
            current_chunk_tokens = self.count_tokens(current_chunk)
            
            if sentence_tokens > target_tokens:
                # å•ä¸ªå¥å­å¤ªé•¿ï¼Œå¼ºåˆ¶åˆ†å‰²
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                chunks.extend(self.simple_chunk_text(sentence, target_tokens))
            elif current_chunk_tokens + sentence_tokens > target_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

    def chunk_long_paragraph(self, paragraph: str, target_tokens: int) -> List[str]:
        """åˆ†å‰²é•¿æ®µè½"""
        if self.valves.preserve_sentence_integrity:
            return self.chunk_text_by_sentences(paragraph, target_tokens)
        else:
            return self.simple_chunk_text(paragraph, target_tokens)

    def chunk_large_code_block(self, code_text: str, target_tokens: int) -> List[str]:
        """åˆ†å‰²å¤§ä»£ç å—"""
        lines = code_text.split('\n')
        chunks = []
        current_chunk = ""
        
        for line in lines:
            test_chunk = current_chunk + "\n" + line if current_chunk else line
            if self.count_tokens(test_chunk) > target_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = line
            else:
                current_chunk = test_chunk
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

    def simple_chunk_text(self, text: str, target_tokens: int) -> List[str]:
        """ç®€å•æŒ‰é•¿åº¦åˆ†å‰²æ–‡æœ¬"""
        chunks = []
        words = text.split()
        current_chunk = ""
        
        for word in words:
            test_chunk = current_chunk + " " + word if current_chunk else word
            if self.count_tokens(test_chunk) > target_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = word
            else:
                current_chunk = test_chunk
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

    def chunk_message_content_v3(self, message: dict, target_tokens: int) -> List[dict]:
        """å¢å¼ºçš„æ¶ˆæ¯å†…å®¹åˆ†ç‰‡ v3 - ä¸ä¸¢å¼ƒå†…å®¹"""
        content = self.extract_text_from_content(message.get("content", ""))
        if not content:
            return [message]
        
        current_tokens = self.count_tokens(content)
        if current_tokens <= target_tokens:
            return [message]
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé«˜ä¼˜å…ˆçº§å†…å®¹
        is_high_priority = self.is_high_priority_content(content)
        
        # ä½¿ç”¨æ™ºèƒ½åˆ†ç‰‡
        chunks = self.intelligent_chunk_text_v3(content, target_tokens)
        if not chunks:
            return [message]
        
        # åˆ›å»ºåˆ†ç‰‡æ¶ˆæ¯
        chunked_messages = []
        for i, chunk in enumerate(chunks):
            chunked_message = message.copy()
            
            # æ·»åŠ åˆ†ç‰‡æ ‡è¯†å’Œä¼˜å…ˆçº§æ ‡è¯†
            priority_mark = "ğŸ¯[é«˜ä¼˜å…ˆçº§]" if is_high_priority else ""
            chunked_message["content"] = f"{priority_mark}[åˆ†ç‰‡{i+1}/{len(chunks)}] {chunk}"
            chunked_messages.append(chunked_message)
        
        self.debug_log(2, f"ğŸ§© æ¶ˆæ¯åˆ†ç‰‡: {current_tokens}tokens -> {len(chunks)}ç‰‡ {'(é«˜ä¼˜å…ˆçº§)' if is_high_priority else ''}", "ğŸ§©")
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.chunked_messages += 1
        
        return chunked_messages

    # ========== å¢å¼ºçš„APIè°ƒç”¨æ–¹æ³• ==========
    def is_json_response(self, content: str) -> bool:
        """æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºJSONæ ¼å¼"""
        if not content:
            return False
        content = content.strip()
        return content.startswith('{') or content.startswith('[')

    def extract_error_info(self, content: str) -> str:
        """ä»é”™è¯¯å“åº”ä¸­æå–å…³é”®ä¿¡æ¯"""
        if not content:
            return "ç©ºå“åº”"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºHTMLé”™è¯¯é¡µé¢
        if content.strip().startswith('<!DOCTYPE') or '<html' in content:
            # å°è¯•æå–title
            title_match = re.search(r'<title>(.*?)</title>', content, re.IGNORECASE)
            if title_match:
                return f"HTMLé”™è¯¯é¡µé¢: {title_match.group(1)}"
            return "HTMLé”™è¯¯é¡µé¢"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºJSONé”™è¯¯
        try:
            if self.is_json_response(content):
                error_data = json.loads(content)
                if isinstance(error_data, dict):
                    error_msg = error_data.get('error', error_data.get('message', ''))
                    if error_msg:
                        return f"APIé”™è¯¯: {error_msg}"
            return f"å“åº”å†…å®¹: {content[:200]}..."
        except:
            return f"å“åº”å†…å®¹: {content[:200]}..."

    async def safe_api_call(self, call_func, call_name: str, *args, **kwargs):
        """å®‰å…¨çš„APIè°ƒç”¨åŒ…è£…å™¨"""
        error_key = f"{call_name}_{hash(str(args) + str(kwargs))}"
        
        # æ£€æŸ¥é”™è¯¯ç¼“å­˜
        if error_key in self.api_error_cache:
            cache_time, error_msg = self.api_error_cache[error_key]
            if time.time() - cache_time < 300:  # 5åˆ†é’Ÿç¼“å­˜
                self.debug_log(1, f"ä½¿ç”¨ç¼“å­˜çš„é”™è¯¯ç»“æœ: {error_msg}", "âš ï¸")
                return None
        
        for attempt in range(self.valves.api_error_retry_times + 1):
            try:
                result = await call_func(*args, **kwargs)
                # æ¸…é™¤é”™è¯¯ç¼“å­˜
                if error_key in self.api_error_cache:
                    del self.api_error_cache[error_key]
                return result
            except Exception as e:
                error_msg = str(e)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºJSONè§£æé”™è¯¯
                if "is not valid JSON" in error_msg or "Unexpected token" in error_msg:
                    self.debug_log(1, f"{call_name} JSONè§£æé”™è¯¯: {error_msg}", "âŒ")
                    # è®°å½•åˆ°é”™è¯¯ç¼“å­˜
                    self.api_error_cache[error_key] = (time.time(), error_msg)
                    return None
                
                if attempt < self.valves.api_error_retry_times:
                    self.debug_log(
                        1, f"{call_name} ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{self.valves.api_error_retry_delay}ç§’åé‡è¯•: {error_msg}", "ğŸ”„"
                    )
                    await asyncio.sleep(self.valves.api_error_retry_delay)
                else:
                    self.debug_log(1, f"{call_name} æœ€ç»ˆå¤±è´¥: {error_msg}", "âŒ")
                    # è®°å½•åˆ°é”™è¯¯ç¼“å­˜
                    self.api_error_cache[error_key] = (time.time(), error_msg)
                    return None
        
        return None

    # ========== å‘é‡åŒ–åŠŸèƒ½ ==========
    def get_text_vector_client(self):
        if not OPENAI_AVAILABLE:
            return None
        
        if self._text_vector_client:
            return self._text_vector_client
        
        api_key = self.valves.text_vector_api_key
        if not api_key:
            api_key = (
                self.valves.multimodal_vector_api_key or self.valves.vision_api_key
            )
        
        if api_key:
            self._text_vector_client = AsyncOpenAI(
                base_url=self.valves.text_vector_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout,
            )
            self.debug_log(2, "æ–‡æœ¬å‘é‡å®¢æˆ·ç«¯å·²åˆ›å»º", "ğŸ“")
        
        return self._text_vector_client

    def get_multimodal_vector_client(self):
        if not OPENAI_AVAILABLE:
            return None
        
        if self._multimodal_vector_client:
            return self._multimodal_vector_client
        
        api_key = self.valves.multimodal_vector_api_key
        if not api_key:
            api_key = self.valves.text_vector_api_key or self.valves.vision_api_key
        
        if api_key:
            self._multimodal_vector_client = AsyncOpenAI(
                base_url=self.valves.multimodal_vector_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout,
            )
            self.debug_log(2, "å¤šæ¨¡æ€å‘é‡å®¢æˆ·ç«¯å·²åˆ›å»º", "ğŸ–¼ï¸")
        
        return self._multimodal_vector_client

    async def _get_text_embedding_impl(self, text: str, __event_emitter__):
        """å®é™…çš„æ–‡æœ¬å‘é‡è·å–å®ç°"""
        client = self.get_text_vector_client()
        if not client:
            return None
        
        response = await client.embeddings.create(
            model=self.valves.text_vector_model,
            input=[text[:8000]],
            encoding_format="float",
        )
        
        if response.data:
            return response.data[0].embedding
        return None

    async def get_text_embedding(
        self, text: str, __event_emitter__
    ) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬å‘é‡"""
        if not text or not self.valves.enable_text_vector:
            return None
        
        text_hash = hashlib.md5(text.encode()).hexdigest()
        cache_key = f"text_emb_{text_hash}"
        
        if cache_key in self.vector_cache:
            return self.vector_cache[cache_key]
        
        embedding = await self.safe_api_call(
            self._get_text_embedding_impl, "æ–‡æœ¬å‘é‡", text, __event_emitter__
        )
        
        if embedding:
            self.vector_cache[cache_key] = embedding
            self.debug_log(3, f"æ–‡æœ¬å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ“")
        
        return embedding

    async def _get_multimodal_embedding_impl(self, content, __event_emitter__):
        """å®é™…çš„å¤šæ¨¡æ€å‘é‡è·å–å®ç°"""
        client = self.get_multimodal_vector_client()
        if not client:
            return None
        
        # å¤„ç†è¾“å…¥æ ¼å¼
        if isinstance(content, list):
            input_data = content
        else:
            input_data = [{"type": "text", "text": str(content)[:8000]}]
        
        response = await client.embeddings.create(
            model=self.valves.multimodal_vector_model, input=input_data
        )
        
        if response.data:
            return response.data[0].embedding
        return None

    async def get_multimodal_embedding(
        self, content, __event_emitter__
    ) -> Optional[List[float]]:
        """è·å–å¤šæ¨¡æ€å‘é‡"""
        if not content or not self.valves.enable_multimodal_vector:
            return None
        
        # ç”Ÿæˆç¼“å­˜key
        if isinstance(content, list):
            content_str = json.dumps(content, sort_keys=True)
        else:
            content_str = str(content)
        
        content_hash = hashlib.md5(content_str.encode()).hexdigest()
        cache_key = f"multimodal_emb_{content_hash}"
        
        if cache_key in self.vector_cache:
            return self.vector_cache[cache_key]
        
        embedding = await self.safe_api_call(
            self._get_multimodal_embedding_impl, "å¤šæ¨¡æ€å‘é‡", content, __event_emitter__
        )
        
        if embedding:
            self.vector_cache[cache_key] = embedding
            self.debug_log(3, f"å¤šæ¨¡æ€å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ–¼ï¸")
        
        return embedding

    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)

    async def vector_retrieve_relevant_messages(
        self, query_message: dict, candidate_messages: List[dict], progress: ProgressTracker
    ) -> List[dict]:
        """åŸºäºå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³æ¶ˆæ¯"""
        if not candidate_messages or not self.valves.enable_vector_retrieval:
            return candidate_messages
        
        await progress.start_phase("å‘é‡æ£€ç´¢", len(candidate_messages))
        
        self.debug_log(
            1, f"å¼€å§‹å‘é‡æ£€ç´¢: æŸ¥è¯¢1æ¡ï¼Œå€™é€‰{len(candidate_messages)}æ¡", "ğŸ”"
        )
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.vector_retrievals += 1
        
        # è·å–æŸ¥è¯¢å‘é‡
        query_content = query_message.get("content", "")
        query_vector = None
        strategy = self.valves.vector_strategy.lower()
        
        await progress.update_progress(0, len(candidate_messages), "è·å–æŸ¥è¯¢å‘é‡")
        
        # æ ¹æ®ç­–ç•¥é€‰æ‹©å‘é‡åŒ–æ–¹æ³•
        if self.has_images_in_content(query_content):
            if strategy in ["auto", "multimodal_first"]:
                query_vector = await self.get_multimodal_embedding(
                    query_content, progress.event_emitter
                )
            if not query_vector and strategy in ["auto", "fallback"]:
                # è½¬æ¢ä¸ºæ–‡æœ¬å†å‘é‡åŒ–
                text_content = self.extract_text_from_content(query_content)
                if text_content:
                    query_vector = await self.get_text_embedding(
                        text_content, progress.event_emitter
                    )
        else:
            text_content = self.extract_text_from_content(query_content)
            if text_content:
                query_vector = await self.get_text_embedding(
                    text_content, progress.event_emitter
                )
        
        if not query_vector:
            self.debug_log(1, "æŸ¥è¯¢å‘é‡è·å–å¤±è´¥ï¼Œè¿”å›åŸå§‹æ¶ˆæ¯", "âš ï¸")
            await progress.complete_phase("æŸ¥è¯¢å‘é‡è·å–å¤±è´¥")
            return candidate_messages
        
        # è®¡ç®—å€™é€‰æ¶ˆæ¯çš„ç›¸ä¼¼åº¦
        similarities = []
        for i, msg in enumerate(candidate_messages):
            await progress.update_progress(i + 1, len(candidate_messages), f"è®¡ç®—ç›¸ä¼¼åº¦ {i+1}/{len(candidate_messages)}")
            
            msg_content = msg.get("content", "")
            msg_vector = None
            
            # ä¸ºå€™é€‰æ¶ˆæ¯è·å–å‘é‡
            if self.has_images_in_content(msg_content):
                msg_vector = await self.get_multimodal_embedding(
                    msg_content, progress.event_emitter
                )
                if not msg_vector:
                    text_content = self.extract_text_from_content(msg_content)
                    if text_content:
                        msg_vector = await self.get_text_embedding(
                            text_content, progress.event_emitter
                        )
            else:
                text_content = self.extract_text_from_content(msg_content)
                if text_content:
                    msg_vector = await self.get_text_embedding(
                        text_content, progress.event_emitter
                    )
            
            if msg_vector:
                similarity = self.cosine_similarity(query_vector, msg_vector)
                
                # é«˜ä¼˜å…ˆçº§å†…å®¹ç»™äºˆåŠ æƒ
                if self.is_high_priority_content(self.extract_text_from_content(msg_content)):
                    similarity = min(1.0, similarity * 1.3)  # æé«˜æƒé‡
                
                similarities.append((i, similarity, msg))
                self.debug_log(3, f"æ¶ˆæ¯{i}ç›¸ä¼¼åº¦: {similarity:.3f}", "ğŸ“Š")
            else:
                # æ²¡æœ‰å‘é‡çš„æ¶ˆæ¯ç»™åŸºç¡€åˆ†æ•°
                similarities.append((i, 0.3, msg))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼è¿‡æ»¤
        threshold = self.valves.vector_similarity_threshold
        filtered_similarities = [item for item in similarities if item[1] >= threshold]
        
        # å¦‚æœè¿‡æ»¤åå¤ªå°‘ï¼Œé™ä½é˜ˆå€¼
        if len(filtered_similarities) < len(candidate_messages) * 0.3:
            lower_threshold = max(0.1, threshold - 0.1)
            filtered_similarities = [item for item in similarities if item[1] >= lower_threshold]
            self.debug_log(2, f"é™ä½é˜ˆå€¼åˆ°{lower_threshold:.2f}ï¼Œä¿ç•™æ›´å¤šæ¶ˆæ¯", "ğŸ”")
        
        # é™åˆ¶æ•°é‡ä½†ä¿ç•™æ›´å¤š
        top_similarities = filtered_similarities[: self.valves.vector_top_k]
        
        # æå–æ¶ˆæ¯å¹¶ä¿æŒåŸå§‹é¡ºåº
        relevant_messages = []
        selected_indices = sorted([item[0] for item in top_similarities])
        for idx in selected_indices:
            relevant_messages.append(candidate_messages[idx])
        
        self.debug_log(
            1,
            f"å‘é‡æ£€ç´¢å®Œæˆ: {len(candidate_messages)} -> {len(relevant_messages)}æ¡",
            "âœ…",
        )
        
        await progress.complete_phase(f"æ£€ç´¢åˆ°{len(relevant_messages)}æ¡ç›¸å…³æ¶ˆæ¯")
        
        return relevant_messages

    def extract_text_from_content(self, content) -> str:
        """ä»å†…å®¹ä¸­æå–æ–‡æœ¬"""
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
            return " ".join(text_parts)
        else:
            return str(content)

    # ========== é‡æ’åºåŠŸèƒ½ ==========
    async def _rerank_messages_impl(self, query_text: str, documents: List[str], __event_emitter__):
        """å®é™…çš„é‡æ’åºå®ç°"""
        if not HTTPX_AVAILABLE:
            return None
        
        async with httpx.AsyncClient(timeout=self.valves.request_timeout) as client:
            headers = {
                "Authorization": f"Bearer {self.valves.rerank_api_key}",
                "Content-Type": "application/json",
            }
            
            data = {
                "model": self.valves.rerank_model,
                "query": query_text,
                "documents": documents,
                "top_n": min(self.valves.rerank_top_k, len(documents)),
                "return_documents": True,
            }
            
            response = await client.post(
                f"{self.valves.rerank_api_base}/v1/rerank",
                headers=headers,
                json=data,
            )
            
            if response.status_code != 200:
                raise Exception(f"HTTP {response.status_code}: {response.text}")
            
            # æ£€æŸ¥å“åº”æ ¼å¼
            response_text = response.text
            if not self.is_json_response(response_text):
                error_info = self.extract_error_info(response_text)
                raise Exception(f"éJSONå“åº”: {error_info}")
            
            result = response.json()
            if result.get("code") != 200:
                error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                raise Exception(f"APIé”™è¯¯: {error_msg}")
            
            return result.get("data", {}).get("results", [])

    async def rerank_messages(
        self, query_message: dict, candidate_messages: List[dict], progress: ProgressTracker
    ) -> List[dict]:
        """é‡æ’åºæ¶ˆæ¯"""
        if not candidate_messages or not self.valves.enable_reranking:
            return candidate_messages
        
        await progress.start_phase("é‡æ’åº", len(candidate_messages))
        
        self.debug_log(1, f"å¼€å§‹é‡æ’åº: æŸ¥è¯¢1æ¡ï¼Œå€™é€‰{len(candidate_messages)}æ¡", "ğŸ”„")
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.rerank_operations += 1
        
        # å‡†å¤‡æŸ¥è¯¢æ–‡æœ¬
        query_text = self.extract_text_from_content(query_message.get("content", ""))
        if not query_text:
            await progress.complete_phase("æŸ¥è¯¢æ–‡æœ¬ä¸ºç©º")
            return candidate_messages
        
        await progress.update_progress(0, 1, "å‡†å¤‡æ–‡æ¡£åˆ—è¡¨")
        
        # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
        documents = []
        for msg in candidate_messages:
            text = self.extract_text_from_content(msg.get("content", ""))
            if text:
                # æé«˜æ–‡æ¡£é•¿åº¦é™åˆ¶
                if len(text) > 5000:
                    text = text[:5000] + "..."
                documents.append(text)
            else:
                documents.append("ç©ºæ¶ˆæ¯")
        
        if not documents:
            await progress.complete_phase("æ— æœ‰æ•ˆæ–‡æ¡£")
            return candidate_messages
        
        await progress.update_progress(1, 1, "è°ƒç”¨é‡æ’åºAPI")
        
        # è°ƒç”¨é‡æ’åºAPI
        rerank_results = await self.safe_api_call(
            self._rerank_messages_impl, "é‡æ’åº", query_text, documents, progress.event_emitter
        )
        
        if rerank_results:
            # æŒ‰é‡æ’åºç»“æœé‡æ–°æ’åˆ—æ¶ˆæ¯
            reranked_messages = []
            for item in rerank_results:
                original_index = item.get("index", 0)
                if 0 <= original_index < len(candidate_messages):
                    reranked_messages.append(candidate_messages[original_index])
                    score = item.get("relevance_score", 0)
                    self.debug_log(
                        3,
                        f"é‡æ’åºç»“æœ: index={original_index}, score={score:.3f}",
                        "ğŸ“Š",
                    )
            
            self.debug_log(
                1,
                f"é‡æ’åºå®Œæˆ: {len(candidate_messages)} -> {len(reranked_messages)}æ¡",
                "âœ…",
            )
            
            await progress.complete_phase(f"é‡æ’åºåˆ°{len(reranked_messages)}æ¡æ¶ˆæ¯")
            
            return reranked_messages
        
        await progress.complete_phase("é‡æ’åºå¤±è´¥")
        return candidate_messages

    # ========== Visionå¤„ç† ==========
    def get_vision_client(self):
        if not OPENAI_AVAILABLE:
            return None
        
        if self._vision_client:
            return self._vision_client
        
        api_key = self.valves.vision_api_key
        if not api_key:
            api_key = (
                self.valves.multimodal_vector_api_key or self.valves.text_vector_api_key
            )
        
        if api_key:
            self._vision_client = AsyncOpenAI(
                base_url=self.valves.vision_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout,
            )
            self.debug_log(2, "Visionå®¢æˆ·ç«¯å·²åˆ›å»º", "ğŸ‘ï¸")
        
        return self._vision_client

    async def _describe_image_impl(self, image_url: str, __event_emitter__):
        """å®é™…çš„å›¾ç‰‡æè¿°å®ç°"""
        client = self.get_vision_client()
        if not client:
            return None
        
        response = await client.chat.completions.create(
            model=self.valves.vision_model,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": self.valves.vision_prompt_template,
                        },
                        {"type": "image_url", "image_url": {"url": image_url}},
                    ],
                }
            ],
            max_tokens=self.valves.vision_max_tokens,
            temperature=0.2,
        )
        
        if response.choices:
            return response.choices[0].message.content.strip()
        return None

    async def describe_image(self, image_url: str, __event_emitter__) -> str:
        """æè¿°å•å¼ å›¾ç‰‡"""
        image_hash = hashlib.md5(image_url.encode()).hexdigest()
        
        if image_hash in self.vision_cache:
            self.debug_log(3, f"ä½¿ç”¨ç¼“å­˜çš„å›¾ç‰‡æè¿°: {image_hash[:8]}", "ğŸ“‹")
            return self.vision_cache[image_hash]
        
        self.debug_log(2, f"å¼€å§‹è¯†åˆ«å›¾ç‰‡: {image_hash[:8]}", "ğŸ‘ï¸")
        
        description = await self.safe_api_call(
            self._describe_image_impl, "å›¾ç‰‡è¯†åˆ«", image_url, __event_emitter__
        )
        
        if description:
            # æé«˜æè¿°é•¿åº¦é™åˆ¶
            if len(description) > 2000:
                description = description[:2000] + "..."
            
            self.vision_cache[image_hash] = description
            self.debug_log(2, f"å›¾ç‰‡è¯†åˆ«å®Œæˆ: {len(description)}å­—ç¬¦", "âœ…")
            return description
        
        return "å›¾ç‰‡å¤„ç†å¤±è´¥ï¼šæ— æ³•è·å–æè¿°"

    async def process_message_images(self, message: dict, progress: ProgressTracker) -> dict:
        """å¤„ç†å•æ¡æ¶ˆæ¯ä¸­çš„å›¾ç‰‡"""
        content = message.get("content", "")
        if not isinstance(content, list):
            return message
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        images = [item for item in content if item.get("type") == "image_url"]
        if not images:
            return message
        
        # å¤„ç†å›¾ç‰‡
        processed_content = []
        image_count = 0
        
        for item in content:
            if item.get("type") == "text":
                processed_content.append(item.get("text", ""))
            elif item.get("type") == "image_url":
                image_count += 1
                image_url = item.get("image_url", {}).get("url", "")
                if image_url:
                    if progress:
                        await progress.update_progress(image_count, len(images), f"å¤„ç†å›¾ç‰‡ {image_count}/{len(images)}")
                    
                    description = await self.describe_image(
                        image_url, progress.event_emitter if progress else None
                    )
                    processed_content.append(f"[å›¾ç‰‡{image_count}æè¿°] {description}")
        
        # åˆ›å»ºæ–°æ¶ˆæ¯
        processed_message = message.copy()
        processed_message["content"] = (
            "\n".join(processed_content) if processed_content else ""
        )
        
        self.debug_log(2, f"æ¶ˆæ¯å›¾ç‰‡å¤„ç†å®Œæˆ: {image_count}å¼ å›¾ç‰‡", "ğŸ–¼ï¸")
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.multimodal_processed += image_count
        
        return processed_message

    async def process_multimodal_content(
        self, messages: List[dict], model_name: str, progress: ProgressTracker
    ) -> List[dict]:
        """å¤„ç†å¤šæ¨¡æ€å†…å®¹"""
        if not self.valves.enable_multimodal:
            return messages
        
        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return messages
        
        should_process = self.should_process_images_for_model(model_name)
        is_multimodal = self.is_multimodal_model(model_name)
        
        self.debug_log(
            1,
            f"å¤šæ¨¡æ€å¤„ç†æ£€æŸ¥: æ¨¡å‹={model_name}, å¤šæ¨¡æ€={is_multimodal}, éœ€è¦å¤„ç†={should_process}",
            "ğŸ–¼ï¸",
        )
        
        if (
            is_multimodal
            and self.valves.preserve_images_in_multimodal
            and not should_process
        ):
            self.debug_log(2, f"å¤šæ¨¡æ€æ¨¡å‹ {model_name} ä¿ç•™åŸå§‹å›¾ç‰‡", "ğŸ“¸")
            return messages
        
        # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
        total_images = 0
        for msg in messages:
            if isinstance(msg.get("content"), list):
                total_images += len(
                    [
                        item
                        for item in msg.get("content", [])
                        if item.get("type") == "image_url"
                    ]
                )
        
        if total_images == 0:
            return messages
        
        await progress.start_phase("å¤šæ¨¡æ€å¤„ç†", total_images)
        
        self.debug_log(1, f"å¼€å§‹å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼š{total_images} å¼ å›¾ç‰‡", "ğŸ–¼ï¸")
        
        # å¤„ç†æ‰€æœ‰æ¶ˆæ¯
        processed_messages = []
        processed_count = 0
        
        for i, message in enumerate(messages):
            if self.has_images_in_content(message.get("content")):
                image_count = len([item for item in message.get("content", []) if item.get("type") == "image_url"])
                
                # åˆ›å»ºå­è¿›åº¦è¿½è¸ª
                class SubProgress:
                    def __init__(self, parent, base_count):
                        self.parent = parent
                        self.base_count = base_count
                        self.event_emitter = parent.event_emitter
                    
                    async def update_progress(self, current, total, detail):
                        await self.parent.update_progress(
                            self.base_count + current, 
                            self.parent.phase_total, 
                            detail
                        )
                
                sub_progress = SubProgress(progress, processed_count)
                
                processed_message = await self.process_message_images(
                    message, sub_progress
                )
                processed_messages.append(processed_message)
                processed_count += image_count
            else:
                processed_messages.append(message)
        
        self.debug_log(1, f"å¤šæ¨¡æ€å¤„ç†å®Œæˆï¼š{processed_count} å¼ å›¾ç‰‡", "âœ…")
        
        await progress.complete_phase(f"å¤„ç†å®Œæˆ {processed_count} å¼ å›¾ç‰‡")
        
        return processed_messages

    # ========== æ‘˜è¦åŠŸèƒ½ ==========
    def get_summary_client(self):
        """è·å–æ‘˜è¦å®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE:
            return None
        
        api_key = self.valves.summary_api_key
        if not api_key:
            api_key = (
                self.valves.multimodal_vector_api_key
                or self.valves.text_vector_api_key
                or self.valves.vision_api_key
            )
        
        if api_key:
            return AsyncOpenAI(
                base_url=self.valves.summary_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout,
            )
        return None

    async def _summarize_messages_impl(self, conversation_text: str, original_tokens: int, iteration: int):
        """å®é™…çš„æ‘˜è¦å®ç°"""
        client = self.get_summary_client()
        if not client:
            return None
        
        # åŠ¨æ€è®¡ç®—æœŸæœ›çš„æ‘˜è¦é•¿åº¦
        min_summary_length = self.calculate_min_summary_length()
        target_summary_tokens = max(
            min_summary_length,
            int(original_tokens * self.valves.summary_compression_ratio)
        )
        
        # æ‘˜è¦æç¤º
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¯¹è¯æ‘˜è¦åŠ©æ‰‹ã€‚è¯·ä¸ºä»¥ä¸‹å¯¹è¯åˆ›å»º**è¯¦ç»†å®Œæ•´**çš„ç»“æ„åŒ–æ‘˜è¦ã€‚

âš ï¸ é‡è¦è¦æ±‚ï¼š
1. æ‘˜è¦å¿…é¡»è¾¾åˆ° {target_summary_tokens} tokensä»¥ä¸Šï¼ˆæœ€å°{min_summary_length}tokensï¼‰
2. ä¿æŒå¯¹è¯çš„å®Œæ•´é€»è¾‘è„‰ç»œå’Œæ—¶é—´é¡ºåº
3. å®Œæ•´ä¿ç•™æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼šæŠ€æœ¯ç»†èŠ‚ã€å‚æ•°é…ç½®ã€æ•°æ®ã€ä»£ç ç‰‡æ®µã€é”™è¯¯ä¿¡æ¯ã€è§£å†³æ–¹æ¡ˆã€æ“ä½œæ­¥éª¤
4. ä½¿ç”¨æ¸…æ™°çš„ç»“æ„åŒ–æ ¼å¼
5. ä¿ç•™ç”¨æˆ·çš„å…·ä½“é—®é¢˜å’ŒåŠ©æ‰‹çš„è¯¦ç»†å›ç­”
6. å¦‚æœå†…å®¹å¾ˆé‡è¦ï¼Œå¿…é¡»ä¿ç•™ï¼Œå¯ä»¥è¶…å‡ºé•¿åº¦é™åˆ¶

åŸå§‹tokenæ•°ï¼š{original_tokens}
æœŸæœ›æ‘˜è¦tokenæ•°ï¼š{target_summary_tokens}+
æœ€å°æ‘˜è¦tokenæ•°ï¼š{min_summary_length}
è¿­ä»£æ¬¡æ•°ï¼š{iteration+1}

å¯¹è¯å†…å®¹ï¼š"""
        
        response = await client.chat.completions.create(
            model=self.valves.summary_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": conversation_text},
            ],
            max_tokens=self.valves.max_summary_length,
            temperature=0.05,
            timeout=self.valves.request_timeout,
        )
        
        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            summary_tokens = self.count_tokens(summary)
            
            self.debug_log(2, f"æ‘˜è¦ç”Ÿæˆ: {summary_tokens}tokens (æœŸæœ›{target_summary_tokens}+, æœ€å°{min_summary_length})", "ğŸ“")
            
            # åŠ¨æ€æ‘˜è¦è´¨é‡åˆ¤æ–­
            if summary_tokens >= min_summary_length:
                return summary
            else:
                self.debug_log(1, f"æ‘˜è¦è¿‡çŸ­({summary_tokens}tokens < {min_summary_length})ï¼Œä½†ä»ç„¶ä½¿ç”¨", "ğŸ“")
                return summary  # å³ä½¿çŸ­ä¹Ÿä½¿ç”¨ï¼Œè€Œä¸æ˜¯ä¸¢å¼ƒ
        
        return None

    async def summarize_messages_batch(
        self, messages: List[dict], progress: ProgressTracker, iteration: int = 0
    ) -> str:
        """æ‰¹é‡æ‘˜è¦æ¶ˆæ¯"""
        if not messages:
            return ""
        
        await progress.start_phase("æ‘˜è¦ç”Ÿæˆ", len(messages))
        
        # è®¡ç®—åŸå§‹tokenæ•°
        original_tokens = self.count_messages_tokens(messages)
        
        await progress.update_progress(0, 3, "é¢„å¤„ç†æ¶ˆæ¯")
        
        # å…ˆå¤„ç†å›¾ç‰‡
        processed_messages = messages
        if self.valves.always_process_images_before_summary:
            has_images = any(
                self.has_images_in_content(msg.get("content")) for msg in messages
            )
            if has_images:
                self.debug_log(2, f"æ‘˜è¦å‰å¤„ç†å›¾ç‰‡: {len(messages)}æ¡æ¶ˆæ¯", "ğŸ–¼ï¸")
                processed_messages = []
                for msg in messages:
                    if self.has_images_in_content(msg.get("content")):
                        processed_msg = await self.process_message_images(
                            msg, None  # ä¸éœ€è¦è¯¦ç»†è¿›åº¦
                        )
                        processed_messages.append(processed_msg)
                    else:
                        processed_messages.append(msg)
        
        await progress.update_progress(1, 3, "æ ¼å¼åŒ–å¯¹è¯")
        
        # æŒ‰è§’è‰²åˆ†ç»„å¤„ç†
        conversation_parts = []
        current_exchange = []
        
        for msg in processed_messages:
            role = msg.get("role", "unknown")
            content = self.extract_text_from_content(msg.get("content", ""))
            
            # å¢åŠ å†…å®¹é•¿åº¦é™åˆ¶
            if len(content) > 12000:
                content = content[:12000] + "...(é•¿å†…å®¹å·²æˆªæ–­)"
            
            if role == "user":
                if current_exchange:
                    conversation_parts.append(self.format_exchange(current_exchange))
                    current_exchange = []
                current_exchange.append(f"ğŸ‘¤ ç”¨æˆ·: {content}")
            elif role == "assistant":
                current_exchange.append(f"ğŸ¤– åŠ©æ‰‹: {content}")
            else:
                current_exchange.append(f"[{role}]: {content}")
        
        if current_exchange:
            conversation_parts.append(self.format_exchange(current_exchange))
        
        conversation_text = "\n\n".join(conversation_parts)
        
        await progress.update_progress(2, 3, "è°ƒç”¨æ‘˜è¦API")
        
        # è°ƒç”¨æ‘˜è¦API
        summary = await self.safe_api_call(
            self._summarize_messages_impl, "æ‘˜è¦ç”Ÿæˆ", conversation_text, original_tokens, iteration
        )
        
        if summary:
            self.debug_log(1, f"ğŸ“ æ‘˜è¦æˆåŠŸ: {len(summary)}å­—ç¬¦", "ğŸ“")
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats.summarized_messages += 1
            
            await progress.complete_phase(f"æ‘˜è¦ç”ŸæˆæˆåŠŸ {len(summary)}å­—ç¬¦")
            return summary
        
        self.debug_log(1, f"ğŸ“ æ‘˜è¦å¤±è´¥", "âš ï¸")
        await progress.complete_phase("æ‘˜è¦ç”Ÿæˆå¤±è´¥")
        return ""

    def format_exchange(self, exchange: List[str]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯è½®æ¬¡"""
        return "\n".join(exchange)

    # ========== æ ¸å¿ƒå¤„ç†ç­–ç•¥ - è¿­ä»£å¤„ç† ==========
    def smart_message_selection_v7(
        self, messages: List[dict], current_user_message: Optional[dict], target_tokens: int, iteration: int = 0
    ) -> Tuple[List[dict], List[dict]]:
        """
        æ™ºèƒ½æ¶ˆæ¯é€‰æ‹©ç­–ç•¥ v7 - ä¿®å¤å½“å‰æ¶ˆæ¯åˆ¤æ–­
        """
        if not messages:
            return [], []
        
        # åˆ†ç¦»ä¸åŒç±»å‹çš„æ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        assistant_messages = [msg for msg in messages if msg.get("role") == "assistant"]
        
        protected = []
        current_tokens = 0
        
        # 1. å¼ºåˆ¶ä¿ç•™å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€æ–°çš„ç”¨æˆ·è¾“å…¥ï¼‰
        if self.valves.force_preserve_current_user_message and current_user_message:
            protected.append(current_user_message)
            current_tokens += self.count_message_tokens(current_user_message)
            self.debug_log(2, f"ğŸ”’ å¼ºåˆ¶ä¿ç•™å½“å‰ç”¨æˆ·æ¶ˆæ¯: {current_tokens}tokens", "ğŸ’¬")
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats.current_user_message_preserved = True
        
        # 2. ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
        for msg in system_messages:
            msg_tokens = self.count_message_tokens(msg)
            if current_tokens + msg_tokens <= target_tokens:
                protected.append(msg)
                current_tokens += msg_tokens
        
        # 3. æ ¹æ®è¿­ä»£æ¬¡æ•°è°ƒæ•´ä¿æŠ¤ç­–ç•¥
        preserve_exchanges = max(2, self.valves.preserve_recent_exchanges - iteration)
        max_preserve_ratio = max(0.2, self.valves.max_preserve_ratio - iteration * 0.05)
        max_preserve_tokens = int(target_tokens * max_preserve_ratio)
        
        self.debug_log(
            2,
            f"ğŸ”„ ç¬¬{iteration+1}æ¬¡è¿­ä»£: ä¿æŠ¤{preserve_exchanges}è½®å¯¹è¯, æœ€å¤§{max_preserve_tokens}tokens",
            "ğŸ“Š",
        )
        
        # 4. ä¿æŠ¤æœ€è¿‘çš„å¯¹è¯è½®æ¬¡ï¼ˆæ’é™¤å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼‰
        remaining_messages = [msg for msg in messages if msg not in protected]
        
        # æŒ‰æ—¶é—´é¡ºåºæ‰¾åˆ°æœ€è¿‘çš„å¯¹è¯è½®æ¬¡
        exchanges_protected = 0
        i = len(remaining_messages) - 1
        
        while (
            i >= 0
            and exchanges_protected < preserve_exchanges
            and current_tokens < max_preserve_tokens
        ):
            msg = remaining_messages[i]
            msg_tokens = self.count_message_tokens(msg)
            
            if current_tokens + msg_tokens <= max_preserve_tokens:
                if msg.get("role") == "assistant" and i > 0:
                    # å°è¯•ä¿æŠ¤å®Œæ•´çš„å¯¹è¯è½®æ¬¡
                    prev_msg = remaining_messages[i - 1]
                    if prev_msg.get("role") == "user":
                        prev_tokens = self.count_message_tokens(prev_msg)
                        if (
                            current_tokens + msg_tokens + prev_tokens
                            <= max_preserve_tokens
                        ):
                            protected.insert(-1, prev_msg)
                            protected.insert(-1, msg)
                            current_tokens += msg_tokens + prev_tokens
                            exchanges_protected += 1
                            i -= 2
                            continue
                
                # å•ç‹¬ä¿æŠ¤è¿™æ¡æ¶ˆæ¯
                protected.insert(-1, msg)
                current_tokens += msg_tokens
            
            i -= 1
        
        # 5. ç¡®å®šéœ€è¦å¤„ç†çš„æ¶ˆæ¯
        to_process = [msg for msg in messages if msg not in protected]
        
        self.debug_log(
            1,
            f"ğŸ“‹ ç¬¬{iteration+1}æ¬¡é€‰æ‹©: ä¿æŠ¤{len(protected)}æ¡({current_tokens}tokens), å¤„ç†{len(to_process)}æ¡",
            "ğŸ“",
        )
        
        return protected, to_process

    async def iterative_content_processing_v4(
        self, messages: List[dict], target_tokens: int, progress: ProgressTracker
    ) -> List[dict]:
        """è¿­ä»£å†…å®¹å¤„ç†çš„æ ¸å¿ƒé€»è¾‘ v4 - ä¿®å¤æ¶ˆæ¯åˆ¤æ–­å’Œè¿›åº¦æ˜¾ç¤º"""
        start_time = time.time()
        
        # åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯
        current_user_message, history_messages = self.separate_current_and_history_messages(messages)
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        self.stats.original_tokens = self.count_messages_tokens(messages)
        self.stats.original_messages = len(messages)
        self.stats.token_limit = target_tokens
        
        current_tokens = self.stats.original_tokens
        
        if current_tokens <= target_tokens:
            self.stats.final_tokens = current_tokens
            self.stats.final_messages = len(messages)
            self.stats.processing_time = time.time() - start_time
            return messages
        
        # åŠ¨æ€è®¡ç®—å†…å®¹ä¸¢å¤±æ¯”ä¾‹é˜ˆå€¼
        max_content_loss_ratio = self.calculate_dynamic_loss_ratio(self.stats.original_tokens, target_tokens)
        
        await progress.start_phase("è¿­ä»£å¤„ç†", self.valves.max_processing_iterations)
        
        self.debug_log(
            1,
            f"ğŸ”„ å¼€å§‹è¿­ä»£å¤„ç†: {current_tokens:,} -> {target_tokens:,} tokens (å‹ç¼©ç‡{target_tokens/current_tokens:.2%})",
            "ğŸ”„",
        )
        
        iteration = 0
        processed_messages = messages
        
        while iteration < self.valves.max_processing_iterations:
            await progress.update_progress(iteration + 1, self.valves.max_processing_iterations, f"ç¬¬{iteration+1}è½®è¿­ä»£")
            
            current_tokens = self.count_messages_tokens(processed_messages)
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­å¤„ç†
            if not self.should_continue_processing(current_tokens, target_tokens, self.stats.original_tokens):
                self.debug_log(
                    1,
                    f"âœ… å¤„ç†å®Œæˆ: è¿­ä»£{iteration+1}æ¬¡, æœ€ç»ˆ{current_tokens:,}tokens",
                    "âœ…",
                )
                break
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats.iterations = iteration + 1
            
            # 1. æ™ºèƒ½é€‰æ‹©æ¶ˆæ¯ï¼ˆä½¿ç”¨ä¿®å¤åçš„æ–¹æ³•ï¼‰
            protected_messages, to_process = self.smart_message_selection_v7(
                processed_messages, current_user_message, target_tokens, iteration
            )
            
            if not to_process:
                self.debug_log(1, f"âš ï¸ æ²¡æœ‰å¯å¤„ç†çš„æ¶ˆæ¯ï¼Œä½†tokenä»è¶…é™ï¼Œå¼ºåˆ¶æˆªæ–­", "âš ï¸")
                # å¼ºåˆ¶æˆªæ–­åˆ°ç›®æ ‡å¤§å°
                final_messages = self.emergency_truncate_to_target(
                    protected_messages, target_tokens
                )
                break
            
            # 2. å‘é‡æ£€ç´¢ç›¸å…³æ¶ˆæ¯
            if self.valves.enable_vector_retrieval and len(to_process) > 3:
                # ä½¿ç”¨å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºæŸ¥è¯¢ï¼ˆè€Œä¸æ˜¯å†å²æ¶ˆæ¯ï¼‰
                if current_user_message:
                    self.debug_log(2, f"ğŸ” ä½¿ç”¨å½“å‰ç”¨æˆ·æ¶ˆæ¯è¿›è¡Œå‘é‡æ£€ç´¢", "ğŸ”")
                    relevant_messages = await self.vector_retrieve_relevant_messages(
                        current_user_message, to_process, progress
                    )
                    
                    # 3. é‡æ’åº
                    if self.valves.enable_reranking and len(relevant_messages) > 3:
                        self.debug_log(
                            2, f"ğŸ”„ å¯¹{len(relevant_messages)}æ¡æ¶ˆæ¯è¿›è¡Œé‡æ’åº", "ğŸ”„"
                        )
                        relevant_messages = await self.rerank_messages(
                            current_user_message, relevant_messages, progress
                        )
                    
                    to_process = relevant_messages
            
            # 4. å¤„ç†æ¶ˆæ¯
            new_messages = protected_messages.copy()
            
            # è®¡ç®—å‰©ä½™tokenç©ºé—´
            remaining_tokens = target_tokens - self.count_messages_tokens(protected_messages)
            
            if to_process and remaining_tokens > 1000:
                # åˆ†ææ¶ˆæ¯ç±»å‹
                large_messages = []
                normal_messages = []
                
                for msg in to_process:
                    if self.count_message_tokens(msg) > self.valves.max_single_message_tokens:
                        large_messages.append(msg)
                    else:
                        normal_messages.append(msg)
                
                # å¤„ç†å¤§æ¶ˆæ¯ - åˆ†ç‰‡
                if large_messages:
                    sub_progress = ProgressTracker(progress.event_emitter)
                    await sub_progress.start_phase(f"åˆ†ç‰‡å¤„ç†", len(large_messages))
                    
                    self.debug_log(2, f"ğŸ§© åˆ†ç‰‡å¤„ç†{len(large_messages)}æ¡å¤§æ¶ˆæ¯", "ğŸ§©")
                    
                    for j, large_msg in enumerate(large_messages):
                        await sub_progress.update_progress(j + 1, len(large_messages), f"åˆ†ç‰‡æ¶ˆæ¯ {j+1}/{len(large_messages)}")
                        
                        # åˆ†ç‰‡å¤„ç†
                        chunked_messages = self.chunk_message_content_v3(
                            large_msg, self.valves.chunk_target_tokens
                        )
                        
                        # å¯¹åˆ†ç‰‡è¿›è¡Œå‘é‡æ£€ç´¢å’Œé‡æ’åº
                        if (
                            self.valves.enable_chunk_vector_retrieval 
                            and len(chunked_messages) > 5
                            and current_user_message
                        ):
                            self.debug_log(2, f"ğŸ” å¯¹{len(chunked_messages)}ä¸ªåˆ†ç‰‡è¿›è¡Œå‘é‡æ£€ç´¢", "ğŸ”")
                            relevant_chunks = await self.vector_retrieve_relevant_messages(
                                current_user_message, chunked_messages, sub_progress
                            )
                            
                            if (
                                self.valves.enable_chunk_reranking 
                                and len(relevant_chunks) > 3
                            ):
                                self.debug_log(2, f"ğŸ”„ å¯¹{len(relevant_chunks)}ä¸ªåˆ†ç‰‡è¿›è¡Œé‡æ’åº", "ğŸ”„")
                                relevant_chunks = await self.rerank_messages(
                                    current_user_message, relevant_chunks, sub_progress
                                )
                            
                            chunked_messages = relevant_chunks
                        
                        # é€‰æ‹©æœ€ç›¸å…³çš„åˆ†ç‰‡
                        max_chunks = max(
                            3, 
                            int(len(chunked_messages) * self.valves.chunk_selection_ratio)
                        )
                        selected_chunks = chunked_messages[:max_chunks]
                        
                        self.debug_log(
                            2,
                            f"ğŸ§© åˆ†ç‰‡é€‰æ‹©: {len(chunked_messages)} -> {len(selected_chunks)}ç‰‡",
                            "ğŸ§©"
                        )
                        
                        new_messages.extend(selected_chunks)
                    
                    await sub_progress.complete_phase(f"åˆ†ç‰‡å¤„ç†å®Œæˆ")
                
                # å¤„ç†æ™®é€šæ¶ˆæ¯ - æ‘˜è¦
                if normal_messages:
                    self.debug_log(2, f"ğŸ“ æ‘˜è¦å¤„ç†{len(normal_messages)}æ¡æ™®é€šæ¶ˆæ¯", "ğŸ“")
                    
                    # æ‰¹é‡æ‘˜è¦
                    sub_progress = ProgressTracker(progress.event_emitter)
                    summary_text = await self.summarize_messages_batch(
                        normal_messages, sub_progress, iteration
                    )
                    
                    if summary_text:
                        summary_message = {
                            "role": "system",
                            "content": f"=== ğŸ“‹ æ™ºèƒ½æ‘˜è¦ (ç¬¬{iteration+1}è½®) ===\n{summary_text}\n{'='*60}",
                        }
                        new_messages.append(summary_message)
                        self.debug_log(2, f"ğŸ“ æ‘˜è¦æˆåŠŸ: {len(summary_text)}å­—ç¬¦", "ğŸ“")
                    else:
                        # æ‘˜è¦å¤±è´¥ï¼Œä½¿ç”¨åˆ†ç‰‡
                        self.debug_log(2, f"ğŸ“ æ‘˜è¦å¤±è´¥ï¼Œä½¿ç”¨åˆ†ç‰‡å¤„ç†", "ğŸ§©")
                        
                        # å¯¹æ™®é€šæ¶ˆæ¯è¿›è¡Œåˆ†ç‰‡å¤„ç†
                        for normal_msg in normal_messages:
                            chunked_messages = self.chunk_message_content_v3(
                                normal_msg, self.valves.chunk_target_tokens
                            )
                            
                            # é€‰æ‹©éƒ¨åˆ†åˆ†ç‰‡
                            max_chunks = max(2, len(chunked_messages) // 2)
                            selected_chunks = chunked_messages[:max_chunks]
                            new_messages.extend(selected_chunks)
            
            processed_messages = new_messages
            iteration += 1
            
            # æ£€æŸ¥è¿›åº¦
            new_tokens = self.count_messages_tokens(processed_messages)
            reduction = current_tokens - new_tokens
            
            # è®¡ç®—å½“å‰çª—å£ä½¿ç”¨ç‡
            current_usage = new_tokens / target_tokens if target_tokens > 0 else 0
            
            self.debug_log(
                1,
                f"ğŸ“Š ç¬¬{iteration}è½®: {current_tokens:,} -> {new_tokens:,} tokens (å‡å°‘{reduction:,}, çª—å£ä½¿ç”¨{current_usage:.1%})",
                "ğŸ“Š",
            )
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¶³å¤Ÿçš„å‡å°‘ï¼ˆä½†ä¸èƒ½å› æ­¤åœæ­¢ï¼‰
            if reduction < self.valves.min_reduction_threshold:
                self.debug_log(1, f"âš ï¸ å‡å°‘å¹…åº¦è¿‡å°({reduction:,}tokens)ï¼Œä½†ç»§ç»­å¤„ç†", "ğŸ“")
                # ç»§ç»­å¤„ç†ï¼Œä¸åœæ­¢
        
        # æœ€åæ£€æŸ¥ï¼šå¦‚æœä»ç„¶è¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶æˆªæ–­
        final_tokens = self.count_messages_tokens(processed_messages)
        if final_tokens > target_tokens:
            self.debug_log(1, f"âš ï¸ ä»è¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶æˆªæ–­: {final_tokens:,} -> {target_tokens:,}", "âš ï¸")
            processed_messages = self.emergency_truncate_to_target(
                processed_messages, target_tokens
            )
            final_tokens = self.count_messages_tokens(processed_messages)
        
        # æ›´æ–°æœ€ç»ˆç»Ÿè®¡
        self.stats.final_tokens = final_tokens
        self.stats.final_messages = len(processed_messages)
        self.stats.processing_time = time.time() - start_time
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        retention_ratio = self.stats.calculate_retention_ratio()
        window_usage = self.stats.calculate_window_usage_ratio()
        
        await progress.complete_phase(f"å¤„ç†å®Œæˆ ä¿ç•™{retention_ratio:.1%} çª—å£ä½¿ç”¨{window_usage:.1%}")
        
        return processed_messages

    def emergency_truncate_to_target(
        self, messages: List[dict], target_tokens: int
    ) -> List[dict]:
        """ç´§æ€¥æˆªæ–­åˆ°ç›®æ ‡å¤§å°"""
        if not messages:
            return []
        
        self.debug_log(1, f"ğŸ†˜ ç´§æ€¥æˆªæ–­åˆ°ç›®æ ‡å¤§å°: {target_tokens:,}tokens", "ğŸ†˜")
        
        # æŒ‰é‡è¦æ€§æ’åº
        scored_messages = []
        for msg in messages:
            content = self.extract_text_from_content(msg.get("content", ""))
            score = 0
            
            # ç”¨æˆ·æ¶ˆæ¯ä¼˜å…ˆ
            if msg.get("role") == "user":
                score += 1000
            
            # ç³»ç»Ÿæ¶ˆæ¯å…¶æ¬¡
            if msg.get("role") == "system":
                score += 500
            
            # é«˜ä¼˜å…ˆçº§å†…å®¹
            if self.is_high_priority_content(content):
                score += 300
            
            # å½“å‰ç”¨æˆ·æ¶ˆæ¯æœ€é‡è¦
            current_user_message = self.find_current_user_message(messages)
            if current_user_message and msg is current_user_message:
                score += 2000
            
            scored_messages.append((score, msg))
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_messages.sort(key=lambda x: x[0], reverse=True)
        
        # é€‰æ‹©æ¶ˆæ¯ç›´åˆ°è¾¾åˆ°tokené™åˆ¶
        selected_messages = []
        current_tokens = 0
        
        for score, msg in scored_messages:
            msg_tokens = self.count_message_tokens(msg)
            if current_tokens + msg_tokens <= target_tokens:
                selected_messages.append(msg)
                current_tokens += msg_tokens
            else:
                # å¦‚æœæ˜¯é‡è¦æ¶ˆæ¯ï¼Œå°è¯•æˆªæ–­
                if score > 1000:  # é‡è¦æ¶ˆæ¯
                    remaining_tokens = target_tokens - current_tokens
                    if remaining_tokens > 500:  # è‡³å°‘500tokensæ‰æˆªæ–­
                        content = self.extract_text_from_content(msg.get("content", ""))
                        if content:
                            # æˆªæ–­å†…å®¹
                            truncated_content = content[:remaining_tokens*3] + "...(æˆªæ–­)"
                            truncated_msg = msg.copy()
                            truncated_msg["content"] = truncated_content
                            selected_messages.append(truncated_msg)
                            current_tokens += self.count_message_tokens(truncated_msg)
                break
        
        # ä¿æŒåŸå§‹é¡ºåº
        original_order = {}
        for i, msg in enumerate(messages):
            original_order[id(msg)] = i
        
        selected_messages.sort(key=lambda x: original_order.get(id(x), 0))
        
        self.debug_log(
            1,
            f"ğŸ†˜ ç´§æ€¥æˆªæ–­å®Œæˆ: {len(messages)} -> {len(selected_messages)}æ¡ ({current_tokens:,}tokens)",
            "ğŸ†˜"
        )
        
        return selected_messages

    def print_detailed_stats(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.valves.enable_detailed_stats:
            return
        
        print("\n" + "="*60)
        print(self.stats.get_summary())
        print("="*60)

    async def inlet(
        self,
        body: dict,
        user: Optional[dict] = None,
        __event_emitter__: Optional[Callable] = None,
    ) -> dict:
        """å…¥å£å‡½æ•° - å¤„ç†è¯·æ±‚"""
        print("\nğŸš€ ===== INLET CALLED =====")
        print(f"ğŸ“¨ æ”¶åˆ°è¯·æ±‚: {list(body.keys())}")
        
        if not self.valves.enable_processing:
            print("âŒ å¤„ç†åŠŸèƒ½å·²ç¦ç”¨")
            return body
        
        messages = body.get("messages", [])
        if not messages:
            print("âŒ æ— æ¶ˆæ¯å†…å®¹")
            return body
        
        model_name = body.get("model", "æœªçŸ¥")
        print(f"ğŸ“‹ æ¨¡å‹: {model_name}, æ¶ˆæ¯æ•°: {len(messages)}")
        
        if self.is_model_excluded(model_name):
            print(f"ğŸš« æ¨¡å‹å·²æ’é™¤")
            return body
        
        # é‡ç½®ç»Ÿè®¡
        self.stats = ProcessingStats()
        
        # åˆ›å»ºè¿›åº¦è¿½è¸ªå™¨
        progress = ProgressTracker(__event_emitter__)
        
        # Tokenåˆ†æ
        original_tokens = self.count_messages_tokens(messages)
        token_limit = self.get_model_token_limit(model_name)
        
        print(f"ğŸ“Š Token: {original_tokens:,}/{token_limit:,}")
        print(f"ğŸ¯ å¤„ç†ç­–ç•¥: {self.valves.processing_strategy}")
        print(f"ğŸ“ æœ€å°æ‘˜è¦é•¿åº¦: {self.calculate_min_summary_length()}tokens")
        
        # åˆ†æå½“å‰ç”¨æˆ·æ¶ˆæ¯
        current_user_message = self.find_current_user_message(messages)
        if current_user_message:
            current_tokens = self.count_message_tokens(current_user_message)
            print(f"ğŸ’¬ å½“å‰ç”¨æˆ·æ¶ˆæ¯: {current_tokens}tokens")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯")
        
        # æ˜¾ç¤ºåŠ¨æ€å†…å®¹ä¸¢å¤±æ¯”ä¾‹
        max_loss_ratio = self.calculate_dynamic_loss_ratio(original_tokens, token_limit)
        print(f"ğŸ“‰ åŠ¨æ€å†…å®¹ä¸¢å¤±æ¯”ä¾‹: {max_loss_ratio:.1%}")
        
        # è®¡ç®—é¢„æœŸå‹ç¼©ç‡å’Œçª—å£ä½¿ç”¨ç‡
        expected_compression = token_limit / original_tokens if original_tokens > 0 else 0
        print(f"ğŸ“Š é¢„æœŸå‹ç¼©ç‡: {expected_compression:.2%}")
        print(f"ğŸªŸ ç›®æ ‡çª—å£ä½¿ç”¨ç‡: {self.valves.target_window_usage:.1%}")
        
        try:
            # 1. å¤šæ¨¡æ€å¤„ç†
            if self.valves.enable_detailed_progress:
                await progress.start_phase("å¤šæ¨¡æ€å¤„ç†", 1)
            
            processed_messages = await self.process_multimodal_content(
                messages, model_name, progress
            )
            processed_tokens = self.count_messages_tokens(processed_messages)
            print(f"ğŸ“Š å¤šæ¨¡æ€å¤„ç†å: {processed_tokens:,} tokens")
            
            # 2. å†…å®¹å¤„ç†
            if (
                self.valves.enable_content_maximization
                and processed_tokens > token_limit
            ):
                print(f"ğŸ”„ Tokenè¶…é™ï¼Œå¼€å§‹è¿­ä»£å¤„ç†...")
                
                # ä½¿ç”¨ä¿®å¤åçš„è¿­ä»£å¤„ç†ç­–ç•¥
                final_messages = await self.iterative_content_processing_v4(
                    processed_messages, token_limit, progress
                )
                
                # æ‰“å°è¯¦ç»†ç»Ÿè®¡
                self.print_detailed_stats()
                
                body["messages"] = final_messages
                print("âœ… ä½¿ç”¨è¿­ä»£å¤„ç†åçš„æ¶ˆæ¯")
            else:
                # æ›´æ–°ç»Ÿè®¡
                self.stats.original_tokens = original_tokens
                self.stats.original_messages = len(messages)
                self.stats.final_tokens = processed_tokens
                self.stats.final_messages = len(processed_messages)
                self.stats.token_limit = token_limit
                
                # æ£€æŸ¥å½“å‰ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦ä¿ç•™
                final_current_message = self.find_current_user_message(processed_messages)
                self.stats.current_user_message_preserved = final_current_message is not None
                
                # è®¡ç®—çª—å£ä½¿ç”¨ç‡
                window_usage = self.stats.calculate_window_usage_ratio()
                print(f"ğŸªŸ çª—å£ä½¿ç”¨ç‡: {window_usage:.1%}")
                
                if self.valves.enable_detailed_progress:
                    await progress.complete_phase("æ— éœ€å¤„ç†")
                
                body["messages"] = processed_messages
                print("âœ… ç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯")
                
        except Exception as e:
            print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            
            if self.valves.enable_detailed_progress:
                await progress.update_status(f"å¤„ç†å¤±è´¥: {str(e)[:50]}", True)
        
        print("ğŸ ===== INLET DONE =====\n")
        return body

    async def outlet(
        self,
        body: dict,
        user: Optional[dict] = None,
        __event_emitter__: Optional[Callable] = None,
    ) -> dict:
        """å‡ºå£å‡½æ•° - è¿”å›å“åº”"""
        return body
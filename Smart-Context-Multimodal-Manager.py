"""
title: ğŸš€ Advanced Multimodal Context Manager  
author: JiangNanGenius
version: 1.4.4
license: MIT
required_open_webui_version: 0.5.17
description: æ™ºèƒ½é•¿ä¸Šä¸‹æ–‡å’Œå¤šæ¨¡æ€å†…å®¹å¤„ç†å™¨ï¼Œæ”¯æŒå‘é‡åŒ–æ£€ç´¢ã€è¯­ä¹‰é‡æ’åºã€é€’å½’æ€»ç»“ç­‰åŠŸèƒ½
"""
import json
import hashlib
import asyncio
import re
from typing import Optional, List, Dict, Callable, Any, Tuple, Union
from pydantic import BaseModel, Field
from enum import Enum

try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    tiktoken = None

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None

MULTIMODAL_MODELS = {
    "gpt-4o", "gpt-4o-mini", "gpt-4-vision-preview",
    "doubao-1.5-vision-pro", "doubao-1.5-vision-lite",
    "claude-3", "gemini-pro-vision", "qwen-vl"
}

MODEL_TOKEN_LIMITS = {
    "gpt-4o": 128000, "gpt-4o-mini": 128000, "gpt-4": 8192, "gpt-3.5-turbo": 16385,
    "doubao-1.5-thinking-pro": 128000, "doubao-1.5-vision-pro": 128000,
    "doubao-seed": 50000, "doubao": 50000,
    "claude-3": 200000, "gemini-pro": 128000,
}

class VectorStrategy(str, Enum):
    AUTO = "auto"
    MULTIMODAL_FIRST = "multimodal_first"
    TEXT_FIRST = "text_first"
    MIXED = "mixed"
    FALLBACK = "fallback"
    VISION_TO_TEXT = "vision_to_text"

class MultimodalStrategy(str, Enum):
    ALL_MODELS = "all_models"  # æ‰€æœ‰æ¨¡å‹éƒ½è¿›è¡Œå›¾ç‰‡å¤„ç†
    NON_MULTIMODAL_ONLY = "non_multimodal_only"  # åªå¯¹éå¤šæ¨¡æ€æ¨¡å‹å¤„ç†
    CUSTOM_LIST = "custom_list"  # è‡ªå®šä¹‰æ¨¡å‹åˆ—è¡¨
    SMART_ADAPTIVE = "smart_adaptive"  # æ™ºèƒ½è‡ªé€‚åº”

class Filter:
    class Valves(BaseModel):
        # åŸºç¡€æ§åˆ¶
        enable_processing: bool = Field(default=True, description="ğŸ”„ å¯ç”¨æ‰€æœ‰å¤„ç†åŠŸèƒ½")
        excluded_models: str = Field(default="", description="ğŸš« æ’é™¤æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)")
        
        # å¤šæ¨¡æ€å¤„ç†ç­–ç•¥
        multimodal_processing_strategy: MultimodalStrategy = Field(
            default=MultimodalStrategy.SMART_ADAPTIVE, 
            description="ğŸ–¼ï¸ å¤šæ¨¡æ€å¤„ç†ç­–ç•¥"
        )
        force_vision_processing_models: str = Field(
            default="gpt-4,gpt-3.5-turbo,doubao-1.5-thinking-pro", 
            description="ğŸ” å¼ºåˆ¶è¿›è¡Œè§†è§‰å¤„ç†çš„æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)"
        )
        preserve_images_in_multimodal: bool = Field(
            default=True, 
            description="ğŸ“¸ å¤šæ¨¡æ€æ¨¡å‹æ˜¯å¦ä¿ç•™åŸå§‹å›¾ç‰‡"
        )
        always_process_images_before_summary: bool = Field(
            default=True, 
            description="ğŸ“ æ‘˜è¦å‰æ€»æ˜¯å…ˆå¤„ç†å›¾ç‰‡"
        )
        
        # åŠŸèƒ½å¼€å…³
        enable_multimodal: bool = Field(default=True, description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å¤„ç†")
        enable_vision_preprocessing: bool = Field(default=True, description="ğŸ‘ï¸ å¯ç”¨å›¾ç‰‡é¢„å¤„ç†")
        enable_smart_truncation: bool = Field(default=True, description="âœ‚ï¸ å¯ç”¨æ™ºèƒ½æˆªæ–­")
        enable_vector_retrieval: bool = Field(default=True, description="ğŸ” å¯ç”¨å‘é‡æ£€ç´¢")
        
        # è°ƒè¯•
        debug_level: int = Field(default=2, description="ğŸ› è°ƒè¯•çº§åˆ« 0-3")
        show_frontend_progress: bool = Field(default=True, description="ğŸ“± æ˜¾ç¤ºå¤„ç†è¿›åº¦")
        
        # Tokenç®¡ç†
        default_token_limit: int = Field(default=100000, description="âš–ï¸ é»˜è®¤tokené™åˆ¶")
        token_safety_ratio: float = Field(default=0.75, description="ğŸ›¡ï¸ Tokenå®‰å…¨æ¯”ä¾‹")
        
        # ä¿æŠ¤ç­–ç•¥
        preserve_current_query: bool = Field(default=True, description="ğŸ’¾ å§‹ç»ˆä¿æŠ¤å½“å‰ç”¨æˆ·æŸ¥è¯¢")
        preserve_recent_exchanges: int = Field(default=1, description="ğŸ’¬ ä¿æŠ¤æœ€è¿‘å®Œæ•´å¯¹è¯è½®æ¬¡")
        max_preserve_ratio: float = Field(default=0.4, description="ğŸ”’ ä¿æŠ¤æ¶ˆæ¯æœ€å¤§tokenæ¯”ä¾‹")
        max_single_message_tokens: int = Field(default=8000, description="ğŸ“ å•æ¡æ¶ˆæ¯æœ€å¤§token(è¶…è¿‡åˆ™æ‘˜è¦)")
        context_preserve_ratio: float = Field(default=0.6, description="ğŸ“ ä¸Šä¸‹æ–‡ä¿ç•™æ¯”ä¾‹")
        
        # Visioné…ç½®
        vision_api_base: str = Field(default="https://ark.cn-beijing.volces.com/api/v3", description="ğŸ‘ï¸ Vision APIåœ°å€")
        vision_api_key: str = Field(default="", description="ğŸ”‘ Vision APIå¯†é’¥")
        vision_model: str = Field(default="doubao-1.5-vision-pro-250328", description="ğŸ§  Visionæ¨¡å‹")
        vision_prompt_template: str = Field(
            default="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€æ–‡å­—ã€é¢œè‰²ã€å¸ƒå±€ç­‰æ‰€æœ‰å¯è§ä¿¡æ¯ã€‚ä¿æŒå®¢è§‚å‡†ç¡®ï¼Œé‡ç‚¹çªå‡ºå…³é”®ä¿¡æ¯ã€‚", 
            description="ğŸ‘ï¸ Visionæç¤ºè¯"
        )
        vision_max_tokens: int = Field(default=800, description="ğŸ‘ï¸ Visionæœ€å¤§è¾“å‡ºtokens")
        
        # å¤šæ¨¡æ€å‘é‡
        enable_multimodal_vector: bool = Field(default=True, description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å‘é‡")
        multimodal_vector_api_base: str = Field(default="https://ark.cn-beijing.volces.com/api/v3", description="ğŸ”— å¤šæ¨¡æ€å‘é‡API")
        multimodal_vector_api_key: str = Field(default="", description="ğŸ”‘ å¤šæ¨¡æ€å‘é‡å¯†é’¥")
        multimodal_vector_model: str = Field(default="doubao-embedding-vision-250615", description="ğŸ§  å¤šæ¨¡æ€å‘é‡æ¨¡å‹")
        
        # æ–‡æœ¬å‘é‡
        enable_text_vector: bool = Field(default=True, description="ğŸ“ å¯ç”¨æ–‡æœ¬å‘é‡")
        text_vector_api_base: str = Field(default="https://ark.cn-beijing.volces.com/api/v3", description="ğŸ”— æ–‡æœ¬å‘é‡API")
        text_vector_api_key: str = Field(default="", description="ğŸ”‘ æ–‡æœ¬å‘é‡å¯†é’¥")
        text_vector_model: str = Field(default="doubao-embedding-large-text-250515", description="ğŸ§  æ–‡æœ¬å‘é‡æ¨¡å‹")
        
        # å‘é‡ç­–ç•¥
        vector_strategy: VectorStrategy = Field(default=VectorStrategy.AUTO, description="ğŸ¯ å‘é‡åŒ–ç­–ç•¥")
        vector_similarity_threshold: float = Field(default=0.5, description="ğŸ¯ åŸºç¡€ç›¸ä¼¼åº¦é˜ˆå€¼")
        multimodal_similarity_threshold: float = Field(default=0.45, description="ğŸ–¼ï¸ å¤šæ¨¡æ€ç›¸ä¼¼åº¦é˜ˆå€¼")
        text_similarity_threshold: float = Field(default=0.55, description="ğŸ“ æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼")
        
        # é‡æ’åº
        enable_reranking: bool = Field(default=False, description="ğŸ”„ å¯ç”¨é‡æ’åº")
        rerank_api_base: str = Field(default="https://api.bochaai.com", description="ğŸ”„ é‡æ’åºAPI")
        rerank_api_key: str = Field(default="", description="ğŸ”‘ é‡æ’åºå¯†é’¥")
        rerank_model: str = Field(default="gte-rerank", description="ğŸ§  é‡æ’åºæ¨¡å‹")
        rerank_top_k: int = Field(default=10, description="ğŸ” é‡æ’åºè¿”å›æ•°é‡")
        
        # æ‘˜è¦é…ç½®
        summary_api_base: str = Field(default="https://ark.cn-beijing.volces.com/api/v3", description="ğŸ“ æ‘˜è¦API")
        summary_api_key: str = Field(default="", description="ğŸ”‘ æ‘˜è¦å¯†é’¥")
        summary_model: str = Field(default="doubao-1.5-thinking-pro-250415", description="ğŸ§  æ‘˜è¦æ¨¡å‹")
        max_summary_length: int = Field(default=1500, description="ğŸ“ æ‘˜è¦æœ€å¤§é•¿åº¦")
        max_recursion_depth: int = Field(default=3, description="ğŸ”„ æœ€å¤§é€’å½’æ·±åº¦")
        
        # æ€§èƒ½é…ç½®
        max_concurrent_requests: int = Field(default=3, description="âš¡ æœ€å¤§å¹¶å‘æ•°")
        request_timeout: int = Field(default=60, description="â±ï¸ è¯·æ±‚è¶…æ—¶(ç§’)")
        chunk_size: int = Field(default=800, description="ğŸ“„ åˆ†ç‰‡å¤§å°")
        overlap_size: int = Field(default=80, description="ğŸ”— é‡å å¤§å°")

    def __init__(self):
        print("\n" + "="*60)
        print("ğŸš€ Advanced Multimodal Context Manager v1.4.4")
        print("ğŸ“ æ’ä»¶æ­£åœ¨åˆå§‹åŒ–...")
        
        self.valves = self.Valves()
        self._vision_client = None
        self._encoding = None
        self.vision_cache = {}
        self.vector_cache = {}
        self.processing_cache = {}
        
        print(f"âœ… æ’ä»¶åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ”§ å¤„ç†åŠŸèƒ½: {self.valves.enable_processing}")
        print(f"ğŸ”§ å¤šæ¨¡æ€ç­–ç•¥: {self.valves.multimodal_processing_strategy}")
        print(f"ğŸ”§ ä¿æŠ¤ç­–ç•¥: å½“å‰æŸ¥è¯¢+{self.valves.preserve_recent_exchanges}è½®å¯¹è¯")
        print(f"ğŸ”§ ä¿æŠ¤ä¸Šé™: {self.valves.max_preserve_ratio*100}%")
        print(f"ğŸ”§ è°ƒè¯•çº§åˆ«: {self.valves.debug_level}")
        print("="*60 + "\n")

    def debug_log(self, level: int, message: str, emoji: str = "ğŸ”§"):
        if self.valves.debug_level >= level:
            prefix = ["", "ğŸ›[DEBUG]", "ğŸ”[DETAIL]", "ğŸ“‹[VERBOSE]"][min(level, 3)]
            print(f"{prefix} {emoji} {message}")

    def is_model_excluded(self, model_name: str) -> bool:
        if not self.valves.excluded_models or not model_name:
            return False
        
        excluded_list = [model.strip().lower() for model in self.valves.excluded_models.split(",") if model.strip()]
        if not excluded_list:
            return False
        
        model_lower = model_name.lower()
        for excluded_model in excluded_list:
            if excluded_model in model_lower:
                self.debug_log(1, f"æ¨¡å‹ {model_name} åœ¨æ’é™¤åˆ—è¡¨ä¸­", "ğŸš«")
                return True
        return False

    def get_encoding(self):
        if not TIKTOKEN_AVAILABLE:
            return None
        
        if self._encoding is None:
            try:
                self._encoding = tiktoken.get_encoding("cl100k_base")
                self.debug_log(3, "tiktokenç¼–ç å™¨å·²åˆå§‹åŒ–", "ğŸ”§")
            except Exception as e:
                self.debug_log(1, f"tiktokenåˆå§‹åŒ–å¤±è´¥: {e}", "âš ï¸")
        
        return self._encoding

    def count_tokens(self, text: str) -> int:
        if not text:
            return 0
        
        text = str(text)
        encoding = self.get_encoding()
        if encoding:
            try:
                return len(encoding.encode(text))
            except Exception as e:
                self.debug_log(2, f"tokenè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ä¼°ç®—: {e}", "âš ï¸")
        
        return max(len(text) // 3, len(text.encode('utf-8')) // 4)

    def count_message_tokens(self, message: dict) -> int:
        if not message:
            return 0
        
        content = message.get("content", "")
        role = message.get("role", "")
        total_tokens = 0
        
        if isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    total_tokens += self.count_tokens(item.get("text", ""))
                elif item.get("type") == "image_url":
                    total_tokens += 1500  # å›¾ç‰‡tokenä¼°ç®—
        else:
            total_tokens = self.count_tokens(str(content))
        
        # è§’è‰²å’Œæ ¼å¼å¼€é”€
        total_tokens += self.count_tokens(role) + 10
        return total_tokens

    def count_messages_tokens(self, messages: List[dict]) -> int:
        if not messages:
            return 0
        return sum(self.count_message_tokens(msg) for msg in messages)

    def get_model_token_limit(self, model_name: str) -> int:
        for model_key, limit in MODEL_TOKEN_LIMITS.items():
            if model_key in model_name.lower():
                safe_limit = int(limit * self.valves.token_safety_ratio)
                self.debug_log(2, f"æ¨¡å‹ {model_name} é™åˆ¶: {limit} -> {safe_limit}", "âš–ï¸")
                return safe_limit
        
        safe_limit = int(self.valves.default_token_limit * self.valves.token_safety_ratio)
        self.debug_log(1, f"æœªçŸ¥æ¨¡å‹ {model_name}, ä½¿ç”¨é»˜è®¤é™åˆ¶: {safe_limit}", "âš ï¸")
        return safe_limit

    def is_multimodal_model(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŸç”Ÿæ”¯æŒå¤šæ¨¡æ€"""
        return any(mm in model_name.lower() for mm in MULTIMODAL_MODELS)

    def should_process_images_for_model(self, model_name: str) -> bool:
        """æ ¹æ®ç­–ç•¥åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸ºæ­¤æ¨¡å‹å¤„ç†å›¾ç‰‡"""
        if not self.valves.enable_multimodal:
            return False
        
        model_lower = model_name.lower()
        
        # æ£€æŸ¥å¼ºåˆ¶å¤„ç†åˆ—è¡¨
        force_list = [m.strip().lower() for m in self.valves.force_vision_processing_models.split(",") if m.strip()]
        if any(force_model in model_lower for force_model in force_list):
            self.debug_log(2, f"æ¨¡å‹ {model_name} åœ¨å¼ºåˆ¶å¤„ç†åˆ—è¡¨ä¸­", "ğŸ”")
            return True
        
        # æ ¹æ®ç­–ç•¥åˆ¤æ–­
        is_multimodal = self.is_multimodal_model(model_name)
        
        if self.valves.multimodal_processing_strategy == MultimodalStrategy.ALL_MODELS:
            return True
        elif self.valves.multimodal_processing_strategy == MultimodalStrategy.NON_MULTIMODAL_ONLY:
            return not is_multimodal
        elif self.valves.multimodal_processing_strategy == MultimodalStrategy.SMART_ADAPTIVE:
            # æ™ºèƒ½è‡ªé€‚åº”ï¼šéå¤šæ¨¡æ€æ¨¡å‹æ€»æ˜¯å¤„ç†ï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨éœ€è¦æ‘˜è¦æ—¶å¤„ç†
            return True  # è®©åç»­é€»è¾‘å†³å®šå…·ä½“å¤„ç†æ–¹å¼
        else:
            return not is_multimodal

    def has_images_in_content(self, content) -> bool:
        if isinstance(content, list):
            return any(item.get("type") == "image_url" for item in content)
        return False

    def has_images_in_messages(self, messages: List[dict]) -> bool:
        return any(self.has_images_in_content(msg.get("content")) for msg in messages)

    async def send_status(self, __event_emitter__, message: str, done: bool = True, emoji: str = "ğŸ”„"):
        self.debug_log(2, f"çŠ¶æ€: {message}", emoji)
        if __event_emitter__ and self.valves.show_frontend_progress:
            try:
                await __event_emitter__({
                    "type": "status",
                    "data": {"description": f"{emoji} {message}", "done": done}
                })
            except:
                pass

    def get_vision_client(self):
        if not OPENAI_AVAILABLE:
            return None
        
        if self._vision_client:
            return self._vision_client
        
        api_key = self.valves.vision_api_key
        if not api_key:
            if self.valves.multimodal_vector_api_key:
                api_key = self.valves.multimodal_vector_api_key
            elif self.valves.text_vector_api_key:
                api_key = self.valves.text_vector_api_key
        
        if api_key:
            self._vision_client = AsyncOpenAI(
                base_url=self.valves.vision_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout
            )
            self.debug_log(2, "Visionå®¢æˆ·ç«¯å·²åˆ›å»º", "ğŸ‘ï¸")
        
        return self._vision_client

    async def describe_image(self, image_url: str, __event_emitter__) -> str:
        """æè¿°å•å¼ å›¾ç‰‡"""
        image_hash = hashlib.md5(image_url.encode()).hexdigest()
        
        if image_hash in self.vision_cache:
            self.debug_log(3, f"ä½¿ç”¨ç¼“å­˜çš„å›¾ç‰‡æè¿°: {image_hash[:8]}", "ğŸ“‹")
            return self.vision_cache[image_hash]
        
        client = self.get_vision_client()
        if not client:
            return "æ— æ³•å¤„ç†å›¾ç‰‡ï¼šVisionæœåŠ¡æœªé…ç½®"
        
        try:
            self.debug_log(2, f"å¼€å§‹è¯†åˆ«å›¾ç‰‡: {image_hash[:8]}", "ğŸ‘ï¸")
            
            response = await client.chat.completions.create(
                model=self.valves.vision_model,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self.valves.vision_prompt_template},
                        {"type": "image_url", "image_url": {"url": image_url}}
                    ]
                }],
                max_tokens=self.valves.vision_max_tokens,
                temperature=0.2
            )
            
            if response.choices:
                description = response.choices[0].message.content.strip()
                
                # é™åˆ¶æè¿°é•¿åº¦
                if len(description) > 600:
                    description = description[:600] + "..."
                
                self.vision_cache[image_hash] = description
                self.debug_log(2, f"å›¾ç‰‡è¯†åˆ«å®Œæˆ: {len(description)}å­—ç¬¦", "âœ…")
                return description
            
            return "å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥"
            
        except Exception as e:
            error_msg = f"å›¾ç‰‡å¤„ç†é”™è¯¯: {str(e)[:100]}"
            self.debug_log(1, error_msg, "âŒ")
            return error_msg

    async def process_message_images(self, message: dict, __event_emitter__) -> dict:
        """å¤„ç†å•æ¡æ¶ˆæ¯ä¸­çš„å›¾ç‰‡"""
        content = message.get("content", "")
        
        if not isinstance(content, list):
            return message
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        has_images = any(item.get("type") == "image_url" for item in content)
        if not has_images:
            return message
        
        # å¤„ç†å›¾ç‰‡
        processed_content = []
        image_count = 0
        
        for item in content:
            if item.get("type") == "text":
                processed_content.append(item.get("text", ""))
            elif item.get("type") == "image_url":
                image_count += 1
                image_url = item.get("image_url", {}).get("url", "")
                if image_url:
                    description = await self.describe_image(image_url, __event_emitter__)
                    processed_content.append(f"[å›¾ç‰‡{image_count}æè¿°] {description}")
        
        # åˆ›å»ºæ–°æ¶ˆæ¯
        processed_message = message.copy()
        processed_message["content"] = "\n".join(processed_content) if processed_content else ""
        
        self.debug_log(2, f"æ¶ˆæ¯å›¾ç‰‡å¤„ç†å®Œæˆ: {image_count}å¼ å›¾ç‰‡", "ğŸ–¼ï¸")
        return processed_message

    async def process_multimodal_content(self, messages: List[dict], model_name: str, __event_emitter__) -> List[dict]:
        """å¤„ç†å¤šæ¨¡æ€å†…å®¹çš„ä¸»è¦é€»è¾‘"""
        if not self.valves.enable_multimodal:
            return messages
        
        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return messages
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†å›¾ç‰‡
        should_process = self.should_process_images_for_model(model_name)
        is_multimodal = self.is_multimodal_model(model_name)
        
        self.debug_log(1, f"å¤šæ¨¡æ€å¤„ç†æ£€æŸ¥: æ¨¡å‹={model_name}, å¤šæ¨¡æ€={is_multimodal}, éœ€è¦å¤„ç†={should_process}", "ğŸ–¼ï¸")
        
        # å¦‚æœæ˜¯å¤šæ¨¡æ€æ¨¡å‹ä¸”è®¾ç½®ä¿ç•™åŸå§‹å›¾ç‰‡ï¼Œåˆ™ä¸å¤„ç†
        if is_multimodal and self.valves.preserve_images_in_multimodal and not should_process:
            self.debug_log(2, f"å¤šæ¨¡æ€æ¨¡å‹ {model_name} ä¿ç•™åŸå§‹å›¾ç‰‡", "ğŸ“¸")
            return messages
        
        # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
        total_images = 0
        for msg in messages:
            if isinstance(msg.get("content"), list):
                total_images += len([item for item in msg.get("content", []) if item.get("type") == "image_url"])
        
        self.debug_log(1, f"å¼€å§‹å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼š{total_images} å¼ å›¾ç‰‡", "ğŸ–¼ï¸")
        await self.send_status(__event_emitter__, f"å¤„ç† {total_images} å¼ å›¾ç‰‡...", False, "ğŸ–¼ï¸")
        
        # å¤„ç†æ‰€æœ‰æ¶ˆæ¯
        processed_messages = []
        processed_count = 0
        
        for message in messages:
            if self.has_images_in_content(message.get("content")):
                processed_message = await self.process_message_images(message, __event_emitter__)
                processed_messages.append(processed_message)
                # ç»Ÿè®¡å¤„ç†çš„å›¾ç‰‡æ•°é‡
                if isinstance(message.get("content"), list):
                    processed_count += len([item for item in message.get("content", []) if item.get("type") == "image_url"])
            else:
                processed_messages.append(message)
        
        self.debug_log(1, f"å¤šæ¨¡æ€å¤„ç†å®Œæˆï¼š{processed_count} å¼ å›¾ç‰‡", "âœ…")
        await self.send_status(__event_emitter__, "å›¾ç‰‡å¤„ç†å®Œæˆ", True, "âœ…")
        
        return processed_messages

    def get_summary_client(self):
        """è·å–æ‘˜è¦å®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE:
            return None
        
        api_key = self.valves.summary_api_key
        if not api_key:
            if self.valves.multimodal_vector_api_key:
                api_key = self.valves.multimodal_vector_api_key
            elif self.valves.text_vector_api_key:
                api_key = self.valves.text_vector_api_key
        
        if api_key:
            return AsyncOpenAI(
                base_url=self.valves.summary_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout
            )
        return None

    def smart_message_selection(self, messages: List[dict], target_tokens: int) -> Tuple[List[dict], List[dict]]:
        """
        æ™ºèƒ½é€‰æ‹©éœ€è¦ä¿æŠ¤çš„æ¶ˆæ¯å’Œéœ€è¦æ‘˜è¦çš„æ¶ˆæ¯
        """
        if not messages:
            return [], []
        
        # åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå…¶ä»–æ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        other_messages = [msg for msg in messages if msg.get("role") != "system"]
        
        if not other_messages:
            return system_messages, []
        
        # è®¡ç®—æœ€å¤§ä¿æŠ¤tokenæ•°
        max_protect_tokens = int(target_tokens * self.valves.max_preserve_ratio)
        system_tokens = self.count_messages_tokens(system_messages)
        available_protect_tokens = max_protect_tokens - system_tokens
        
        self.debug_log(1, f"ğŸ”’ ä¿æŠ¤ç­–ç•¥: æœ€å¤§{max_protect_tokens}tokens, ç³»ç»Ÿæ¶ˆæ¯{system_tokens}tokens, å¯ç”¨{available_protect_tokens}tokens", "ğŸ“Š")
        
        protected = []
        current_protect_tokens = 0
        
        # 1. å§‹ç»ˆä¿æŠ¤æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆå½“å‰æŸ¥è¯¢ï¼‰
        current_user_msg = None
        if self.valves.preserve_current_query:
            for msg in reversed(other_messages):
                if msg.get("role") == "user":
                    current_user_msg = msg
                    msg_tokens = self.count_message_tokens(msg)
                    
                    # å¦‚æœå½“å‰ç”¨æˆ·æ¶ˆæ¯è¿‡å¤§ï¼Œéœ€è¦å¤„ç†
                    if msg_tokens > self.valves.max_single_message_tokens:
                        self.debug_log(1, f"âš ï¸ å½“å‰ç”¨æˆ·æ¶ˆæ¯è¿‡å¤§({msg_tokens}tokens)ï¼Œéœ€è¦æˆªæ–­", "ğŸ“")
                        # æˆªæ–­ç”¨æˆ·æ¶ˆæ¯å†…å®¹
                        content = msg.get("content", "")
                        if isinstance(content, str) and len(content) > 1000:
                            truncated_content = content[:1000] + "...(ç”¨æˆ·æ¶ˆæ¯å·²æˆªæ–­)"
                            current_user_msg = msg.copy()
                            current_user_msg["content"] = truncated_content
                            msg_tokens = self.count_message_tokens(current_user_msg)
                    
                    if msg_tokens <= available_protect_tokens:
                        protected.append(current_user_msg)
                        current_protect_tokens += msg_tokens
                        available_protect_tokens -= msg_tokens
                        self.debug_log(1, f"ğŸ”’ ä¿æŠ¤å½“å‰æŸ¥è¯¢: {msg_tokens}tokens", "ğŸ’¾")
                    else:
                        self.debug_log(1, f"âš ï¸ å½“å‰æŸ¥è¯¢å¤ªå¤§({msg_tokens}tokens)ï¼Œæ— æ³•å®Œå…¨ä¿æŠ¤", "âš ï¸")
                    break
        
        # 2. ä¿æŠ¤æœ€è¿‘çš„å®Œæ•´å¯¹è¯è½®æ¬¡
        remaining_messages = [msg for msg in other_messages if msg != (current_user_msg or other_messages[-1])]
        exchanges_protected = 0
        
        # ä»åå¾€å‰å¯»æ‰¾å®Œæ•´çš„å¯¹è¯è½®æ¬¡
        i = len(remaining_messages) - 1
        while i >= 0 and exchanges_protected < self.valves.preserve_recent_exchanges and available_protect_tokens > 0:
            if remaining_messages[i].get("role") == "assistant" and i > 0:
                # æ‰¾åˆ°assistantå›å¤ï¼ŒæŸ¥æ‰¾å¯¹åº”çš„useræ¶ˆæ¯
                assistant_msg = remaining_messages[i]
                user_msg = remaining_messages[i-1] if remaining_messages[i-1].get("role") == "user" else None
                
                if user_msg:
                    user_tokens = self.count_message_tokens(user_msg)
                    assistant_tokens = self.count_message_tokens(assistant_msg)
                    
                    # æ£€æŸ¥assistantæ¶ˆæ¯æ˜¯å¦è¿‡å¤§
                    if assistant_tokens > self.valves.max_single_message_tokens:
                        self.debug_log(1, f"âš ï¸ Assistantæ¶ˆæ¯è¿‡å¤§({assistant_tokens}tokens)ï¼Œè·³è¿‡ä¿æŠ¤", "ğŸ“")
                        break
                    
                    pair_tokens = user_tokens + assistant_tokens
                    if pair_tokens <= available_protect_tokens:
                        protected.insert(0, user_msg)
                        protected.insert(1, assistant_msg)
                        current_protect_tokens += pair_tokens
                        available_protect_tokens -= pair_tokens
                        exchanges_protected += 1
                        self.debug_log(1, f"ğŸ”’ ä¿æŠ¤å¯¹è¯è½®æ¬¡{exchanges_protected}: {pair_tokens}tokens", "ğŸ’¾")
                        i -= 2
                    else:
                        self.debug_log(1, f"âš ï¸ å‰©ä½™ä¿æŠ¤tokenä¸è¶³({available_protect_tokens}tokens)ï¼Œåœæ­¢ä¿æŠ¤", "ğŸ“")
                        break
                else:
                    i -= 1
            else:
                i -= 1
        
        # 3. ç¡®å®šéœ€è¦æ‘˜è¦çš„æ¶ˆæ¯
        to_summarize = [msg for msg in other_messages if msg not in protected]
        
        total_protect_tokens = system_tokens + current_protect_tokens
        protect_ratio = total_protect_tokens / target_tokens if target_tokens > 0 else 0
        
        self.debug_log(1, f"ğŸ“‹ æ¶ˆæ¯åˆ†é…: ç³»ç»Ÿ{len(system_messages)}æ¡, ä¿æŠ¤{len(protected)}æ¡({current_protect_tokens}tokens), æ‘˜è¦{len(to_summarize)}æ¡", "ğŸ“")
        self.debug_log(1, f"ğŸ“Š ä¿æŠ¤æ¯”ä¾‹: {protect_ratio:.2%} (é™åˆ¶: {self.valves.max_preserve_ratio:.2%})", "ğŸ“Š")
        
        return system_messages + protected, to_summarize

    async def summarize_messages(self, messages: List[dict], __event_emitter__, depth: int = 0) -> str:
        """æ‰¹é‡æ‘˜è¦æ¶ˆæ¯ - å…ˆå¤„ç†å›¾ç‰‡å†æ‘˜è¦"""
        if not messages:
            return ""
        
        # å¦‚æœéœ€è¦ï¼Œå…ˆå¤„ç†å›¾ç‰‡
        processed_messages = messages
        if self.valves.always_process_images_before_summary:
            has_images = any(self.has_images_in_content(msg.get("content")) for msg in messages)
            if has_images:
                self.debug_log(2, f"æ‘˜è¦å‰å¤„ç†å›¾ç‰‡: {len(messages)}æ¡æ¶ˆæ¯", "ğŸ–¼ï¸")
                processed_messages = []
                for msg in messages:
                    if self.has_images_in_content(msg.get("content")):
                        processed_msg = await self.process_message_images(msg, __event_emitter__)
                        processed_messages.append(processed_msg)
                    else:
                        processed_messages.append(msg)
        
        self.debug_log(1, f"ğŸ¤– å¼€å§‹è°ƒç”¨æ‘˜è¦APIï¼Œæ¶ˆæ¯æ•°: {len(processed_messages)}", "ğŸ“")
        
        client = self.get_summary_client()
        if not client:
            return ""
        
        # è½¬æ¢æ¶ˆæ¯ä¸ºæ–‡æœ¬
        conversation_parts = []
        total_chars = 0
        
        for i, msg in enumerate(processed_messages):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            
            if isinstance(content, list):
                text_parts = [item.get("text", "") for item in content if item.get("type") == "text"]
                content = " ".join(text_parts)
            
            content = str(content)
            
            # é€‚å½“æˆªæ–­è¿‡é•¿çš„å•æ¡æ¶ˆæ¯
            if len(content) > 3000:
                content = content[:3000] + "...(å†…å®¹å·²æˆªæ–­)"
            
            # æ ¼å¼åŒ–æ¶ˆæ¯
            formatted_msg = f"## {role.title()}\n{content}\n"
            conversation_parts.append(formatted_msg)
            total_chars += len(formatted_msg)
            
            # é˜²æ­¢æ€»é•¿åº¦è¿‡é•¿
            if total_chars > 12000:
                conversation_parts.append("\n...(æ›´å¤šæ¶ˆæ¯å·²çœç•¥)")
                break
        
        conversation_text = "\n".join(conversation_parts)
        self.debug_log(2, f"ğŸ“ å¯¹è¯æ–‡æœ¬é•¿åº¦: {len(conversation_text)}å­—ç¬¦", "ğŸ“")
        
        # æ”¹è¿›çš„æ‘˜è¦æç¤º
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¯¹è¯æ‘˜è¦åŠ©æ‰‹ã€‚è¯·ä¸ºä»¥ä¸‹å¯¹è¯åˆ›å»ºä¸€ä¸ªç»“æ„åŒ–çš„æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. æŒ‰å¯¹è¯é¡ºåºæ•´ç†å…³é”®ä¿¡æ¯
2. ä¿ç•™é‡è¦çš„é—®é¢˜ã€å›ç­”å’Œè®¨è®ºè¦ç‚¹
3. å¦‚æœ‰æŠ€æœ¯å†…å®¹ï¼Œä¿ç•™å…·ä½“çš„å‚æ•°ã€é…ç½®æˆ–æ–¹æ³•
4. å¦‚æœ‰å›¾ç‰‡æè¿°ï¼Œä¿ç•™å…³é”®è§†è§‰ä¿¡æ¯
5. ä¿æŒé€»è¾‘æ€§å’Œè¿è´¯æ€§
6. æ§åˆ¶åœ¨{self.valves.max_summary_length}å­—ç¬¦ä»¥å†…
7. ä½¿ç”¨æ¸…æ™°çš„ç»“æ„ï¼Œå¦‚ï¼šç”¨æˆ·é—®é¢˜ -> åŠ©æ‰‹å›ç­” -> è¿›ä¸€æ­¥è®¨è®º

åŸå§‹æ¶ˆæ¯æ•°é‡ï¼š{len(processed_messages)}
é€’å½’æ·±åº¦ï¼š{depth}
è¯·å¼€å§‹æ‘˜è¦ï¼š"""
        
        try:
            response = await client.chat.completions.create(
                model=self.valves.summary_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": conversation_text}
                ],
                max_tokens=self.valves.max_summary_length // 2,
                temperature=0.1,
                timeout=self.valves.request_timeout
            )
            
            if response.choices and response.choices[0].message.content:
                summary = response.choices[0].message.content.strip()
                summary_length = len(summary)
                
                # æ£€æŸ¥æ‘˜è¦è´¨é‡
                if summary_length < 50:
                    self.debug_log(1, f"âš ï¸ æ‘˜è¦è¿‡çŸ­({summary_length}å­—ç¬¦)ï¼Œå¯èƒ½è´¨é‡ä¸ä½³", "ğŸ“")
                    return ""
                
                self.debug_log(1, f"ğŸ“ æ‘˜è¦ç”ŸæˆæˆåŠŸ: {summary_length}å­—ç¬¦", "ğŸ“")
                
                # ç¡®ä¿æ‘˜è¦ä¸è¶…é•¿
                if summary_length > self.valves.max_summary_length:
                    summary = summary[:self.valves.max_summary_length] + "..."
                
                return summary
            else:
                self.debug_log(1, f"âŒ æ‘˜è¦APIè¿”å›ç©ºå“åº”", "ğŸ“")
                return ""
                
        except Exception as e:
            error_msg = f"æ‘˜è¦APIè°ƒç”¨å¤±è´¥: {str(e)[:200]}"
            self.debug_log(1, error_msg, "âŒ")
            return ""

    async def summarize_single_message(self, message: dict, __event_emitter__) -> dict:
        """æ‘˜è¦å•æ¡è¶…é•¿æ¶ˆæ¯ - å…ˆå¤„ç†å›¾ç‰‡"""
        # å¦‚æœæœ‰å›¾ç‰‡ï¼Œå…ˆå¤„ç†å›¾ç‰‡
        processed_message = message
        if self.has_images_in_content(message.get("content")):
            processed_message = await self.process_message_images(message, __event_emitter__)
        
        content = processed_message.get("content", "")
        if isinstance(content, list):
            text_parts = [item.get("text", "") for item in content if item.get("type") == "text"]
            content = " ".join(text_parts)
        
        content = str(content)
        if not content:
            return processed_message
        
        self.debug_log(1, f"ğŸ”„ æ‘˜è¦å•æ¡æ¶ˆæ¯ï¼Œé•¿åº¦: {len(content)}å­—ç¬¦", "ğŸ“")
        
        # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œå…ˆæˆªæ–­
        if len(content) > 5000:
            content = content[:5000] + "...(å†…å®¹å·²æˆªæ–­)"
        
        try:
            client = self.get_summary_client()
            if not client:
                # ç®€å•æˆªæ–­
                truncated_content = content[:800] + "..."
                result = processed_message.copy()
                result["content"] = truncated_content
                return result
            
            system_prompt = f"""è¯·å°†ä»¥ä¸‹å†…å®¹æ‘˜è¦ä¸ºç®€æ´ç‰ˆæœ¬ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œé‡è¦ç»†èŠ‚ï¼ˆåŒ…æ‹¬å›¾ç‰‡æè¿°ä¿¡æ¯ï¼‰ï¼Œæ§åˆ¶åœ¨{self.valves.max_summary_length//2}å­—ç¬¦ä»¥å†…ï¼š"""
            
            response = await client.chat.completions.create(
                model=self.valves.summary_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": content}
                ],
                max_tokens=self.valves.max_summary_length // 3,
                temperature=0.1,
                timeout=self.valves.request_timeout
            )
            
            if response.choices and response.choices[0].message.content:
                summary = response.choices[0].message.content.strip()
                if len(summary) > self.valves.max_summary_length // 2:
                    summary = summary[:self.valves.max_summary_length // 2] + "..."
                
                result = processed_message.copy()
                result["content"] = f"[æ‘˜è¦] {summary}"
                self.debug_log(1, f"âœ… å•æ¡æ¶ˆæ¯æ‘˜è¦å®Œæˆ: {len(content)} -> {len(summary)}å­—ç¬¦", "ğŸ“")
                return result
            else:
                # æ‘˜è¦å¤±è´¥ï¼Œæˆªæ–­å¤„ç†
                truncated_content = content[:800] + "..."
                result = processed_message.copy()
                result["content"] = truncated_content
                return result
                
        except Exception as e:
            self.debug_log(1, f"âŒ å•æ¡æ¶ˆæ¯æ‘˜è¦å¤±è´¥: {e}", "ğŸ“")
            truncated_content = content[:800] + "..."
            result = processed_message.copy()
            result["content"] = truncated_content
            return result

    async def recursive_summarize(self, messages: List[dict], target_tokens: int, __event_emitter__, depth: int = 0) -> List[dict]:
        """é€’å½’æ‘˜è¦å¤„ç†"""
        self.debug_log(1, f"ğŸ”„ å¼€å§‹ç¬¬{depth+1}è½®é€’å½’æ‘˜è¦", "ğŸ“")
        
        if depth >= self.valves.max_recursion_depth:
            self.debug_log(1, f"âŒ è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦ï¼Œä½¿ç”¨ç´§æ€¥ä¿æŠ¤", "ğŸ”„")
            return self.emergency_truncate(messages, target_tokens)
        
        current_tokens = self.count_messages_tokens(messages)
        self.debug_log(1, f"ğŸ“Š å½“å‰token: {current_tokens}, ç›®æ ‡: {target_tokens}", "ğŸ“")
        
        if current_tokens <= target_tokens:
            self.debug_log(1, f"âœ… Tokenå·²æ»¡è¶³è¦æ±‚", "ğŸ“")
            return messages
        
        await self.send_status(__event_emitter__, f"ç¬¬{depth+1}è½®æ‘˜è¦ ({current_tokens}â†’{target_tokens})", False, "ğŸ“")
        
        # æ™ºèƒ½é€‰æ‹©æ¶ˆæ¯
        protected_messages, to_summarize = self.smart_message_selection(messages, target_tokens)
        
        if not to_summarize:
            self.debug_log(1, f"âš ï¸ æ²¡æœ‰å¯æ‘˜è¦çš„æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶å¤„ç†", "ğŸ“")
            # å¦‚æœä¿æŠ¤çš„æ¶ˆæ¯ä»ç„¶è¶…é™ï¼Œéœ€è¦å¼ºåˆ¶å¤„ç†
            protected_tokens = self.count_messages_tokens(protected_messages)
            if protected_tokens > target_tokens:
                self.debug_log(1, f"âš ï¸ ä¿æŠ¤æ¶ˆæ¯è¶…é™({protected_tokens}>{target_tokens})ï¼Œå¼ºåˆ¶å¤„ç†", "ğŸ“")
                return self.emergency_truncate(protected_messages, target_tokens)
            else:
                return protected_messages
        
        # å¤„ç†éœ€è¦æ‘˜è¦çš„æ¶ˆæ¯
        processed_messages = protected_messages.copy()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶…å¤§æ¶ˆæ¯éœ€è¦å•ç‹¬å¤„ç†
        large_messages = []
        normal_messages = []
        
        for msg in to_summarize:
            msg_tokens = self.count_message_tokens(msg)
            if msg_tokens > self.valves.max_single_message_tokens:
                large_messages.append(msg)
            else:
                normal_messages.append(msg)
        
        # å¤„ç†è¶…å¤§æ¶ˆæ¯
        if large_messages:
            self.debug_log(1, f"ğŸ”„ å¤„ç†{len(large_messages)}æ¡è¶…å¤§æ¶ˆæ¯", "ğŸ“")
            for large_msg in large_messages:
                summarized_msg = await self.summarize_single_message(large_msg, __event_emitter__)
                processed_messages.append(summarized_msg)
        
        # å¤„ç†æ­£å¸¸æ¶ˆæ¯
        if normal_messages:
            self.debug_log(1, f"ğŸ”„ æ‰¹é‡æ‘˜è¦{len(normal_messages)}æ¡æ¶ˆæ¯", "ğŸ“")
            summary_text = await self.summarize_messages(normal_messages, __event_emitter__, depth)
            
            if summary_text and len(summary_text) > 50:
                summary_message = {
                    "role": "system",
                    "content": f"=== ğŸ“‹ å¯¹è¯æ‘˜è¦ (ç¬¬{depth+1}è½®) ===\n{summary_text}\n{'='*50}"
                }
                processed_messages.append(summary_message)
            else:
                # æ‘˜è¦å¤±è´¥ï¼Œä¿ç•™éƒ¨åˆ†é‡è¦æ¶ˆæ¯
                self.debug_log(1, f"âŒ æ‘˜è¦è´¨é‡ä¸ä½³ï¼Œä¿ç•™é‡è¦æ¶ˆæ¯", "ğŸ“")
                important_messages = normal_messages[-1:] if normal_messages else []
                processed_messages.extend(important_messages)
        
        new_tokens = self.count_messages_tokens(processed_messages)
        self.debug_log(1, f"ğŸ“Š å¤„ç†åtoken: {new_tokens}, å‡å°‘{current_tokens - new_tokens}", "ğŸ“")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­é€’å½’
        if new_tokens > target_tokens:
            self.debug_log(1, f"ğŸ”„ ä»è¶…é™ï¼Œç»§ç»­é€’å½’", "ğŸ“")
            return await self.recursive_summarize(processed_messages, target_tokens, __event_emitter__, depth + 1)
        else:
            self.debug_log(1, f"âœ… æ‘˜è¦æˆåŠŸ: {current_tokens}â†’{new_tokens}tokens", "ğŸ“")
            await self.send_status(__event_emitter__, f"æ‘˜è¦å®Œæˆ ({new_tokens}/{target_tokens})", True, "âœ…")
            return processed_messages

    def emergency_truncate(self, messages: List[dict], target_tokens: int) -> List[dict]:
        """ç´§æ€¥æˆªæ–­ç­–ç•¥ï¼šä¿ç•™æœ€æ ¸å¿ƒçš„æ¶ˆæ¯"""
        self.debug_log(1, f"ğŸ†˜ å¯ç”¨ç´§æ€¥æˆªæ–­ç­–ç•¥", "ğŸ“")
        
        # åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå…¶ä»–æ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        other_messages = [msg for msg in messages if msg.get("role") != "system"]
        
        result = system_messages.copy()
        current_tokens = self.count_messages_tokens(result)
        
        # ä»åå¾€å‰ä¿ç•™æ¶ˆæ¯ï¼Œä¼˜å…ˆä¿ç•™ç”¨æˆ·æ¶ˆæ¯
        for msg in reversed(other_messages):
            msg_tokens = self.count_message_tokens(msg)
            if current_tokens + msg_tokens <= target_tokens:
                result.append(msg)
                current_tokens += msg_tokens
            elif msg.get("role") == "user":
                # ç”¨æˆ·æ¶ˆæ¯ä¼˜å…ˆä¿ç•™ï¼Œå³ä½¿éœ€è¦æˆªæ–­
                content = msg.get("content", "")
                if isinstance(content, str) and len(content) > 200:
                    truncated_msg = msg.copy()
                    truncated_msg["content"] = content[:200] + "...(ç´§æ€¥æˆªæ–­)"
                    truncated_tokens = self.count_message_tokens(truncated_msg)
                    if current_tokens + truncated_tokens <= target_tokens:
                        result.append(truncated_msg)
                        current_tokens += truncated_tokens
                break
        
        # é‡æ–°æ’åºï¼ˆä¿æŒç³»ç»Ÿæ¶ˆæ¯åœ¨å‰ï¼‰
        other_result = [msg for msg in result if msg.get("role") != "system"]
        other_result.reverse()
        final_result = system_messages + other_result
        
        final_tokens = self.count_messages_tokens(final_result)
        self.debug_log(1, f"ğŸ†˜ ç´§æ€¥æˆªæ–­å®Œæˆ: {len(final_result)}æ¡æ¶ˆæ¯, {final_tokens}tokens", "ğŸ“")
        
        return final_result

    async def smart_truncate_messages(self, messages: List[dict], target_tokens: int, __event_emitter__) -> List[dict]:
        """æ™ºèƒ½æˆªæ–­æ¶ˆæ¯"""
        current_tokens = self.count_messages_tokens(messages)
        
        if current_tokens <= target_tokens:
            return messages
        
        self.debug_log(1, f"å¼€å§‹æ™ºèƒ½æˆªæ–­: {current_tokens} -> {target_tokens} tokens", "âœ‚ï¸")
        return await self.recursive_summarize(messages, target_tokens, __event_emitter__)

    async def inlet(self, body: dict, user: Optional[dict] = None, __event_emitter__: Optional[Callable] = None) -> dict:
        """å…¥å£å‡½æ•° - å¤„ç†è¯·æ±‚"""
        print("\nğŸš€ ===== INLET CALLED =====")
        print(f"ğŸ“¨ æ”¶åˆ°è¯·æ±‚: {list(body.keys())}")
        
        if not self.valves.enable_processing:
            print("âŒ å¤„ç†åŠŸèƒ½å·²ç¦ç”¨")
            return body
        
        messages = body.get("messages", [])
        if not messages:
            print("âŒ æ— æ¶ˆæ¯å†…å®¹")
            return body
        
        model_name = body.get("model", "æœªçŸ¥")
        print(f"ğŸ“‹ æ¨¡å‹: {model_name}, æ¶ˆæ¯æ•°: {len(messages)}")
        
        if self.is_model_excluded(model_name):
            print(f"ğŸš« æ¨¡å‹å·²æ’é™¤")
            return body
        
        # Tokenåˆ†æ
        original_tokens = self.count_messages_tokens(messages)
        token_limit = self.get_model_token_limit(model_name)
        print(f"ğŸ“Š Token: {original_tokens}/{token_limit}")
        
        try:
            # 1. å¤šæ¨¡æ€å¤„ç†
            should_process_images = self.should_process_images_for_model(model_name)
            is_multimodal = self.is_multimodal_model(model_name)
            
            print(f"ğŸ–¼ï¸ å›¾ç‰‡å¤„ç†ç­–ç•¥: åº”è¯¥å¤„ç†={should_process_images}, å¤šæ¨¡æ€æ¨¡å‹={is_multimodal}")
            
            processed_messages = await self.process_multimodal_content(messages, model_name, __event_emitter__)
            processed_tokens = self.count_messages_tokens(processed_messages)
            print(f"ğŸ“Š å¤šæ¨¡æ€å¤„ç†å: {processed_tokens} tokens")
            
            # 2. æ™ºèƒ½æˆªæ–­
            if self.valves.enable_smart_truncation and processed_tokens > token_limit:
                print(f"âš ï¸ Tokenè¶…é™ï¼Œå¼€å§‹æ™ºèƒ½æˆªæ–­...")
                final_messages = await self.smart_truncate_messages(processed_messages, token_limit, __event_emitter__)
                final_tokens = self.count_messages_tokens(final_messages)
                print(f"ğŸ“Š æˆªæ–­å: {final_tokens} tokens")
                
                # æ›´ä¸¥æ ¼çš„æ£€æŸ¥
                if final_tokens <= token_limit:
                    body["messages"] = final_messages
                    print("âœ… ä½¿ç”¨æˆªæ–­åçš„æ¶ˆæ¯")
                    await self.send_status(__event_emitter__, f"å¤„ç†å®Œæˆ ({final_tokens}/{token_limit})", True, "âœ…")
                else:
                    print(f"âš ï¸ æˆªæ–­æ•ˆæœä¸ä½³ï¼Œå¯ç”¨ç´§æ€¥ç­–ç•¥")
                    emergency_messages = self.emergency_truncate(final_messages, token_limit)
                    emergency_tokens = self.count_messages_tokens(emergency_messages)
                    body["messages"] = emergency_messages
                    print(f"ğŸ†˜ ç´§æ€¥å¤„ç†: {emergency_tokens} tokens")
                    await self.send_status(__event_emitter__, f"ç´§æ€¥å¤„ç†å®Œæˆ ({emergency_tokens}/{token_limit})", True, "ğŸ†˜")
            else:
                body["messages"] = processed_messages
                print("âœ… ç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯")
                
        except Exception as e:
            print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            await self.send_status(__event_emitter__, f"å¤„ç†å¤±è´¥: {str(e)[:50]}", True, "âŒ")
        
        print("ğŸ ===== INLET DONE =====\n")
        return body

    async def outlet(self, body: dict, user: Optional[dict] = None, __event_emitter__: Optional[Callable] = None) -> dict:
        """å‡ºå£å‡½æ•° - è¿”å›å“åº”"""
        return body

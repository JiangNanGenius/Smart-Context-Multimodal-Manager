"""
title: ğŸš€ Advanced Context Manager - Zero-Loss Coverage-First v2.4.5
author: JiangNanGenius
version: 2.4.5
license: MIT
required_open_webui_version: 0.5.17
Github: https://github.com/JiangNanGenius
description: æ€§èƒ½ä¼˜åŒ–ç‰ˆ - å¹¶å‘æ‘˜è¦ç”Ÿæˆ + ç¼“å­˜æœºåˆ¶ + ç»Ÿè®¡ä¿®æ­£
"""

import json
import hashlib
import asyncio
import re
import base64
import math
import time
import copy
import html
from typing import Optional, List, Dict, Callable, Any, Tuple, Union
from pydantic import BaseModel, Field
from enum import Enum
from collections import defaultdict

# å¯¼å…¥ä¾èµ–åº“
try:
    import tiktoken

    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    tiktoken = None

try:
    import httpx

    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None

try:
    from openai import AsyncOpenAI

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None


class EmbeddingCache:
    """å‘é‡ç¼“å­˜å™¨ - åŸºäºcontent_keyç¼“å­˜"""

    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
        self.access_count = {}

    def get(self, content_key: str) -> Optional[List[float]]:
        """è·å–ç¼“å­˜çš„å‘é‡"""
        if content_key in self.cache:
            self.access_count[content_key] = self.access_count.get(content_key, 0) + 1
            return self.cache[content_key]
        return None

    def set(self, content_key: str, embedding: List[float]):
        """è®¾ç½®ç¼“å­˜çš„å‘é‡"""
        if len(self.cache) >= self.max_size:
            # æ¸…ç†æœ€å°‘ä½¿ç”¨çš„é¡¹
            least_used = min(self.access_count.items(), key=lambda x: x[1])
            del self.cache[least_used[0]]
            del self.access_count[least_used[0]]

        self.cache[content_key] = embedding
        self.access_count[content_key] = 1

    def clear(self):
        """æ¸…ç†ç¼“å­˜"""
        self.cache.clear()
        self.access_count.clear()


class MessageOrder:
    """æ¶ˆæ¯é¡ºåºç®¡ç†å™¨ - IDç¨³å®šåŒ–æ”¹è¿›"""

    def __init__(self, original_messages: List[dict]):
        # ä¸è¦deepcopyï¼Œç›´æ¥åœ¨åŸæ¶ˆæ¯ä¸Šæ‰“æ ‡ç­¾
        self.original_messages = original_messages
        self.order_map = {}  # æ¶ˆæ¯IDåˆ°åŸå§‹ç´¢å¼•çš„æ˜ å°„
        self.message_ids = {}  # åŸå§‹ç´¢å¼•åˆ°æ¶ˆæ¯IDçš„æ˜ å°„
        self.content_map = {}  # å†…å®¹æ ‡è¯†åˆ°åŸå§‹ç´¢å¼•çš„æ˜ å°„

        # IDç¨³å®šåŒ–ï¼šä½¿ç”¨å¯é‡ç°çš„hashï¼Œä¸æ··å…¥time.time()
        for i, msg in enumerate(self.original_messages):
            content_key = self._generate_stable_content_key(msg)
            # ä½¿ç”¨ç´¢å¼•+å†…å®¹ç”Ÿæˆç¨³å®šIDï¼Œä¸åŒ…å«æ—¶é—´æˆ³
            msg_id = hashlib.md5(f"{i}_{content_key}".encode()).hexdigest()[:16]

            self.order_map[msg_id] = i
            self.message_ids[i] = msg_id
            self.content_map[content_key] = i

            # åœ¨æ¶ˆæ¯ä¸­æ·»åŠ é¡ºåºæ ‡è®°ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰
            msg["_order_id"] = msg_id
            msg["_original_index"] = i
            msg["_content_key"] = content_key

    def _generate_stable_content_key(self, msg: dict) -> str:
        """ç”Ÿæˆç¨³å®šçš„æ¶ˆæ¯å†…å®¹æ ‡è¯†ï¼ˆä¸å«æ—¶é—´æˆ³ï¼‰"""
        role = msg.get("role", "")
        content = msg.get("content", "")

        # å¤„ç†å¤šæ¨¡æ€å†…å®¹
        if isinstance(content, list):
            content_parts = []
            for item in content:
                if item.get("type") == "text":
                    content_parts.append(item.get("text", "")[:100])  # å–å‰100å­—ç¬¦
                elif item.get("type") == "image_url":
                    image_data = item.get("image_url", {}).get("url", "")
                    if image_data.startswith("data:"):
                        # å¯¹base64å›¾ç‰‡ï¼Œå–header+å‰50å­—ç¬¦ä½œä¸ºç¨³å®šæ ‡è¯†
                        try:
                            header, data = image_data.split("base64,", 1)
                            content_parts.append(f"[IMAGE:{header}:{data[:50]}]")
                        except:
                            content_parts.append("[IMAGE:invalid]")
                    else:
                        content_parts.append(f"[IMAGE:url:{image_data[:50]}]")
            content_str = " ".join(content_parts)
        else:
            content_str = str(content)[:200]  # å–å‰200å­—ç¬¦

        return f"{role}:{content_str}"

    def generate_chunk_id(self, msg_id: str, chunk_index: int) -> str:
        """ç”Ÿæˆchunk IDï¼š{msg_id}#k æ ¼å¼ä¿æŒä¸€è‡´"""
        return f"{msg_id}#{chunk_index}"

    def find_current_user_message_index(self, messages: List[dict]) -> int:
        """æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•ï¼ˆæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰"""
        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if msg.get("role") == "user":
                return i
        return -1

    def sort_messages_preserve_user(
        self, messages: List[dict], current_user_message: dict = None
    ) -> List[dict]:
        """
        æ ¹æ®åŸå§‹é¡ºåºæ’åºæ¶ˆæ¯ï¼Œä½†ä¿æŠ¤å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„ä½ç½®
        ç¡®ä¿å½“å‰ç”¨æˆ·æ¶ˆæ¯å§‹ç»ˆåœ¨æœ€å
        """
        if not messages:
            return messages

        # åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå…¶ä»–æ¶ˆæ¯
        other_messages = []
        current_user_in_list = None

        for msg in messages:
            if current_user_message and msg.get(
                "_order_id"
            ) == current_user_message.get("_order_id"):
                current_user_in_list = msg
            else:
                other_messages.append(msg)

        # æŒ‰åŸå§‹é¡ºåºæ’åºå…¶ä»–æ¶ˆæ¯
        def get_order(msg):
            return msg.get("_original_index", 999999)

        other_messages.sort(key=get_order)

        # å¦‚æœæ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼Œå°†å…¶æ”¾åœ¨æœ€å
        if current_user_in_list:
            return other_messages + [current_user_in_list]
        else:
            return other_messages

    def get_message_preview(self, msg: dict) -> str:
        """è·å–æ¶ˆæ¯é¢„è§ˆç”¨äºè°ƒè¯•"""
        if isinstance(msg.get("content"), list):
            text_parts = []
            for item in msg.get("content", []):
                if item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
                elif item.get("type") == "image_url":
                    text_parts.append("[å›¾ç‰‡]")
            content = " ".join(text_parts)
        else:
            content = str(msg.get("content", ""))

        # æœ€å°æ¸…ç†
        content = content.replace("\n", " ").replace("\r", " ")
        content = re.sub(r"\s+", " ", content).strip()
        return content[:100] + "..." if len(content) > 100 else content


class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯è®°å½•å™¨ - å¢å¼ºç‰ˆ"""

    def __init__(self):
        # åŸºç¡€ç»Ÿè®¡
        self.original_tokens = 0
        self.original_messages = 0
        self.final_tokens = 0
        self.final_messages = 0
        self.token_limit = 0
        self.target_tokens = 0
        self.current_user_tokens = 0

        # å¤„ç†ç»Ÿè®¡
        self.iterations = 0
        self.chunked_messages = 0
        self.summarized_messages = 0
        self.vector_retrievals = 0
        self.rerank_operations = 0
        self.multimodal_processed = 0
        self.processing_time = 0.0

        # Coverage-Firstç­–ç•¥ç»Ÿè®¡
        self.coverage_rate = 0.0
        self.coverage_total_messages = 0
        self.coverage_preserved_count = 0
        self.coverage_preserved_tokens = 0
        self.coverage_summary_count = 0
        self.coverage_summary_tokens = 0
        self.coverage_micro_summaries = 0
        self.coverage_block_summaries = 0
        self.coverage_upgrade_count = 0
        self.coverage_upgrade_tokens_saved = 0
        self.coverage_budget_usage = 0.0

        # æ–°å¢ï¼šåˆ†å—ä¸é¢„ç®—ç»Ÿè®¡
        self.chunked_messages_count = 0  # è¢«åˆ†ç‰‡çš„æ¶ˆæ¯æ•°
        self.total_chunks_created = 0  # åˆ›å»ºçš„æ€»ç‰‡æ•°
        self.adaptive_blocks_created = 0  # è‡ªé€‚åº”å—æ•°
        self.block_merge_operations = 0  # å—åˆå¹¶æ“ä½œæ•°
        self.budget_scaling_applied = 0  # é¢„ç®—ç¼©æ”¾åº”ç”¨æ¬¡æ•°
        self.scaling_factor = 1.0  # å®é™…ç¼©æ”¾å› å­

        # æŠ¤æ ç»Ÿè®¡
        self.guard_a_warnings = 0  # æŠ¤æ Aè­¦å‘Šæ¬¡æ•°
        self.guard_b_fallbacks = 0  # æŠ¤æ Bå›é€€æ¬¡æ•°
        self.id_mapping_errors = 0  # IDæ˜ å°„é”™è¯¯æ¬¡æ•°

        # é›¶ä¸¢å¤±ä¿éšœç»Ÿè®¡
        self.zero_loss_guarantee = True
        self.budget_adjustments = 0
        self.min_budget_applied = 0
        self.insurance_truncation_avoided = 0

        # Top-upç»Ÿè®¡
        self.topup_applied = 0
        self.topup_micro_upgraded = 0
        self.topup_raw_added = 0
        self.topup_tokens_added = 0

        # æ€§èƒ½ç»Ÿè®¡
        self.api_failures = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.concurrent_tasks = 0
        self.embedding_requests = 0
        self.summary_requests = 0

        # å…¶ä»–ç»Ÿè®¡ä¿æŒä¸å˜...
        self.preserved_messages = 0
        self.processed_messages = 0
        self.summary_messages = 0
        self.emergency_truncations = 0
        self.content_loss_ratio = 0.0
        self.discarded_messages = 0
        self.recovered_messages = 0
        self.window_utilization = 0.0
        self.try_preserve_tokens = 0
        self.try_preserve_messages = 0
        self.try_preserve_summary_messages = 0
        self.keyword_generations = 0
        self.context_maximization_detections = 0
        self.chunk_created = 0
        self.chunk_processed = 0
        self.recursive_summaries = 0
        self.context_max_direct_preserve = 0
        self.context_max_chunked = 0
        self.context_max_summarized = 0
        self.multimodal_extracted = 0
        self.fallback_preserve_applied = 0
        self.user_message_recovery_count = 0
        self.rag_no_results_count = 0
        self.history_message_separation_count = 0
        self.image_processing_errors = 0
        self.syntax_errors_fixed = 0
        self.truncation_skip_count = 0
        self.truncation_recovered_messages = 0
        self.smart_truncation_applied = 0

    def calculate_retention_ratio(self) -> float:
        """è®¡ç®—å†…å®¹ä¿ç•™æ¯”ä¾‹"""
        if self.original_tokens == 0:
            return 0.0
        return self.final_tokens / self.original_tokens

    def calculate_window_usage_ratio(self) -> float:
        """è®¡ç®—å¯¹è¯çª—å£ä½¿ç”¨ç‡"""
        if self.target_tokens == 0:
            return 0.0
        return self.final_tokens / self.target_tokens

    def get_summary(self) -> str:
        """è·å–ç»Ÿè®¡æ‘˜è¦ - ç®€åŒ–ç‰ˆæœ¬"""
        retention = self.calculate_retention_ratio()
        window_usage = self.calculate_window_usage_ratio()

        return f"""ğŸ“Š Coverage-Firstå¤„ç†å®Œæˆ:
â”œâ”€ æ¶ˆæ¯: {self.original_messages} -> {self.final_messages}æ¡ | tokens: {self.original_tokens:,} -> {self.final_tokens:,}
â”œâ”€ çª—å£ä½¿ç”¨: {window_usage:.1%} | å†…å®¹ä¿ç•™: {retention:.1%}
â”œâ”€ Coverage: è¦†ç›–ç‡{self.coverage_rate:.1%}, åŸæ–‡{self.coverage_preserved_count}æ¡+æ‘˜è¦{self.coverage_summary_count}æ¡
â”œâ”€ æ€§èƒ½: å¤„ç†{self.processing_time:.1f}s, APIè°ƒç”¨{self.summary_requests}æ¬¡, ç¼“å­˜å‘½ä¸­{self.cache_hits}æ¬¡
â””â”€ é›¶ä¸¢å¤±: {'âœ…' if self.zero_loss_guarantee else 'âš ï¸'}"""


class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨"""

    def __init__(self, event_emitter):
        self.event_emitter = event_emitter
        self.current_step = 0
        self.total_steps = 0
        self.current_phase = ""
        self.phase_progress = 0
        self.phase_total = 0
        self.logged_phases = set()

    def create_progress_bar(self, percentage: float, width: int = 15) -> str:
        """åˆ›å»ºç¾è§‚çš„è¿›åº¦æ¡"""
        filled = int(percentage * width / 100)
        if percentage >= 100:
            bar = "â–ˆ" * width
        else:
            bar = "â–ˆ" * filled + "â–“" * max(0, 1) + "â–‘" * max(0, width - filled - 1)
        return f"[{bar}] {percentage:.1f}%"

    async def start_phase(self, phase_name: str, total_items: int = 0):
        """å¼€å§‹æ–°é˜¶æ®µ"""
        self.current_phase = phase_name
        self.phase_progress = 0
        self.phase_total = total_items
        self.logged_phases.add(phase_name)
        await self.update_status(f"å¼€å§‹ {phase_name}")

    async def update_progress(
        self, completed: int, total: int = None, detail: str = ""
    ):
        """æ›´æ–°è¿›åº¦"""
        if total is None:
            total = self.phase_total

        self.phase_progress = completed

        if total > 0:
            percentage = (completed / total) * 100
            progress_bar = self.create_progress_bar(percentage)
            status = f"{self.current_phase} {progress_bar}"
            if detail:
                status += f" - {detail}"
        else:
            status = f"{self.current_phase}"
            if detail:
                status += f" - {detail}"

        await self.update_status(status, False)

    async def complete_phase(self, message: str = ""):
        """å®Œæˆå½“å‰é˜¶æ®µ"""
        final_message = f"{self.current_phase} å®Œæˆ"
        if message:
            final_message += f" - {message}"
        await self.update_status(final_message, True)

    async def update_status(self, message: str, done: bool = False):
        """æ›´æ–°çŠ¶æ€"""
        if self.event_emitter:
            try:
                # åŸºæœ¬æ¸…ç†
                message = message.replace("\n", " ").replace("\r", " ")
                message = re.sub(r"\s+", " ", message).strip()
                await self.event_emitter(
                    {
                        "type": "status",
                        "data": {"description": message, "done": done},
                    }
                )
            except Exception as e:
                if str(e) not in self.logged_phases:
                    print(f"âš ï¸ è¿›åº¦æ›´æ–°å¤±è´¥: {e}")
                    self.logged_phases.add(str(e))


class ModelMatcher:
    """æ™ºèƒ½æ¨¡å‹åŒ¹é…å™¨ - æ›´æ–°ç‰ˆæœ¬æ”¯æŒæ›´å¤šæ¨¡å‹"""

    def __init__(self):
        # ç²¾ç¡®åŒ¹é…è§„åˆ™ï¼ˆæ›´æ–°ç‰ˆæœ¬ï¼‰
        self.exact_matches = {
            # ===== OpenAI / GPT å®¶æ— =====
            "gpt-5": {
                "family": "gpt",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 2000,
            },
            "gpt-5-mini": {
                "family": "gpt",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1800,
            },
            "gpt-5-nano": {
                "family": "gpt",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1600,
            },
            "gpt-4o": {
                "family": "gpt",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 1500,
            },
            "gpt-4o-mini": {
                "family": "gpt",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 1200,
            },
            "gpt-4-turbo": {
                "family": "gpt",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 1500,
            },
            "gpt-4-vision-preview": {
                "family": "gpt",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 1500,
            },
            "gpt-4": {
                "family": "gpt",
                "multimodal": False,
                "limit": 8192,
                "image_tokens": 0,
            },
            "gpt-3.5-turbo": {
                "family": "gpt",
                "multimodal": False,
                "limit": 16385,
                "image_tokens": 0,
            },
            # ===== Anthropic / Claude å®¶æ—ï¼ˆæ›´æ–° 4 / 4.1ï¼‰=====
            "claude-4.1": {
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            "claude-4": {
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            # å…¼å®¹ä¿ç•™ï¼šæ—§æ¡ç›®
            "claude-3-5-sonnet": {
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            "claude-3-opus": {
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            "claude-3-haiku": {
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 800,
            },
            "claude-3": {
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            # thinking ç‰¹ä¾‹
            "anthropic.claude-4-sonnet-latest-extended-thinking": {
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
                "special": "thinking",
            },
            # ===== ByteDance / Doubao å®¶æ—ï¼ˆä¿ç•™åŸè¡¨ï¼‰=====
            "doubao-1.5-vision-pro": {
                "family": "doubao",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 1500,
            },
            "doubao-1.5-vision-lite": {
                "family": "doubao",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 1200,
            },
            "doubao-1.5-thinking-pro": {
                "family": "doubao",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
                "special": "thinking",
            },
            "doubao-seed-1-6-250615": {
                "family": "doubao",
                "multimodal": True,
                "limit": 50000,
                "image_tokens": 1000,
            },
            "doubao-seed": {
                "family": "doubao",
                "multimodal": False,
                "limit": 50000,
                "image_tokens": 0,
            },
            "doubao": {
                "family": "doubao",
                "multimodal": False,
                "limit": 50000,
                "image_tokens": 0,
            },
            "doubao-1-5-pro-256k": {
                "family": "doubao",
                "multimodal": False,
                "limit": 200000,
                "image_tokens": 0,
            },
            # ===== Google / Geminiï¼ˆä¿ç•™åŸè¡¨ï¼‰=====
            "gemini-pro": {
                "family": "gemini",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            "gemini-pro-vision": {
                "family": "gemini",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 800,
            },
            # ===== Alibaba / Qwen =====
            "qwen-vl": {
                "family": "qwen",
                "multimodal": True,
                "limit": 32000,
                "image_tokens": 1000,
            },
            # æ–°å¢ï¼šQwen3 Coder Instruct ç³»åˆ—
            "Qwen/Qwen3-Coder-480B-Instruct": {
                "family": "qwen",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            "Qwen/Qwen3-Coder-A35B-Instruct": {
                "family": "qwen",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== Huawei / Pangu =====
            "pangu-pro-moe": {
                "family": "pangu",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== Zhipu / GLM =====
            "zai-org/GLM-4.5": {
                "family": "glm",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            "glm-4.5": {
                "family": "glm",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== Tencent / Hunyuan =====
            "hunyuan-large": {
                "family": "hunyuan",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            "hunyuan-pro": {
                "family": "hunyuan",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            "hunyuan-vision": {
                "family": "hunyuan",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 800,
            },
            # ===== Moonshot / Kimi =====
            "kimi-k2-instruct": {
                "family": "kimi",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== Baidu / ERNIE 4 / 4.5 =====
            "ernie-4.0": {
                "family": "ernie",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            "ernie-4.5": {
                "family": "ernie",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            "ernie-4.0-turbo": {
                "family": "ernie",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            "ernie-4.5-turbo": {
                "family": "ernie",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== stepfun =====
            "stepfun-ai/step3": {
                "family": "stepfun",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            "stepfun-ai/step-3": {
                "family": "stepfun",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            "step3": {
                "family": "stepfun",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
        }

        # æ¨¡ç³ŠåŒ¹é…è§„åˆ™ï¼ˆæ›´æ–°ç‰ˆæœ¬ï¼‰
        self.fuzzy_patterns = [
            # ===== Thinking ç±»æ¨¡å‹ï¼ˆä¼˜å…ˆæ‹¦æˆªï¼‰=====
            {
                "pattern": r".*thinking.*",
                "family": "thinking",
                "multimodal": False,
                "limit": 200000,
                "image_tokens": 0,
                "special": "thinking",
            },
            # ===== GPT-5 / GPT-4 =====
            {
                "pattern": r"gpt-5.*nano.*",
                "family": "gpt",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1600,
            },
            {
                "pattern": r"gpt-5.*mini.*",
                "family": "gpt",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1800,
            },
            {
                "pattern": r"gpt-5.*",
                "family": "gpt",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 2000,
            },
            {
                "pattern": r"gpt-4o.*",
                "family": "gpt",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 1500,
            },
            {
                "pattern": r"gpt-4.*vision.*",
                "family": "gpt",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 1500,
            },
            {
                "pattern": r"gpt-4.*turbo.*",
                "family": "gpt",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 1500,
            },
            {
                "pattern": r"gpt-4.*",
                "family": "gpt",
                "multimodal": False,
                "limit": 8192,
                "image_tokens": 0,
            },
            {
                "pattern": r"gpt-3\.5.*",
                "family": "gpt",
                "multimodal": False,
                "limit": 16385,
                "image_tokens": 0,
            },
            {
                "pattern": r"gpt.*",
                "family": "gpt",
                "multimodal": False,
                "limit": 16385,
                "image_tokens": 0,
            },
            # ===== Claudeï¼ˆ4.1 å…ˆäº 4ï¼‰=====
            {
                "pattern": r"claude-?4\.1.*",
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            {
                "pattern": r"claude-?4(?!\.1).*",
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            # å…¼å®¹æ—§ 3.xï¼ˆä»ä¿ç•™ï¼‰
            {
                "pattern": r"claude.*3.*5.*sonnet.*",
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            {
                "pattern": r"claude.*3.*opus.*",
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            {
                "pattern": r"claude.*3.*haiku.*",
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 800,
            },
            {
                "pattern": r"claude.*3.*",
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            {
                "pattern": r"anthropic.*claude.*",
                "family": "claude",
                "multimodal": True,
                "limit": 200000,
                "image_tokens": 1000,
            },
            # ===== Huawei / Pangu =====
            {
                "pattern": r"pangu.*pro.*moe.*",
                "family": "pangu",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== Zhipu / GLM =====
            {
                "pattern": r"(?:zai-org/)?glm-4\.5.*",
                "family": "glm",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== Alibaba / Qwen =====
            {
                "pattern": r"qwen.*/?qwen3.*coder.*480b.*instruct.*",
                "family": "qwen",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            {
                "pattern": r"qwen.*/?qwen3.*coder.*a35b.*instruct.*",
                "family": "qwen",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            {
                "pattern": r"qwen.*vl.*",
                "family": "qwen",
                "multimodal": True,
                "limit": 32000,
                "image_tokens": 1000,
            },
            {
                "pattern": r"qwen.*",
                "family": "qwen",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== Tencent / Hunyuan =====
            {
                "pattern": r"hunyuan.*vision.*",
                "family": "hunyuan",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 800,
            },
            {
                "pattern": r"hunyuan.*",
                "family": "hunyuan",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== Moonshot / Kimi =====
            {
                "pattern": r"kimi.*k2.*instruct.*",
                "family": "kimi",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            {
                "pattern": r"kimi.*k2.*",
                "family": "kimi",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== Baidu / ERNIE =====
            {
                "pattern": r"ernie-?4(?:\.5)?(?:-turbo)?",
                "family": "ernie",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            {
                "pattern": r"ernie.*",
                "family": "ernie",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== stepfun =====
            {
                "pattern": r"(?:stepfun(-ai)?/)?step-?3.*",
                "family": "stepfun",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            # ===== ByteDance / Doubaoï¼ˆä¿ç•™åŸæ³›é…é¡ºåºï¼‰=====
            {
                "pattern": r"doubao.*vision.*",
                "family": "doubao",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 1500,
            },
            {
                "pattern": r"doubao.*seed.*",
                "family": "doubao",
                "multimodal": True,
                "limit": 50000,
                "image_tokens": 1000,
            },
            {
                "pattern": r"doubao.*256k.*",
                "family": "doubao",
                "multimodal": False,
                "limit": 200000,
                "image_tokens": 0,
            },
            {
                "pattern": r"doubao.*1\.5.*",
                "family": "doubao",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            {
                "pattern": r"doubao.*",
                "family": "doubao",
                "multimodal": False,
                "limit": 50000,
                "image_tokens": 0,
            },
            # ===== Google / Geminiï¼ˆä¿ç•™ï¼‰=====
            {
                "pattern": r"gemini.*vision.*",
                "family": "gemini",
                "multimodal": True,
                "limit": 128000,
                "image_tokens": 800,
            },
            {
                "pattern": r"gemini.*pro.*",
                "family": "gemini",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
            {
                "pattern": r"gemini.*",
                "family": "gemini",
                "multimodal": False,
                "limit": 128000,
                "image_tokens": 0,
            },
        ]

    def match_model(self, model_name: str) -> Dict[str, Any]:
        """æ™ºèƒ½åŒ¹é…æ¨¡å‹ä¿¡æ¯"""
        if not model_name:
            return {
                "family": "unknown",
                "multimodal": False,
                "limit": 200000,
                "image_tokens": 1500,
            }

        model_lower = model_name.lower().strip()

        # ç²¾ç¡®åŒ¹é…
        for exact_name, info in self.exact_matches.items():
            if exact_name.lower() == model_lower:
                return {**info, "matched_name": exact_name, "match_type": "exact"}

        # æ¨¡ç³ŠåŒ¹é…
        for pattern_info in self.fuzzy_patterns:
            pattern = pattern_info["pattern"]
            if re.match(pattern, model_lower):
                return {
                    "family": pattern_info["family"],
                    "multimodal": pattern_info["multimodal"],
                    "limit": pattern_info["limit"],
                    "image_tokens": pattern_info["image_tokens"],
                    "special": pattern_info.get("special"),
                    "matched_pattern": pattern,
                    "match_type": "fuzzy",
                }

        # é»˜è®¤åŒ¹é…
        return {
            "family": "unknown",
            "multimodal": False,
            "limit": 200000,
            "image_tokens": 1500,
            "match_type": "default",
        }


class TokenCalculator:
    """Tokenè®¡ç®—å™¨ - ä¼˜åŒ–å›¾ç‰‡tokenè®¡ç®—"""

    def __init__(self):
        self._encoding = None
        self.model_info = None

    def set_model_info(self, model_info: dict):
        """è®¾ç½®å½“å‰æ¨¡å‹ä¿¡æ¯"""
        self.model_info = model_info

    def get_encoding(self):
        """è·å–tiktokenç¼–ç å™¨"""
        if not TIKTOKEN_AVAILABLE:
            return None
        if self._encoding is None:
            try:
                self._encoding = tiktoken.get_encoding("cl100k_base")
            except Exception:
                pass
        return self._encoding

    def count_tokens(self, text: str) -> int:
        """ç®€åŒ–çš„tokenè®¡ç®—"""
        if not text:
            return 0

        encoding = self.get_encoding()
        if encoding:
            try:
                return len(encoding.encode(str(text)))
            except Exception:
                pass

        # ç®€å•fallback
        return len(str(text)) // 4

    def calculate_image_tokens(self, image_data: str) -> int:
        """ä¼˜åŒ–çš„å›¾ç‰‡tokenè®¡ç®— - æ ¹æ®æ¨¡å‹è°ƒæ•´"""
        # Ignore the actual image content; use the model's image token estimate.
        if self.model_info:
            return self.model_info.get("image_tokens", 1500)
        return 1500  # é»˜è®¤fallback


class InputCleaner:
    """è¾“å…¥æ¸…æ´—ä¸ä¸¥æ ¼å…œåº• - æ”¯æŒHTTPå›¾ç‰‡URL"""

    @staticmethod
    def clean_text_for_regex(text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬ç”¨äºæ­£åˆ™è¡¨è¾¾å¼ï¼Œé˜²æ­¢è¯­æ³•é”™è¯¯"""
        if not text:
            return ""
        try:
            # ç§»é™¤ä¸å¯è§åˆ†éš”ç¬¦
            text = text.replace("\u2028", " ").replace("\u2029", " ")
            # ç§»é™¤å…¶ä»–æ§åˆ¶å­—ç¬¦
            text = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]", "", text)
            # åŸºæœ¬æ¸…ç†
            text = text.replace("\n", " ").replace("\r", " ")
            text = re.sub(r"\s+", " ", text).strip()
            return text
        except Exception as e:
            print(f"âš ï¸ æ–‡æœ¬æ¸…ç†å¼‚å¸¸: {str(e)[:100]}")
            # æç«¯å…œåº•ï¼šåªä¿ç•™åŸºæœ¬å­—ç¬¦
            return "".join(c for c in str(text) if c.isprintable() or c.isspace())[
                :1000
            ]

    @staticmethod
    def validate_and_clean_image_url(image_url: str) -> Tuple[bool, str]:
        """éªŒè¯å¹¶æ¸…æ´—å›¾ç‰‡URL - æ”¯æŒHTTPå’Œdata URI"""
        if not image_url or not isinstance(image_url, str):
            return False, ""

        try:
            image_url = image_url.strip()

            # æ”¯æŒHTTP(S) URL
            if image_url.startswith(("http://", "https://")):
                return True, image_url

            # æ”¯æŒdata URI
            if image_url.startswith("data:"):
                if "base64," not in image_url:
                    return False, ""

                header, b64 = image_url.split("base64,", 1)
                # åªæ¥å—å›¾ç‰‡ MIME
                if not header.lower().startswith("data:image/"):
                    return False, ""

                # å»ç©ºç™½
                b64_str = re.sub(r"\s+", "", b64)
                # å¤ªçŸ­åŸºæœ¬ä¸å¯èƒ½æ˜¯æœ‰æ•ˆå›¾ç‰‡
                if len(b64_str) < 100:
                    return False, ""

                # æ ¡éªŒå‰100å­—ç¬¦ï¼ˆè¡¥é½ = é¿å… padding é”™ï¼‰
                head = b64_str[:100]
                pad_len = (-len(head)) % 4
                try:
                    base64.b64decode(head + ("=" * pad_len), validate=True)
                except Exception:
                    return False, ""

                # è¿”å›æ¸…æ´—åçš„ data uri
                return True, f"{header}base64,{b64_str}"

            return False, ""
        except Exception as e:
            print(f"âš ï¸ å›¾ç‰‡URLéªŒè¯å¼‚å¸¸: {str(e)[:100]}")
            return False, ""

    @staticmethod
    def safe_regex_match(pattern: str, text: str) -> bool:
        """å®‰å…¨çš„æ­£åˆ™åŒ¹é…ï¼Œä½¿ç”¨searchæ›¿ä»£match"""
        try:
            cleaned_text = InputCleaner.clean_text_for_regex(text)
            return re.search(pattern, cleaned_text) is not None
        except Exception as e:
            print(f"âš ï¸ æ­£åˆ™åŒ¹é…å¼‚å¸¸: {str(e)[:100]}")
            return False


class MessageChunker:
    """å•æ¡æ¶ˆæ¯å†…åˆ†ç‰‡å¤„ç†å™¨"""

    def __init__(self, token_calculator: TokenCalculator, valves):
        self.token_calculator = token_calculator
        self.valves = valves

    def should_chunk_message(self, message: dict) -> bool:
        """åˆ¤æ–­æ¶ˆæ¯æ˜¯å¦éœ€è¦åˆ†ç‰‡"""
        tokens = self.token_calculator.count_tokens(self.extract_text_content(message))
        return tokens > self.valves.large_message_threshold

    def extract_text_content(self, message: dict) -> str:
        """ä»æ¶ˆæ¯ä¸­æå–æ–‡æœ¬å†…å®¹"""
        content = message.get("content", "")
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
                elif item.get("type") == "image_url":
                    text_parts.append("[å›¾ç‰‡]")  # å ä½ç¬¦
            return " ".join(text_parts)
        else:
            return str(content)

    def chunk_single_message(
        self, message: dict, message_order: MessageOrder
    ) -> List[dict]:
        """å¯¹å•æ¡æ¶ˆæ¯è¿›è¡Œåˆ†ç‰‡å¤„ç†"""
        content_text = self.extract_text_content(message)

        if not self.should_chunk_message(message):
            return [message]  # ä¸éœ€è¦åˆ†ç‰‡

        # åˆ†ç‰‡ç­–ç•¥ï¼šä¿æŒä»£ç å—/æ®µè½/å¥å­å®Œæ•´
        chunks = self._intelligent_chunk_text(content_text)

        if len(chunks) <= 1:
            return [message]  # åˆ†ç‰‡ååªæœ‰ä¸€ä¸ªï¼Œç›´æ¥è¿”å›åŸæ¶ˆæ¯

        # åˆ›å»ºåˆ†ç‰‡æ¶ˆæ¯
        chunked_messages = []
        msg_id = message.get("_order_id", "unknown")

        for i, chunk_text in enumerate(chunks):
            chunk_id = message_order.generate_chunk_id(msg_id, i)

            chunk_message = copy.deepcopy(message)
            chunk_message["content"] = chunk_text
            chunk_message["_order_id"] = chunk_id
            chunk_message["_is_chunk"] = True
            chunk_message["_parent_msg_id"] = msg_id
            chunk_message["_chunk_index"] = i
            chunk_message["_total_chunks"] = len(chunks)

            chunked_messages.append(chunk_message)

        return chunked_messages

    def _intelligent_chunk_text(self, text: str) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†ç‰‡ï¼šä¿æŒå®Œæ•´æ€§ - ä¿®å¤æ¢è¡ŒæŠ¹æ‰é—®é¢˜"""
        if not text:
            return [text]

        # ä¿ç•™æ¢è¡Œç”¨äºæ®µè½åˆ‡åˆ†ï¼šä»…å»æ§åˆ¶å­—ç¬¦ï¼Œå¹¶æŠŠåˆ†éš”ç¬¦æ­£è§„åŒ–æˆæ¢è¡Œ
        text = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]", "", text)
        text = text.replace("\u2028", "\n").replace("\u2029", "\n")

        # åŸºæœ¬å‚æ•°
        target_size = self.valves.chunk_target_tokens * 4  # ç²—ç•¥å­—ç¬¦æ•°
        min_size = self.valves.chunk_min_tokens * 4
        max_size = self.valves.chunk_max_tokens * 4
        overlap_size = self.valves.chunk_overlap_tokens * 4

        chunks = []
        current_chunk = ""

        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r"\n\s*\n", text)

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            # å¦‚æœå½“å‰æ®µè½åŠ å…¥åè¶…è¿‡ç›®æ ‡å¤§å°
            if len(current_chunk) + len(paragraph) > target_size and current_chunk:
                # å®Œæˆå½“å‰chunk
                if len(current_chunk) >= min_size:
                    chunks.append(current_chunk.strip())
                    # æ·»åŠ é‡å å†…å®¹
                    if self.valves.chunk_overlap_tokens > 0:
                        overlap_text = (
                            current_chunk[-overlap_size:]
                            if len(current_chunk) > overlap_size
                            else current_chunk
                        )
                        current_chunk = overlap_text + "\n\n" + paragraph
                    else:
                        current_chunk = paragraph
                else:
                    current_chunk += "\n\n" + paragraph
            else:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph

            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œéœ€è¦å¥å­çº§åˆ‡åˆ†
            if len(current_chunk) > max_size:
                # ä¿å­˜å½“å‰chunk
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""

        # å¤„ç†æœ€åä¸€ä¸ªchunk
        if (
            current_chunk and len(current_chunk.strip()) >= min_size // 2
        ):  # æœ€åä¸€å—å¯ä»¥ç¨çŸ­
            chunks.append(current_chunk.strip())
        elif current_chunk and chunks:
            # å¤ªçŸ­äº†ï¼Œåˆå¹¶åˆ°æœ€åä¸€ä¸ªchunk
            chunks[-1] += "\n\n" + current_chunk.strip()
        elif current_chunk:
            chunks.append(current_chunk.strip())

        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªchunk
        if not chunks and text:
            chunks = [text]

        return chunks

    def preprocess_messages_with_chunking(
        self, messages: List[dict], message_order: MessageOrder
    ) -> List[dict]:
        """é¢„å¤„ç†æ¶ˆæ¯ï¼šå¯¹å¤§æ¶ˆæ¯è¿›è¡Œåˆ†ç‰‡"""
        processed_messages = []
        chunked_count = 0

        for message in messages:
            if self.should_chunk_message(message):
                chunked_messages = self.chunk_single_message(message, message_order)
                processed_messages.extend(chunked_messages)
                if len(chunked_messages) > 1:
                    chunked_count += 1
            else:
                processed_messages.append(message)

        return processed_messages


class CoveragePlanner:
    """Coverageè®¡åˆ’å™¨ - é‡æ„ç‰ˆï¼Œå®ç°è‡ªé€‚åº”åˆ†å—å’Œä¸€æ¬¡æ€§ç¼©æ”¾"""

    def __init__(self, token_calculator: TokenCalculator, valves):
        self.token_calculator = token_calculator
        self.valves = valves

    def plan_adaptive_coverage_summaries(
        self, scored_msgs: List[dict], total_budget: int
    ) -> Tuple[List[dict], int]:
        """è§„åˆ’è‡ªé€‚åº”è¦†ç›–æ‘˜è¦ï¼šæŒ‰åŸæ–‡tokené‡è‡ªé€‚åº”åˆ†å— + ä¸€æ¬¡æ€§æ¯”ä¾‹ç¼©æ”¾"""
        if not scored_msgs:
            return [], 0

        # 1. æŒ‰åˆ†æ•°åˆ†æ¡£
        HIGH, MID, LOW = self._classify_messages_by_score(scored_msgs)

        # 2. å¯¹ä½æƒé‡æ¶ˆæ¯è¿›è¡Œè‡ªé€‚åº”åˆ†å—
        adaptive_blocks = self._create_adaptive_blocks(LOW)

        # 3. è®¡ç®—ç†æƒ³é¢„ç®—éœ€æ±‚
        entries, ideal_total_cost = self._calculate_ideal_budgets(
            HIGH, MID, adaptive_blocks
        )

        # 4. ä¸€æ¬¡æ€§æ¯”ä¾‹ç¼©æ”¾ï¼ˆå¦‚æœé¢„ç®—ä¸è¶³ï¼‰æˆ–å‘ä¸Šæ‰©å¼ ï¼ˆå¦‚æœé¢„ç®—å……è¶³ï¼‰
        if ideal_total_cost > total_budget:
            entries, actual_cost = self._apply_proportional_scaling(
                entries, total_budget
            )
        else:
            # å‘ä¸Šæ‰©å¼ æ¨¡å¼ - ä¿®æ­£ç›®æ ‡ä½¿ç”¨ç‡
            entries, actual_cost = self._apply_upward_expansion(
                entries, total_budget, ideal_total_cost
            )

        # 5. æç«¯é€€åŒ–å¤„ç†
        if actual_cost > total_budget * 1.1:  # å…è®¸10%çš„å®¹å·®
            entries, actual_cost = self._apply_extreme_fallback(
                scored_msgs, total_budget
            )

        return entries, actual_cost

    def _classify_messages_by_score(
        self, scored_msgs: List[dict]
    ) -> Tuple[List[dict], List[dict], List[dict]]:
        """æŒ‰åˆ†æ•°åˆ†æ¡£æ¶ˆæ¯"""
        HIGH, MID, LOW = [], [], []
        for item in scored_msgs:
            if item["score"] >= self.valves.coverage_high_score_threshold:
                HIGH.append(item)
            elif item["score"] >= self.valves.coverage_mid_score_threshold:
                MID.append(item)
            else:
                LOW.append(item)
        return HIGH, MID, LOW

    def _create_adaptive_blocks(self, low_messages: List[dict]) -> List[dict]:
        """æŒ‰åŸæ–‡tokené‡è‡ªé€‚åº”åˆ†å—"""
        if not low_messages:
            return []

        # æŒ‰åŸå§‹ç´¢å¼•æ’åº
        low_sorted = sorted(low_messages, key=lambda x: x["idx"])
        blocks = []
        current_block = []
        current_tokens = 0
        raw_block_target = self.valves.raw_block_target  # 15k tokensç›®æ ‡

        for item in low_sorted:
            msg_tokens = item["tokens"]

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ‡å—
            should_cut_block = False

            # æ¡ä»¶1ï¼šç´¯è®¡tokensè¾¾åˆ°ç›®æ ‡
            if current_tokens + msg_tokens > raw_block_target and current_block:
                should_cut_block = True

            # æ¡ä»¶2ï¼šæ—¶é—´å¤§è·³ï¼ˆç›¸é‚»æ¶ˆæ¯ç´¢å¼•å·®è·å¤§ï¼‰
            if current_block and abs(item["idx"] - current_block[-1]["idx"]) > 5:
                should_cut_block = True

            # æ¡ä»¶3ï¼šè§’è‰²åˆ‡æ¢ï¼ˆç”¨æˆ·->åŠ©æ‰‹æˆ–åŠ©æ‰‹->ç”¨æˆ·ï¼‰
            if current_block:
                prev_role = current_block[-1]["msg"].get("role", "")
                curr_role = item["msg"].get("role", "")
                if (
                    prev_role != curr_role
                    and prev_role in ["user", "assistant"]
                    and curr_role in ["user", "assistant"]
                ):
                    should_cut_block = True

            # æ¡ä»¶4ï¼šç›¸ä¼¼åº¦çªå˜ï¼ˆåˆ†æ•°å·®å¼‚è¿‡å¤§ï¼‰
            if current_block:
                score_diff = abs(item["score"] - current_block[-1]["score"])
                if score_diff > 0.3:  # åˆ†æ•°å·®å¼‚è¶…è¿‡0.3å°±åˆ‡å—
                    should_cut_block = True

            # æ‰§è¡Œåˆ‡å—
            if should_cut_block:
                # å®Œæˆå½“å‰å—
                if current_block:
                    blocks.append(
                        {
                            "type": "adaptive_block",
                            "idx_range": (
                                current_block[0]["idx"],
                                current_block[-1]["idx"],
                            ),
                            "msgs": [item["msg"] for item in current_block],
                            "raw_tokens": current_tokens,
                            "avg_score": sum(item["score"] for item in current_block)
                            / len(current_block),
                            "msg_count": len(current_block),
                        }
                    )
                # å¼€å§‹æ–°å—
                current_block = [item]
                current_tokens = msg_tokens
            else:
                # åŠ å…¥å½“å‰å—
                current_block.append(item)
                current_tokens += msg_tokens

        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_block:
            blocks.append(
                {
                    "type": "adaptive_block",
                    "idx_range": (current_block[0]["idx"], current_block[-1]["idx"]),
                    "msgs": [item["msg"] for item in current_block],
                    "raw_tokens": current_tokens,
                    "avg_score": sum(item["score"] for item in current_block)
                    / len(current_block),
                    "msg_count": len(current_block),
                }
            )

        # å—åˆå¹¶ï¼šå¦‚æœå—æ•°å¤ªå¤šï¼Œåˆå¹¶å°å—
        if len(blocks) > self.valves.max_blocks:
            blocks = self._merge_small_blocks(blocks)

        return blocks

    def _merge_small_blocks(self, blocks: List[dict]) -> List[dict]:
        """åˆå¹¶å°å—ï¼Œæ§åˆ¶æ€»å—æ•°"""
        if len(blocks) <= self.valves.max_blocks:
            return blocks

        # æŒ‰raw_tokensæ’åºï¼Œä¼˜å…ˆåˆå¹¶å°å—
        blocks.sort(key=lambda x: x["raw_tokens"])
        merged_blocks = []
        i = 0

        while i < len(blocks):
            current_block = blocks[i]

            # å°è¯•ä¸ä¸‹ä¸€ä¸ªå—åˆå¹¶
            if (
                i + 1 < len(blocks)
                and len(merged_blocks) + (len(blocks) - i) > self.valves.max_blocks
            ):
                next_block = blocks[i + 1]

                # åˆå¹¶æ¡ä»¶ï¼šæ€»tokenæ•°ä¸è¶…è¿‡2å€ç›®æ ‡
                if (
                    current_block["raw_tokens"] + next_block["raw_tokens"]
                    <= self.valves.raw_block_target * 2
                ):
                    # æ‰§è¡Œåˆå¹¶
                    merged_block = {
                        "type": "adaptive_block",
                        "idx_range": (
                            current_block["idx_range"][0],
                            next_block["idx_range"][1],
                        ),
                        "msgs": current_block["msgs"] + next_block["msgs"],
                        "raw_tokens": current_block["raw_tokens"]
                        + next_block["raw_tokens"],
                        "avg_score": (
                            current_block["avg_score"] * current_block["msg_count"]
                            + next_block["avg_score"] * next_block["msg_count"]
                        )
                        / (current_block["msg_count"] + next_block["msg_count"]),
                        "msg_count": current_block["msg_count"]
                        + next_block["msg_count"],
                    }
                    merged_blocks.append(merged_block)
                    i += 2  # è·³è¿‡ä¸¤ä¸ªå—
                    continue

            merged_blocks.append(current_block)
            i += 1

        return merged_blocks

    def _calculate_ideal_budgets(
        self, high_msgs: List[dict], mid_msgs: List[dict], adaptive_blocks: List[dict]
    ) -> Tuple[List[dict], int]:
        """è®¡ç®—ç†æƒ³é¢„ç®—éœ€æ±‚"""
        entries = []
        total_cost = 0

        # é«˜æƒé‡å’Œä¸­æƒé‡ï¼šå¾®æ‘˜è¦
        for grp, per_token in [
            (high_msgs, self.valves.coverage_high_summary_tokens),
            (mid_msgs, self.valves.coverage_mid_summary_tokens),
        ]:
            for item in grp:
                msg_id = item["msg"].get("_order_id", f"msg_{item['idx']}")
                entry = {
                    "type": "micro",
                    "msg_id": msg_id,
                    "ideal_budget": per_token,
                    "floor_budget": max(self.valves.min_summary_tokens, per_token // 3),
                    "msg": item["msg"],
                    "score": item["score"],
                }
                entries.append(entry)
                total_cost += per_token

        # è‡ªé€‚åº”å—ï¼šå—é¢„ç®—æŒ‰å¤§å°åˆ†é…
        for block in adaptive_blocks:
            # åŸºç¡€é¢„ç®—
            floor_budget = max(
                self.valves.min_block_summary_tokens, self.valves.floor_block
            )

            # ç†æƒ³é¢„ç®—ï¼šåŸºç¡€ + æŒ‰åŸæ–‡tokené‡çš„æ¯”ä¾‹åˆ†é…
            size_factor = min(3.0, block["raw_tokens"] / self.valves.raw_block_target)
            ideal_budget = int(
                floor_budget
                + (self.valves.coverage_block_summary_tokens - floor_budget)
                * size_factor
            )

            block_key = f"block_{block['idx_range'][0]}_{block['idx_range'][1]}"
            entry = {
                "type": "adaptive_block",
                "block_key": block_key,
                "idx_range": block["idx_range"],
                "ideal_budget": ideal_budget,
                "floor_budget": floor_budget,
                "msgs": block["msgs"],
                "raw_tokens": block["raw_tokens"],
                "avg_score": block["avg_score"],
            }
            entries.append(entry)
            total_cost += ideal_budget

        return entries, total_cost

    def _apply_proportional_scaling(
        self, entries: List[dict], available_budget: int
    ) -> Tuple[List[dict], int]:
        """ä¸€æ¬¡æ€§æ¯”ä¾‹ç¼©æ”¾"""
        # è®¡ç®—æ€»çš„floorå’Œideal
        total_floors = sum(entry["floor_budget"] for entry in entries)
        total_ideals = sum(entry["ideal_budget"] for entry in entries)

        if total_floors > available_budget:
            # è¿flooréƒ½è¶…äº†ï¼Œæ‰§è¡Œæç«¯é€€åŒ–
            return self._apply_extreme_fallback_from_entries(entries, available_budget)

        # è®¡ç®—ç¼©æ”¾å› å­ Î± = (B - Î£floors) / Î£(ideal_i - floor_i)
        available_for_scaling = available_budget - total_floors
        scalable_amount = total_ideals - total_floors

        if scalable_amount <= 0:
            # æ‰€æœ‰æ¡ç›®éƒ½æ˜¯floorï¼Œæ— æ³•ç¼©æ”¾
            alpha = 0
        else:
            alpha = available_for_scaling / scalable_amount
            alpha = min(1.0, alpha)  # é™åˆ¶æœ€å¤§ä¸º1.0

        # åº”ç”¨ç¼©æ”¾ï¼šbudget_i' = floor_i + Î± * (ideal_i - floor_i)
        total_assigned = 0
        for entry in entries:
            floor_budget = entry["floor_budget"]
            ideal_budget = entry["ideal_budget"]
            scaled_budget = floor_budget + alpha * (ideal_budget - floor_budget)
            entry["budget"] = int(round(scaled_budget))
            total_assigned += entry["budget"]

        # è¯¯å·®æŠ¹å¹³ï¼šé«˜åˆ†å…ˆè¡¥ï¼Œä½åˆ†å…ˆæ‰£
        error = available_budget - total_assigned
        if error != 0:
            # æŒ‰åˆ†æ•°æ’åºï¼ˆé«˜åˆ†åœ¨å‰ï¼‰
            scored_entries = [
                (entry.get("score", entry.get("avg_score", 0)), entry)
                for entry in entries
            ]
            scored_entries.sort(key=lambda x: x[0], reverse=True)

            if error > 0:
                # æœ‰ä½™é¢ï¼Œé«˜åˆ†å…ˆè¡¥
                for _, entry in scored_entries:
                    if error <= 0:
                        break
                    entry["budget"] += 1
                    error -= 1
            else:
                # è¶…é¢„ç®—ï¼Œä½åˆ†å…ˆæ‰£
                for _, entry in reversed(scored_entries):
                    if error >= 0:
                        break
                    if entry["budget"] > entry["floor_budget"]:
                        entry["budget"] -= 1
                        error += 1

        final_cost = sum(entry["budget"] for entry in entries)
        return entries, final_cost

    def _apply_upward_expansion(
        self, entries: List[dict], available_budget: int, ideal_total_cost: int
    ) -> Tuple[List[dict], int]:
        """å‘ä¸Šæ‰©å¼ æ¨¡å¼ï¼šä¿®æ­£ç›®æ ‡ä½¿ç”¨ç‡ä¸valvesä¸€è‡´"""
        expansion_cap = 3.0  # æœ€å¤§æ‰©å¼ å€æ•°

        # ä½¿ç”¨valvesä¸­çš„ç›®æ ‡çª—å£ä½¿ç”¨ç‡ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 0.6
        target_usage = self.valves.target_window_usage  # 0.85

        # è®¡ç®—æ‰©å¼ å› å­
        target_cost = int(available_budget * target_usage)
        if ideal_total_cost >= target_cost:
            # ç†æƒ³æˆæœ¬å·²ç»å¤Ÿé«˜ï¼Œä¸éœ€è¦æ‰©å¼ 
            for entry in entries:
                entry["budget"] = entry["ideal_budget"]
            return entries, ideal_total_cost

        # è®¡ç®—æ‰©å¼ å€æ•°
        expansion_factor = min(expansion_cap, target_cost / ideal_total_cost)

        # ä¼˜å…ˆæ‰©å¼ å—æ‘˜è¦å’Œé«˜æƒé‡micro
        total_assigned = 0
        for entry in entries:
            base_budget = entry["ideal_budget"]
            if entry["type"] == "adaptive_block":
                # å—æ‘˜è¦ä¼˜å…ˆæ‰©å¼ 
                expanded_budget = int(base_budget * expansion_factor)
            elif (
                entry["type"] == "micro"
                and entry.get("score", 0) >= self.valves.coverage_high_score_threshold
            ):
                # é«˜æƒé‡microé€‚åº¦æ‰©å¼ 
                expanded_budget = int(base_budget * min(2.0, expansion_factor))
            else:
                # å…¶ä»–ä¿æŒåŸæ ·
                expanded_budget = base_budget

            entry["budget"] = expanded_budget
            total_assigned += expanded_budget

        # ç¡®ä¿ä¸è¶…é¢„ç®—
        if total_assigned > available_budget:
            # æŒ‰æ¯”ä¾‹ç¼©å°
            scale_down = available_budget / total_assigned
            for entry in entries:
                entry["budget"] = int(entry["budget"] * scale_down)
            total_assigned = sum(entry["budget"] for entry in entries)

        return entries, total_assigned

    def _apply_extreme_fallback(
        self, scored_msgs: List[dict], available_budget: int
    ) -> Tuple[List[dict], int]:
        """æç«¯é€€åŒ–ï¼šå•æ¡å…¨å±€å—æ‘˜è¦"""
        # ä½¿ç”¨90%çš„é¢„ç®—ä½œä¸ºå…¨å±€æ‘˜è¦é¢„ç®—ï¼Œç•™10%ä½œä¸ºç¼“å†²
        global_budget = max(
            self.valves.min_block_summary_tokens, int(available_budget * 0.9)
        )

        # æŒ‰ç´¢å¼•æ’åºæ‰€æœ‰æ¶ˆæ¯
        sorted_msgs = sorted(scored_msgs, key=lambda x: x["idx"])
        all_msgs = [item["msg"] for item in sorted_msgs]

        entry = {
            "type": "global_block",
            "block_key": f"global_0_{len(sorted_msgs)-1}",
            "idx_range": (0, len(sorted_msgs) - 1),
            "budget": global_budget,
            "msgs": all_msgs,
            "avg_score": sum(item["score"] for item in sorted_msgs) / len(sorted_msgs),
        }

        return [entry], global_budget

    def _apply_extreme_fallback_from_entries(
        self, entries: List[dict], available_budget: int
    ) -> Tuple[List[dict], int]:
        """ä»ç°æœ‰æ¡ç›®æ‰§è¡Œæç«¯é€€åŒ–"""
        # æ”¶é›†æ‰€æœ‰æ¶ˆæ¯
        all_msgs = []
        for entry in entries:
            if entry["type"] == "micro":
                all_msgs.append(entry["msg"])
            elif entry["type"] == "adaptive_block":
                all_msgs.extend(entry["msgs"])

        # æŒ‰åŸå§‹ç´¢å¼•æ’åº
        all_msgs.sort(key=lambda x: x.get("_original_index", 0))

        global_budget = max(
            self.valves.min_block_summary_tokens, int(available_budget * 0.9)
        )

        entry = {
            "type": "global_block",
            "block_key": f"global_0_{len(all_msgs)-1}",
            "idx_range": (0, len(all_msgs) - 1),
            "budget": global_budget,
            "msgs": all_msgs,
            "avg_score": 0.5,  # é»˜è®¤åˆ†æ•°
        }

        return [entry], global_budget


class Filter:
    class Valves(BaseModel):
        # åŸºç¡€æ§åˆ¶
        enable_processing: bool = Field(
            default=True, description="ğŸ”„ å¯ç”¨å†…å®¹æœ€å¤§åŒ–å¤„ç†"
        )
        excluded_models: str = Field(
            default="", description="ğŸš« æ’é™¤æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)"
        )

        # æ ¸å¿ƒé…ç½®
        max_window_utilization: float = Field(
            default=0.95, description="ğŸªŸ æœ€å¤§çª—å£åˆ©ç”¨ç‡(95%)"
        )
        aggressive_content_recovery: bool = Field(
            default=True, description="ğŸ”„ æ¿€è¿›å†…å®¹åˆå¹¶æ¨¡å¼"
        )
        min_preserve_ratio: float = Field(
            default=0.75, description="ğŸ”’ æœ€å°å†…å®¹ä¿ç•™æ¯”ä¾‹(75%)"
        )

        # Coverage-Firstç­–ç•¥é…ç½®
        enable_coverage_first: bool = Field(
            default=True, description="ğŸ¯ å¯ç”¨Coverage-Firstç­–ç•¥"
        )
        coverage_high_score_threshold: float = Field(
            default=0.7, description="ğŸ¯ é«˜æƒé‡é˜ˆå€¼(70%)"
        )
        coverage_mid_score_threshold: float = Field(
            default=0.4, description="ğŸ¯ ä¸­æƒé‡é˜ˆå€¼(40%)"
        )
        coverage_high_summary_tokens: int = Field(
            default=100, description="ğŸ“„ é«˜æƒé‡æ¶ˆæ¯å¾®æ‘˜è¦ç›®æ ‡tokens"
        )
        coverage_mid_summary_tokens: int = Field(
            default=50, description="ğŸ“„ ä¸­æƒé‡æ¶ˆæ¯å¾®æ‘˜è¦ç›®æ ‡tokens"
        )
        coverage_low_summary_tokens: int = Field(
            default=20, description="ğŸ“„ ä½æƒé‡æ¶ˆæ¯å¾®æ‘˜è¦ç›®æ ‡tokens"
        )
        coverage_block_summary_tokens: int = Field(
            default=350, description="ğŸ“š å—æ‘˜è¦ç›®æ ‡tokens"
        )
        coverage_upgrade_ratio: float = Field(
            default=0.3, description="â¬†ï¸ å‡çº§é¢„ç®—æ¯”ä¾‹(30%)"
        )

        # æ–°å¢ï¼šè‡ªé€‚åº”åˆ†å—é…ç½®
        raw_block_target: int = Field(
            default=15000, description="ğŸ§© è‡ªé€‚åº”å—ç›®æ ‡åŸæ–‡tokens"
        )
        floor_block: int = Field(default=300, description="ğŸ“ å—æ‘˜è¦æœ€å°é¢„ç®—tokens")
        max_blocks: int = Field(default=8, description="ğŸ“š æœ€å¤§å—æ•°é‡")
        upgrade_min_pct: float = Field(
            default=0.2, description="â¬†ï¸ å‡çº§æ± æœ€å°é¢„ç•™æ¯”ä¾‹(20%)"
        )

        # é›¶ä¸¢å¤±ä¿éšœé…ç½®
        enable_zero_loss_guarantee: bool = Field(
            default=True, description="ğŸ›¡ï¸ å¯ç”¨é›¶ä¸¢å¤±ä¿éšœ"
        )
        min_summary_tokens: int = Field(
            default=30, description="ğŸ“ æœ€å°å¾®æ‘˜è¦tokens(ä¿åº•)"
        )
        min_block_summary_tokens: int = Field(
            default=200, description="ğŸ“ æœ€å°å—æ‘˜è¦tokens(ä¿åº•)"
        )
        max_budget_adjustment_rounds: int = Field(
            default=5, description="ğŸ”§ æœ€å¤§é¢„ç®—è°ƒæ•´è½®æ¬¡"
        )
        disable_insurance_truncation: bool = Field(
            default=True, description="ğŸš« ç¦ç”¨ä¿é™©æˆªæ–­(å¼ºåˆ¶é›¶ä¸¢å¤±)"
        )

        # å°½é‡ä¿ç•™é…ç½®
        enable_try_preserve: bool = Field(
            default=True, description="ğŸ”’ å¯ç”¨å°½é‡ä¿ç•™æœºåˆ¶"
        )
        try_preserve_ratio: float = Field(
            default=0.40, description="ğŸ”’ å°½é‡ä¿ç•™é¢„ç®—æ¯”ä¾‹(40%)"
        )
        try_preserve_exchanges: int = Field(
            default=3, description="ğŸ”’ å°½é‡ä¿ç•™å¯¹è¯è½®æ¬¡æ•°"
        )

        # å“åº”ç©ºé—´é…ç½®
        response_buffer_ratio: float = Field(
            default=0.06, description="ğŸ“ å“åº”ç©ºé—´é¢„ç•™æ¯”ä¾‹(6%)"
        )
        response_buffer_max: int = Field(
            default=3000, description="ğŸ“ å“åº”ç©ºé—´æœ€å¤§å€¼(tokens)"
        )
        response_buffer_min: int = Field(
            default=1000, description="ğŸ“ å“åº”ç©ºé—´æœ€å°å€¼(tokens)"
        )

        # å¤šæ¨¡æ€å¤„ç†é…ç½®
        multimodal_direct_threshold: float = Field(
            default=0.70, description="ğŸ¯ å¤šæ¨¡æ€ç›´æ¥è¾“å…¥Tokené¢„ç®—é˜ˆå€¼(70%)"
        )
        preserve_images_in_multimodal: bool = Field(
            default=True, description="ğŸ“¸ å¤šæ¨¡æ€æ¨¡å‹æ˜¯å¦ä¿ç•™åŸå§‹å›¾ç‰‡"
        )
        always_process_images_before_summary: bool = Field(
            default=True, description="ğŸ“ æ‘˜è¦å‰æ€»æ˜¯å…ˆå¤„ç†å›¾ç‰‡"
        )

        # ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†é…ç½®
        enable_context_maximization: bool = Field(
            default=True, description="ğŸ“š å¯ç”¨ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†"
        )
        context_max_direct_preserve_ratio: float = Field(
            default=0.40, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ç›´æ¥ä¿ç•™æ¯”ä¾‹(40%)"
        )
        context_max_processing_ratio: float = Field(
            default=0.45, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†é¢„ç®—æ¯”ä¾‹(45%)"
        )
        context_max_fallback_ratio: float = Field(
            default=0.15, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å®¹é”™é¢„ç®—æ¯”ä¾‹(15%)"
        )
        context_max_skip_rag: bool = Field(
            default=True, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–è·³è¿‡RAGå¤„ç†"
        )
        context_max_prioritize_recent: bool = Field(
            default=True, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¼˜å…ˆä¿ç•™æœ€è¿‘å†…å®¹"
        )

        # å®¹é”™æœºåˆ¶é…ç½®
        enable_fallback_preservation: bool = Field(
            default=True, description="ğŸ›¡ï¸ å¯ç”¨å®¹é”™ä¿æŠ¤æœºåˆ¶"
        )
        fallback_preserve_ratio: float = Field(
            default=0.25, description="ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤é¢„ç•™æ¯”ä¾‹(25%)"
        )
        min_history_messages: int = Field(default=8, description="ğŸ›¡ï¸ æœ€å°‘å†å²æ¶ˆæ¯æ•°é‡")
        force_preserve_recent_user_exchanges: int = Field(
            default=3, description="ğŸ›¡ï¸ å¼ºåˆ¶ä¿ç•™æœ€è¿‘ç”¨æˆ·å¯¹è¯è½®æ¬¡"
        )

        # åŠŸèƒ½å¼€å…³
        enable_multimodal: bool = Field(default=True, description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å¤„ç†")
        enable_vision_preprocessing: bool = Field(
            default=True, description="ğŸ‘ï¸ å¯ç”¨å›¾ç‰‡é¢„å¤„ç†"
        )
        enable_vector_retrieval: bool = Field(
            default=True, description="ğŸ” å¯ç”¨å‘é‡æ£€ç´¢"
        )
        enable_intelligent_chunking: bool = Field(
            default=True, description="ğŸ§© å¯ç”¨æ™ºèƒ½åˆ†ç‰‡"
        )
        enable_recursive_summarization: bool = Field(
            default=True, description="ğŸ”„ å¯ç”¨é€’å½’æ‘˜è¦"
        )
        enable_reranking: bool = Field(default=True, description="ğŸ”„ å¯ç”¨é‡æ’åº")

        # æ™ºèƒ½å…³é”®å­—ç”Ÿæˆå’Œä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹
        enable_keyword_generation: bool = Field(
            default=True, description="ğŸ”‘ å¯ç”¨æ™ºèƒ½å…³é”®å­—ç”Ÿæˆ"
        )
        enable_ai_context_max_detection: bool = Field(
            default=True, description="ğŸ§  å¯ç”¨AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹"
        )
        keyword_generation_for_context_max: bool = Field(
            default=True, description="ğŸ”‘ å¯¹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¯ç”¨å…³é”®å­—ç”Ÿæˆ"
        )

        # ç»Ÿè®¡å’Œè°ƒè¯•
        enable_detailed_stats: bool = Field(default=True, description="ğŸ“Š å¯ç”¨è¯¦ç»†ç»Ÿè®¡")
        enable_detailed_progress: bool = Field(
            default=True, description="ğŸ“± å¯ç”¨è¯¦ç»†è¿›åº¦æ˜¾ç¤º"
        )
        debug_level: int = Field(
            default=0, description="ğŸ› è°ƒè¯•çº§åˆ« 0-3"
        )  # ç®€åŒ–æ—¥å¿—ï¼šé»˜è®¤ä¸º0
        show_frontend_progress: bool = Field(
            default=True, description="ğŸ“± æ˜¾ç¤ºå¤„ç†è¿›åº¦"
        )

        # APIé…ç½®
        api_error_retry_times: int = Field(default=2, description="ğŸ”„ APIé”™è¯¯é‡è¯•æ¬¡æ•°")
        api_error_retry_delay: float = Field(
            default=1.0, description="â±ï¸ APIé”™è¯¯é‡è¯•å»¶è¿Ÿ(ç§’)"
        )

        # Tokenç®¡ç†
        default_token_limit: int = Field(default=200000, description="âš–ï¸ é»˜è®¤tokené™åˆ¶")
        token_safety_ratio: float = Field(
            default=0.92, description="ğŸ›¡ï¸ Tokenå®‰å…¨æ¯”ä¾‹(92%)"
        )
        target_window_usage: float = Field(
            default=0.85, description="ğŸªŸ ç›®æ ‡çª—å£ä½¿ç”¨ç‡(85%)"
        )
        max_processing_iterations: int = Field(
            default=5, description="ğŸ”„ æœ€å¤§å¤„ç†è¿­ä»£æ¬¡æ•°"
        )

        # ä¿æŠ¤ç­–ç•¥
        force_preserve_current_user_message: bool = Field(
            default=True, description="ğŸ”’ å¼ºåˆ¶ä¿ç•™å½“å‰ç”¨æˆ·æ¶ˆæ¯(æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯)"
        )
        preserve_recent_exchanges: int = Field(
            default=4, description="ğŸ’¬ ä¿æŠ¤æœ€è¿‘å®Œæ•´å¯¹è¯è½®æ¬¡"
        )
        max_preserve_ratio: float = Field(
            default=0.3, description="ğŸ”’ ä¿æŠ¤æ¶ˆæ¯æœ€å¤§tokenæ¯”ä¾‹"
        )
        max_single_message_tokens: int = Field(
            default=20000, description="ğŸ“ å•æ¡æ¶ˆæ¯æœ€å¤§token"
        )

        # æ™ºèƒ½åˆ†ç‰‡é…ç½®
        enable_smart_chunking: bool = Field(default=True, description="ğŸ§© å¯ç”¨æ™ºèƒ½åˆ†ç‰‡")
        chunk_target_tokens: int = Field(default=4000, description="ğŸ§© åˆ†ç‰‡ç›®æ ‡tokenæ•°")
        chunk_overlap_tokens: int = Field(default=300, description="ğŸ”— åˆ†ç‰‡é‡å tokenæ•°")
        chunk_min_tokens: int = Field(default=1000, description="ğŸ“ åˆ†ç‰‡æœ€å°tokenæ•°")
        chunk_max_tokens: int = Field(default=4000, description="ğŸ“ åˆ†ç‰‡æœ€å¤§tokenæ•°")
        large_message_threshold: int = Field(
            default=10000, description="ğŸ“ å¤§æ¶ˆæ¯åˆ†ç‰‡é˜ˆå€¼"
        )
        preserve_paragraph_integrity: bool = Field(
            default=True, description="ğŸ“ ä¿æŒæ®µè½å®Œæ•´æ€§"
        )
        preserve_sentence_integrity: bool = Field(
            default=True, description="ğŸ“ ä¿æŒå¥å­å®Œæ•´æ€§"
        )
        preserve_code_blocks: bool = Field(
            default=True, description="ğŸ’» ä¿æŒä»£ç å—å®Œæ•´æ€§"
        )

        # å†…å®¹ä¼˜å…ˆçº§è®¾ç½®
        high_priority_content: str = Field(
            default="ä»£ç ,é…ç½®,å‚æ•°,æ•°æ®,é”™è¯¯,è§£å†³æ–¹æ¡ˆ,æ­¥éª¤,æ–¹æ³•,æŠ€æœ¯ç»†èŠ‚,API,å‡½æ•°,ç±»,å˜é‡,é—®é¢˜,bug,ä¿®å¤,å®ç°,ç®—æ³•,æ¶æ„,ç”¨æˆ·é—®é¢˜,å…³é”®å›ç­”",
            description="ğŸ¯ é«˜ä¼˜å…ˆçº§å†…å®¹å…³é”®è¯(é€—å·åˆ†éš”)",
        )

        # ç»Ÿä¸€çš„APIé…ç½®
        api_base: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸ”— APIåŸºç¡€åœ°å€",
        )
        api_key: str = Field(default="", description="ğŸ”‘ APIå¯†é’¥")

        # å¤šæ¨¡æ€æ¨¡å‹é…ç½®
        multimodal_model: str = Field(
            default="doubao-1.5-vision-pro-250328", description="ğŸ–¼ï¸ å¤šæ¨¡æ€æ¨¡å‹"
        )

        # æ–‡æœ¬æ¨¡å‹é…ç½®
        text_model: str = Field(
            default="doubao-1-5-lite-32k-250115", description="ğŸ“ æ–‡æœ¬å¤„ç†æ¨¡å‹"
        )

        # å‘é‡æ¨¡å‹é…ç½®
        text_vector_model: str = Field(
            default="doubao-embedding-large-text-250515", description="ğŸ§  æ–‡æœ¬å‘é‡æ¨¡å‹"
        )
        multimodal_vector_model: str = Field(
            default="doubao-embedding-vision-250615", description="ğŸ§  å¤šæ¨¡æ€å‘é‡æ¨¡å‹"
        )

        # Visionç›¸å…³é…ç½®
        vision_prompt_template: str = Field(
            default="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€æ–‡å­—ã€é¢œè‰²ã€å¸ƒå±€ç­‰æ‰€æœ‰å¯è§ä¿¡æ¯ã€‚ç‰¹åˆ«æ³¨æ„ä»£ç ã€é…ç½®ã€æ•°æ®ç­‰æŠ€æœ¯ä¿¡æ¯ã€‚ä¿æŒå®¢è§‚å‡†ç¡®ï¼Œé‡ç‚¹çªå‡ºå…³é”®ä¿¡æ¯ã€‚å¦‚æœå›¾ç‰‡åŒ…å«æ–‡å­—å†…å®¹ï¼Œè¯·å®Œæ•´è½¬å½•å‡ºæ¥ã€‚",
            description="ğŸ‘ï¸ Visionæç¤ºè¯",
        )
        vision_max_tokens: int = Field(
            default=2500, description="ğŸ‘ï¸ Visionæœ€å¤§è¾“å‡ºtokens"
        )

        # å…³é”®å­—ç”Ÿæˆé…ç½®
        keyword_generation_prompt: str = Field(
            default="""ä½ æ˜¯ä¸“ä¸šçš„æœç´¢å…³é”®å­—ç”ŸæˆåŠ©æ‰‹ã€‚ç”¨æˆ·è¾“å…¥äº†ä¸€ä¸ªæŸ¥è¯¢ï¼Œä½ éœ€è¦ç”Ÿæˆå¤šä¸ªç›¸å…³çš„æœç´¢å…³é”®å­—æ¥å¸®åŠ©åœ¨å¯¹è¯å†å²ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚

ğŸ“‹ ä»»åŠ¡è¦æ±‚ï¼š
1. åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾å’Œä¸»é¢˜
2. ç”Ÿæˆ5-10ä¸ªç›¸å…³çš„æœç´¢å…³é”®å­—
3. åŒ…å«åŒä¹‰è¯ã€ç›¸å…³è¯ã€æŠ€æœ¯æœ¯è¯­
4. å¯¹äºå®½æ³›æŸ¥è¯¢ï¼ˆå¦‚"èŠäº†ä»€ä¹ˆ"ã€"è¯´äº†ä»€ä¹ˆ"ï¼‰ï¼Œç”Ÿæˆé€šç”¨ä½†æœ‰æ•ˆçš„å…³é”®å­—
5. å…³é”®å­—åº”è¯¥èƒ½è¦†ç›–å¯èƒ½çš„å¯¹è¯ä¸»é¢˜

ğŸ“ è¾“å‡ºæ ¼å¼ï¼š
ç›´æ¥è¾“å‡ºå…³é”®å­—ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

ç°åœ¨è¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”Ÿæˆå…³é”®å­—ï¼š""",
            description="ğŸ”‘ å…³é”®å­—ç”Ÿæˆæç¤ºè¯",
        )

        # ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹é…ç½®
        context_max_detection_prompt: str = Field(
            default="""ä½ æ˜¯ä¸“ä¸šçš„æŸ¥è¯¢æ„å›¾åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢æ˜¯å¦éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†ã€‚

ğŸ“‹ åˆ¤æ–­æ ‡å‡†ï¼š
éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„æŸ¥è¯¢ç‰¹å¾ï¼š
- è¯¢é—®"èŠäº†ä»€ä¹ˆ"ã€"è¯´äº†ä»€ä¹ˆ"ã€"è®¨è®ºäº†ä»€ä¹ˆ"ç­‰å®½æ³›å†…å®¹
- è¯¢é—®"ä¹‹å‰çš„å†…å®¹"ã€"å†å²è®°å½•"ã€"å¯¹è¯å†å²"ç­‰
- ç¼ºä¹å…·ä½“çš„ä¸»é¢˜ã€å…³é”®è¯æˆ–æ˜ç¡®çš„æœç´¢æ„å›¾
- æŸ¥è¯¢è¯æ±‡å°‘äº3ä¸ªæœ‰æ•ˆè¯æ±‡

ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„æŸ¥è¯¢ç‰¹å¾ï¼š
- åŒ…å«æ˜ç¡®çš„ä¸»é¢˜ã€æŠ€æœ¯æœ¯è¯­ã€äº§å“åç§°ç­‰
- æœ‰å…·ä½“çš„é—®é¢˜æŒ‡å‘
- åŒ…å«è¯¦ç»†çš„æè¿°æˆ–èƒŒæ™¯ä¿¡æ¯

ğŸ“ è¾“å‡ºæ ¼å¼ï¼š
åªè¾“å‡º "éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–" æˆ– "ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–"ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

ç°åœ¨è¯·åˆ†æä»¥ä¸‹æŸ¥è¯¢ï¼š""",
            description="ğŸ§  ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹æç¤ºè¯",
        )

        # å‘é‡æ£€ç´¢é…ç½®
        vector_similarity_threshold: float = Field(
            default=0.06, description="ğŸ¯ åŸºç¡€ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        multimodal_similarity_threshold: float = Field(
            default=0.04, description="ğŸ–¼ï¸ å¤šæ¨¡æ€ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        text_similarity_threshold: float = Field(
            default=0.08, description="ğŸ“ æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        vector_top_k: int = Field(default=150, description="ğŸ” å‘é‡æ£€ç´¢Top-Kæ•°é‡")

        # é‡æ’åºAPIé…ç½®
        rerank_api_base: str = Field(
            default="https://api.bochaai.com", description="ğŸ”„ é‡æ’åºAPI"
        )
        rerank_api_key: str = Field(default="", description="ğŸ”‘ é‡æ’åºå¯†é’¥")
        rerank_model: str = Field(default="gte-rerank", description="ğŸ§  é‡æ’åºæ¨¡å‹")
        rerank_top_k: int = Field(default=100, description="ğŸ” é‡æ’åºè¿”å›æ•°é‡")

        # æ‘˜è¦é…ç½®
        max_summary_length: int = Field(default=25000, description="ğŸ“ æ‘˜è¦æœ€å¤§é•¿åº¦")
        min_summary_ratio: float = Field(
            default=0.30, description="ğŸ“ æ‘˜è¦æœ€å°é•¿åº¦æ¯”ä¾‹"
        )
        summary_compression_ratio: float = Field(
            default=0.40, description="ğŸ“Š æ‘˜è¦å‹ç¼©æ¯”ä¾‹"
        )
        max_recursion_depth: int = Field(default=3, description="ğŸ”„ æœ€å¤§é€’å½’æ·±åº¦")

        # æ€§èƒ½é…ç½®
        max_concurrent_requests: int = Field(default=6, description="âš¡ æœ€å¤§å¹¶å‘æ•°")
        request_timeout: int = Field(default=90, description="â±ï¸ è¯·æ±‚è¶…æ—¶(ç§’)")

        # ç¼“å­˜é…ç½®
        enable_embedding_cache: bool = Field(
            default=True, description="ğŸ’¾ å¯ç”¨å‘é‡ç¼“å­˜"
        )
        cache_max_size: int = Field(default=1000, description="ğŸ’¾ ç¼“å­˜æœ€å¤§æ¡æ•°")

    def __init__(self):
        print("ğŸ“ Coverage-Firstä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–ä¸­...")
        self.valves = self.Valves()

        # åˆå§‹åŒ–ç»„ä»¶
        self.model_matcher = ModelMatcher()
        self.token_calculator = TokenCalculator()
        self.input_cleaner = InputCleaner()
        self.message_chunker = MessageChunker(self.token_calculator, self.valves)
        self.coverage_planner = CoveragePlanner(self.token_calculator, self.valves)

        # åˆå§‹åŒ–ç¼“å­˜
        if self.valves.enable_embedding_cache:
            self.embedding_cache = EmbeddingCache(self.valves.cache_max_size)
        else:
            self.embedding_cache = None

        # å¤„ç†ç»Ÿè®¡
        self.stats = ProcessingStats()

        # æ¶ˆæ¯é¡ºåºç®¡ç†å™¨
        self.message_order = None
        self.current_processing_id = None
        self.current_user_message = None
        self.current_model_info = None

        # è§£æé…ç½®
        self._parse_configurations()

        print("âœ… åˆå§‹åŒ–å®Œæˆ - Coverage-Firstä¸Šä¸‹æ–‡ç®¡ç†å™¨")

    def _parse_configurations(self):
        """è§£æé…ç½®é¡¹"""
        # è§£æé«˜ä¼˜å…ˆçº§å†…å®¹å…³é”®è¯
        self.high_priority_keywords = set()
        if self.valves.high_priority_content:
            self.high_priority_keywords = {
                keyword.strip().lower()
                for keyword in self.valves.high_priority_content.split(",")
                if keyword.strip()
            }

    def reset_processing_state(self):
        """é‡ç½®å¤„ç†çŠ¶æ€"""
        self.current_processing_id = None
        self.message_order = None
        self.current_user_message = None
        self.current_model_info = None
        self.stats = ProcessingStats()

        # æ¸…ç†ç¼“å­˜
        if self.embedding_cache:
            self.embedding_cache.clear()

    def debug_log(self, level: int, message: str, emoji: str = "ğŸ”§"):
        """åˆ†çº§è°ƒè¯•æ—¥å¿— - ç®€åŒ–ç‰ˆæœ¬"""
        if self.valves.debug_level >= level:
            prefix = ["", "ğŸ›[DEBUG]", "ğŸ”[DETAIL]", "ğŸ“‹[VERBOSE]"][min(level, 3)]
            # åŸºæœ¬æ¸…ç†
            message = self.input_cleaner.clean_text_for_regex(message)
            print(f"{prefix} {emoji} {message}")

    def is_model_excluded(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«æ’é™¤"""
        if not self.valves.excluded_models or not model_name:
            return False

        excluded_list = [
            model.strip().lower()
            for model in self.valves.excluded_models.split(",")
            if model.strip()
        ]

        if not excluded_list:
            return False

        model_lower = model_name.lower()
        for excluded_model in excluded_list:
            if excluded_model in model_lower:
                self.debug_log(1, f"æ¨¡å‹ {model_name} åœ¨æ’é™¤åˆ—è¡¨ä¸­", "ğŸš«")
                return True

        return False

    def analyze_model(self, model_name: str) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹ä¿¡æ¯ - å¢åŠ å‰ç«¯æ˜¾ç¤º"""
        model_info = self.model_matcher.match_model(model_name)

        # è®¾ç½®tokenè®¡ç®—å™¨çš„æ¨¡å‹ä¿¡æ¯
        self.token_calculator.set_model_info(model_info)

        # å‰ç«¯æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        multimodal_status = "å¤šæ¨¡æ€" if model_info["multimodal"] else "æ–‡æœ¬"
        family_name = model_info["family"].upper()
        tokens_display = f"{model_info['limit']:,}tokens"
        match_type_display = {"exact": "ç²¾ç¡®", "fuzzy": "æ¨¡ç³Š", "default": "é»˜è®¤"}[
            model_info["match_type"]
        ]

        print(f"ğŸ¯ æ¨¡å‹è¯†åˆ«: {model_name}")
        print(f"   â”œâ”€ ç³»åˆ—: {family_name}")
        print(f"   â”œâ”€ ç±»å‹: {multimodal_status}")
        print(f"   â”œâ”€ é™åˆ¶: {tokens_display}")
        print(f"   â””â”€ åŒ¹é…: {match_type_display}åŒ¹é…")

        # ç‰¹æ®Šæ¨¡å‹æç¤º
        if model_info.get("special") == "thinking":
            print(f"   ğŸ’­ ç‰¹æ®Š: Thinkingæ¨¡å‹")

        # GPT-5ç³»åˆ—ç‰¹åˆ«æç¤º
        if model_info.get("family") == "gpt" and "gpt-5" in model_name.lower():
            print(f"   ğŸ†• æ–°æ¨¡å‹: GPT-5ç³»åˆ— (200k tokens + å¤šæ¨¡æ€)")

        return model_info

    def count_tokens(self, text: str) -> int:
        """ç®€åŒ–çš„tokenè®¡ç®—"""
        if not text:
            return 0
        return self.token_calculator.count_tokens(text)

    def count_message_tokens(self, message: dict) -> int:
        """è®¡ç®—å•æ¡æ¶ˆæ¯çš„tokenæ•°é‡"""
        if not message:
            return 0

        content = message.get("content", "")
        role = message.get("role", "")
        total_tokens = 0

        # å¤„ç†å¤šæ¨¡æ€å†…å®¹
        if isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    text = item.get("text", "")
                    total_tokens += self.count_tokens(text)
                elif item.get("type") == "image_url":
                    total_tokens += self.token_calculator.calculate_image_tokens("")
        else:
            total_tokens = self.count_tokens(content)

        # åŠ ä¸Šè§’è‰²å’Œæ ¼å¼å¼€é”€
        total_tokens += self.count_tokens(role) + 20

        return total_tokens

    def count_messages_tokens(self, messages: List[dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€»tokenæ•°é‡"""
        if not messages:
            return 0

        total_tokens = sum(self.count_message_tokens(msg) for msg in messages)
        self.debug_log(
            2,
            f"æ¶ˆæ¯åˆ—è¡¨tokenè®¡ç®—: {len(messages)}æ¡æ¶ˆæ¯ -> {total_tokens:,}tokens",
            "ğŸ“Š",
        )
        return total_tokens

    def get_model_token_limit(self, model_name: str) -> int:
        """è·å–æ¨¡å‹çš„tokené™åˆ¶ï¼ˆåº”ç”¨å®‰å…¨ç³»æ•°ï¼‰"""
        model_info = self.analyze_model(model_name)
        limit = model_info.get("limit", self.valves.default_token_limit)
        safe_limit = int(limit * self.valves.token_safety_ratio)
        self.debug_log(
            2, f"æ¨¡å‹tokené™åˆ¶: {model_name} -> {limit} -> {safe_limit}", "âš–ï¸"
        )
        return safe_limit

    def is_multimodal_model(self, model_name: str) -> bool:
        """åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€è¾“å…¥"""
        model_info = self.analyze_model(model_name)
        return model_info.get("multimodal", False)

    def find_current_user_message(self, messages: List[dict]) -> Optional[dict]:
        """æŸ¥æ‰¾å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€æ–°çš„ç”¨æˆ·è¾“å…¥ï¼‰"""
        if not messages:
            return None

        for msg in reversed(messages):
            if msg.get("role") == "user":
                self.debug_log(
                    2,
                    f"æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯: {len(self.extract_text_from_content(msg.get('content', '')))}å­—ç¬¦",
                    "ğŸ’¬",
                )
                return msg

        return None

    def separate_current_and_history_messages(
        self, messages: List[dict]
    ) -> Tuple[Optional[dict], List[dict]]:
        """åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯"""
        if not messages:
            return None, []

        # æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€æ–°çš„ç”¨æˆ·è¾“å…¥ï¼‰
        current_user_message = None
        current_user_index = -1

        # ä»åå¾€å‰æŸ¥æ‰¾æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if msg.get("role") == "user":
                current_user_message = msg
                current_user_index = i
                break

        if not current_user_message:
            self.debug_log(1, "æœªæ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼Œæ‰€æœ‰æ¶ˆæ¯ä½œä¸ºå†å²æ¶ˆæ¯å¤„ç†", "âš ï¸")
            return None, messages

        # åˆ†ç¦»å†å²æ¶ˆæ¯ï¼ˆå½“å‰ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰çš„æ‰€æœ‰æ¶ˆæ¯ï¼‰
        history_messages = messages[:current_user_index]

        # æ›´æ–°ç»Ÿè®¡
        self.stats.history_message_separation_count += 1

        self.debug_log(
            1,
            f"æ¶ˆæ¯åˆ†ç¦»å®Œæˆ: å½“å‰ç”¨æˆ·æ¶ˆæ¯1æ¡({self.count_message_tokens(current_user_message)}tokens), å†å²æ¶ˆæ¯{len(history_messages)}æ¡({self.count_messages_tokens(history_messages):,}tokens)",
            "ğŸ“‹",
        )

        return current_user_message, history_messages

    def calculate_target_tokens(self, model_name: str, current_user_tokens: int) -> int:
        """è®¡ç®—ç›®æ ‡tokenæ•°ï¼šæ¨¡å‹é™åˆ¶ - å½“å‰ç”¨æˆ·æ¶ˆæ¯ - å“åº”ç©ºé—´"""
        model_token_limit = self.get_model_token_limit(model_name)

        # è®¡ç®—å“åº”ç©ºé—´
        response_buffer = min(
            self.valves.response_buffer_max,
            max(
                self.valves.response_buffer_min,
                int(model_token_limit * self.valves.response_buffer_ratio),
            ),
        )

        # è®¡ç®—ç›®æ ‡ï¼šæ€»é™åˆ¶ - å½“å‰ç”¨æˆ·æ¶ˆæ¯ - å“åº”ç¼“å†²åŒº
        target_tokens = model_token_limit - current_user_tokens - response_buffer

        # ç¡®ä¿ä¸å°äºåŸºç¡€å€¼
        min_target = max(10000, model_token_limit * 0.3)
        target_tokens = max(target_tokens, min_target)

        self.debug_log(
            1,
            f"ç›®æ ‡tokenè®¡ç®—: {model_token_limit} - {current_user_tokens} - {response_buffer} = {target_tokens}",
            "ğŸ¯",
        )

        return int(target_tokens)

    # ========== å¤šæ¨¡æ€å¤„ç† ==========
    def has_images_in_content(self, content) -> bool:
        """æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        if isinstance(content, list):
            return any(item.get("type") == "image_url" for item in content)
        return False

    def has_images_in_messages(self, messages: List[dict]) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯åˆ—è¡¨ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        return any(self.has_images_in_content(msg.get("content")) for msg in messages)

    def extract_text_from_content(self, content) -> str:
        """ä»å†…å®¹ä¸­æå–æ–‡æœ¬"""
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if item.get("type") == "text":
                    text = item.get("text", "")
                    text_parts.append(text)
            return " ".join(text_parts)
        else:
            return str(content) if content else ""

    def extract_images_from_content(self, content) -> List[dict]:
        """ä»å†…å®¹ä¸­æå–å›¾ç‰‡ä¿¡æ¯"""
        if isinstance(content, list):
            images = []
            for item in content:
                if item.get("type") == "image_url":
                    images.append(item)
            return images
        return []

    def is_high_priority_content(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜ä¼˜å…ˆçº§å†…å®¹"""
        if not text or not self.high_priority_keywords:
            return False

        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.high_priority_keywords)

    # ========== APIå®¢æˆ·ç«¯ç®¡ç† ==========
    def get_api_client(self, client_type: str = "default"):
        """è·å–APIå®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE:
            return None

        if self.valves.api_key:
            return AsyncOpenAI(
                base_url=self.valves.api_base,
                api_key=self.valves.api_key,
                timeout=self.valves.request_timeout,
            )
        return None

    # ========== å®‰å…¨APIè°ƒç”¨ ==========
    async def safe_api_call(self, call_func, call_name: str, *args, **kwargs):
        """å®‰å…¨çš„APIè°ƒç”¨åŒ…è£…å™¨"""
        for attempt in range(self.valves.api_error_retry_times + 1):
            try:
                result = await call_func(*args, **kwargs)
                return result
            except Exception as e:
                error_msg = str(e)
                self.stats.api_failures += 1

                if attempt < self.valves.api_error_retry_times:
                    self.debug_log(
                        1,
                        f"{call_name} ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{self.valves.api_error_retry_delay}ç§’åé‡è¯•",
                        "ğŸ”„",
                    )
                    await asyncio.sleep(self.valves.api_error_retry_delay)
                else:
                    self.debug_log(1, f"{call_name} æœ€ç»ˆå¤±è´¥: {error_msg[:100]}", "âŒ")
                    return None

        return None

    # ========== ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹ ==========
    async def detect_context_max_need_impl(self, query_text: str, event_emitter):
        """å®é™…çš„ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹å®ç°"""
        client = self.get_api_client()
        if not client:
            return None

        # ä½¿ç”¨è¾“å…¥æ¸…æ´—
        cleaned_query = self.input_cleaner.clean_text_for_regex(query_text)

        # æ„å»ºæç¤º
        prompt = f"{self.valves.context_max_detection_prompt}\n\n{cleaned_query}"

        response = await client.chat.completions.create(
            model=self.valves.text_model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=50,
            temperature=0.1,
            timeout=self.valves.request_timeout,
        )

        if response.choices and response.choices[0].message.content:
            result = response.choices[0].message.content.strip()
            result = self.input_cleaner.clean_text_for_regex(result)
            need_context_max = "éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–" in result
            self.debug_log(
                2, f"AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹ç»“æœ: {result} -> {need_context_max}", "ğŸ§ "
            )
            return need_context_max

        return None

    async def detect_context_max_need(self, query_text: str, event_emitter) -> bool:
        """ä½¿ç”¨AIæ£€æµ‹æ˜¯å¦éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–"""
        if not self.valves.enable_ai_context_max_detection:
            return self.is_context_max_need_simple(query_text)

        self.debug_log(1, f"AIæ£€æµ‹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚: {query_text[:50]}...", "ğŸ§ ")

        need_context_max = await self.safe_api_call(
            self.detect_context_max_need_impl,
            "ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹",
            query_text,
            event_emitter,
        )

        if need_context_max is not None:
            self.stats.context_maximization_detections += 1
            self.debug_log(
                1,
                f"AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹å®Œæˆ: {'éœ€è¦' if need_context_max else 'ä¸éœ€è¦'}",
                "ğŸ§ ",
            )
            return need_context_max
        else:
            # AIæ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•
            self.debug_log(1, f"AIæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•", "âš ï¸")
            return self.is_context_max_need_simple(query_text)

    def is_context_max_need_simple(self, query_text: str) -> bool:
        """ç®€å•çš„ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚åˆ¤æ–­ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        if not query_text:
            return True

        query_text = self.input_cleaner.clean_text_for_regex(query_text)

        # éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„ç‰¹å¾
        context_max_patterns = [
            r".*èŠ.*ä»€ä¹ˆ.*",
            r".*è¯´.*ä»€ä¹ˆ.*",
            r".*è®¨è®º.*ä»€ä¹ˆ.*",
            r".*è°ˆ.*ä»€ä¹ˆ.*",
            r".*å†…å®¹.*",
            r".*è¯é¢˜.*",
            r".*å†å².*",
            r".*è®°å½•.*",
            r".*ä¹‹å‰.*",
            r"what.*discuss.*",
            r"what.*talk.*",
            r"what.*chat.*",
            r".*conversation.*",
            r".*history.*",
        ]

        query_lower = query_text.lower()
        for pattern in context_max_patterns:
            if self.input_cleaner.safe_regex_match(pattern, query_lower):
                return True

        return len(query_text.split()) <= 3

    # ========== å…³é”®å­—ç”Ÿæˆ ==========
    async def generate_keywords_impl(self, query_text: str, event_emitter):
        """å®é™…çš„å…³é”®å­—ç”Ÿæˆå®ç°"""
        client = self.get_api_client()
        if not client:
            return None

        cleaned_query = self.input_cleaner.clean_text_for_regex(query_text)
        prompt = f"{self.valves.keyword_generation_prompt}\n\n{cleaned_query}"

        response = await client.chat.completions.create(
            model=self.valves.text_model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.3,
            timeout=self.valves.request_timeout,
        )

        if response.choices and response.choices[0].message.content:
            keywords_text = response.choices[0].message.content.strip()
            keywords_text = self.input_cleaner.clean_text_for_regex(keywords_text)
            keywords = [kw.strip() for kw in keywords_text.split(",") if kw.strip()]
            keywords = [kw for kw in keywords if len(kw) >= 2]
            self.debug_log(2, f"ç”Ÿæˆå…³é”®å­—: {keywords[:5]}...", "ğŸ”‘")
            return keywords

        return None

    async def generate_search_keywords(
        self, query_text: str, event_emitter
    ) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®å­—"""
        if not self.valves.enable_keyword_generation:
            return [query_text]

        need_context_max = await self.detect_context_max_need(query_text, event_emitter)

        if not need_context_max and not self.valves.keyword_generation_for_context_max:
            self.debug_log(2, f"å…·ä½“æŸ¥è¯¢ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬: {query_text[:50]}...", "ğŸ”‘")
            return [query_text]

        self.debug_log(1, f"ç”Ÿæˆæœç´¢å…³é”®å­—: {query_text[:50]}...", "ğŸ”‘")

        keywords = await self.safe_api_call(
            self.generate_keywords_impl,
            "å…³é”®å­—ç”Ÿæˆ",
            query_text,
            event_emitter,
        )

        if keywords:
            final_keywords = [query_text] + keywords
            final_keywords = list(dict.fromkeys(final_keywords))
            self.stats.keyword_generations += 1
            self.debug_log(1, f"å…³é”®å­—ç”Ÿæˆå®Œæˆ: {len(final_keywords)}ä¸ª", "ğŸ”‘")
            return final_keywords
        else:
            self.debug_log(1, f"å…³é”®å­—ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢", "âš ï¸")
            return [query_text]

    # ========== å‘é‡å¤„ç† ==========
    async def get_text_embedding_impl(self, text: str, event_emitter):
        """å®é™…çš„æ–‡æœ¬å‘é‡è·å–å®ç°"""
        client = self.get_api_client()
        if not client:
            return None

        cleaned_text = self.input_cleaner.clean_text_for_regex(text)
        self.stats.embedding_requests += 1

        response = await client.embeddings.create(
            model=self.valves.text_vector_model,
            input=[cleaned_text[:8000]],
            encoding_format="float",
        )

        if (
            response
            and response.data
            and len(response.data) > 0
            and response.data[0].embedding
        ):
            return response.data[0].embedding

        return None

    async def get_text_embedding(
        self, text: str, event_emitter
    ) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬å‘é‡ - å¸¦ç¼“å­˜"""
        if not text:
            return None

        # ç”Ÿæˆç¼“å­˜é”®
        content_key = hashlib.md5(text.encode()).hexdigest()[:16]

        # å°è¯•ä»ç¼“å­˜è·å–
        if self.embedding_cache:
            cached_embedding = self.embedding_cache.get(content_key)
            if cached_embedding:
                self.stats.cache_hits += 1
                self.debug_log(3, f"æ–‡æœ¬å‘é‡ç¼“å­˜å‘½ä¸­: {len(cached_embedding)}ç»´", "ğŸ’¾")
                return cached_embedding

        self.stats.cache_misses += 1

        embedding = await self.safe_api_call(
            self.get_text_embedding_impl,
            "æ–‡æœ¬å‘é‡",
            text,
            event_emitter,
        )

        if embedding:
            # ç¼“å­˜ç»“æœ
            if self.embedding_cache:
                self.embedding_cache.set(content_key, embedding)
            self.debug_log(3, f"æ–‡æœ¬å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ“")

        return embedding

    async def get_multimodal_embedding_impl(self, content, event_emitter):
        """å®é™…çš„å¤šæ¨¡æ€å‘é‡è·å–å®ç°"""
        client = self.get_api_client()
        if not client:
            return None

        # å¤„ç†è¾“å…¥æ ¼å¼
        if isinstance(content, list):
            cleaned_content = []
            for item in content:
                if item.get("type") == "text":
                    cleaned_item = item.copy()
                    text = item.get("text", "")
                    cleaned_text = self.input_cleaner.clean_text_for_regex(text)
                    cleaned_item["text"] = cleaned_text
                    cleaned_content.append(cleaned_item)
                elif item.get("type") == "image_url":
                    # éªŒè¯å¹¶æ¸…æ´—å›¾ç‰‡æ•°æ®
                    image_url = item.get("image_url", {}).get("url", "")
                    is_valid, cleaned_url = (
                        self.input_cleaner.validate_and_clean_image_url(image_url)
                    )
                    if is_valid:
                        cleaned_item = copy.deepcopy(item)
                        cleaned_item["image_url"]["url"] = cleaned_url
                        cleaned_content.append(cleaned_item)
                else:
                    cleaned_content.append(item)
            input_data = cleaned_content
        else:
            text = str(content)
            cleaned_text = self.input_cleaner.clean_text_for_regex(text)
            input_data = [{"type": "text", "text": cleaned_text[:8000]}]

        self.stats.embedding_requests += 1

        try:
            response = await client.embeddings.create(
                model=self.valves.multimodal_vector_model,
                input=input_data,
                encoding_format="float",
            )

            if hasattr(response, "data") and hasattr(response.data, "embedding"):
                return response.data.embedding
            elif (
                hasattr(response, "data")
                and isinstance(response.data, list)
                and len(response.data) > 0
            ):
                return response.data[0].embedding
            else:
                self.debug_log(1, f"å¤šæ¨¡æ€å‘é‡å“åº”æ ¼å¼å¼‚å¸¸", "âš ï¸")
                return None
        except Exception as e:
            self.debug_log(1, f"å¤šæ¨¡æ€å‘é‡è°ƒç”¨å¤±è´¥: {str(e)[:100]}", "âŒ")
            raise

    async def get_multimodal_embedding(
        self, content, event_emitter
    ) -> Optional[List[float]]:
        """è·å–å¤šæ¨¡æ€å‘é‡"""
        if not content:
            return None

        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ¨¡æ€å†…å®¹
        has_multimodal_content = False
        if isinstance(content, list):
            has_multimodal_content = any(
                item.get("type") in ["image_url", "video_url"] for item in content
            )

        if not has_multimodal_content:
            self.debug_log(3, "å†…å®¹ä¸åŒ…å«å¤šæ¨¡æ€å…ƒç´ ï¼Œä¸ä½¿ç”¨å¤šæ¨¡æ€å‘é‡", "ğŸ“")
            return None

        embedding = await self.safe_api_call(
            self.get_multimodal_embedding_impl,
            "å¤šæ¨¡æ€å‘é‡",
            content,
            event_emitter,
        )

        if embedding:
            self.debug_log(3, f"å¤šæ¨¡æ€å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ–¼ï¸")

        return embedding

    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0

        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    # ========== ç›¸å…³åº¦è®¡ç®—ï¼ˆå¹¶å‘ä¼˜åŒ– + ä¸¤é˜¶æ®µå¬å›ï¼‰ ==========
    async def compute_relevance_scores(
        self, query_msg: dict, history_msgs: List[dict], progress: ProgressTracker
    ) -> List[dict]:
        """è®¡ç®—æ‰€æœ‰å†å²æ¶ˆæ¯çš„ç›¸å…³åº¦åˆ†æ•°ï¼ˆä¸¤é˜¶æ®µå¬å› + å¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if not history_msgs:
            return []

        self.debug_log(
            1, f"å¼€å§‹è®¡ç®—ç›¸å…³åº¦åˆ†æ•°: æŸ¥è¯¢1æ¡ï¼Œå†å²{len(history_msgs)}æ¡", "ğŸ¯"
        )

        # è·å–æŸ¥è¯¢å‘é‡
        query_content = query_msg.get("content", "")
        query_text = self.extract_text_from_content(query_content)

        # ç¬¬ä¸€é˜¶æ®µï¼šè½»é‡çº§ç­›é€‰ï¼ˆå¦‚æœæ¶ˆæ¯æ•°é‡è¾ƒå¤šï¼‰
        if len(history_msgs) > 40:
            # è½»é‡çº§è¯„åˆ†å…ˆé€‰top-K
            lightweight_scored = self._compute_lightweight_scores(
                query_text, history_msgs
            )

            # é€‰æ‹©top-Kè¿›å…¥ç¬¬äºŒé˜¶æ®µ
            top_k = min(self.valves.vector_top_k, len(lightweight_scored))
            lightweight_scored.sort(key=lambda x: x["score"], reverse=True)
            selected_msgs = lightweight_scored[:top_k]

            self.debug_log(
                1,
                f"ä¸¤é˜¶æ®µå¬å›: {len(history_msgs)} -> {len(selected_msgs)}æ¡è¿›å…¥å‘é‡åŒ–é˜¶æ®µ",
                "âš¡",
            )

            # ç¬¬äºŒé˜¶æ®µï¼šå‘é‡ç›¸ä¼¼åº¦è®¡ç®—
            if len(selected_msgs) > 80:
                # ä»ç„¶å¤ªå¤šï¼Œç›´æ¥ä½¿ç”¨è½»é‡çº§åˆ†æ•°
                scored = selected_msgs
            else:
                # æ™ºèƒ½å‘é‡åŒ–ç­–ç•¥
                if self.has_images_in_content(query_content):
                    query_vector = await self.get_multimodal_embedding(
                        query_content, progress.event_emitter
                    )
                    if not query_vector:
                        query_vector = await self.get_text_embedding(
                            query_text, progress.event_emitter
                        )
                else:
                    query_vector = await self.get_text_embedding(
                        query_text, progress.event_emitter
                    )

                # å¹¶å‘è·å–å‘é‡å¹¶é‡æ–°è®¡ç®—åˆ†æ•°
                scored = await self._compute_vector_scores_concurrent(
                    query_vector, selected_msgs, progress
                )
        else:
            # å°æ•°æ®é›†ï¼šç›´æ¥ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦
            if self.has_images_in_content(query_content):
                query_vector = await self.get_multimodal_embedding(
                    query_content, progress.event_emitter
                )
                if not query_vector:
                    query_vector = await self.get_text_embedding(
                        query_text, progress.event_emitter
                    )
            else:
                query_vector = await self.get_text_embedding(
                    query_text, progress.event_emitter
                )

            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            msg_items = []
            for idx, msg in enumerate(history_msgs):
                msg_items.append(
                    {"msg": msg, "idx": idx, "tokens": self.count_message_tokens(msg)}
                )

            scored = await self._compute_vector_scores_concurrent(
                query_vector, msg_items, progress
            )

        self.debug_log(1, f"ç›¸å…³åº¦è®¡ç®—å®Œæˆ: {len(scored)}æ¡æ¶ˆæ¯å…¨éƒ¨è¯„åˆ†", "ğŸ¯")

        # ç®€åŒ–Top5æ‰“å°ï¼ˆåªåœ¨debug_level >= 2æ—¶æ˜¾ç¤ºï¼‰
        if self.valves.debug_level >= 2:
            top5 = sorted(scored, key=lambda x: x["score"], reverse=True)[:5]
            for i, item in enumerate(top5):
                self.debug_log(
                    2,
                    f"Top{i+1}: score={item['score']:.3f}, {item['tokens']}tokens",
                    "ğŸ“Š",
                )

        return scored

    def _compute_lightweight_scores(
        self, query_text: str, history_msgs: List[dict]
    ) -> List[dict]:
        """è½»é‡çº§è¯„åˆ†ï¼ˆä¸ä½¿ç”¨å‘é‡ï¼‰"""
        scored = []
        query_lower = query_text.lower()
        query_words = set(query_lower.split())

        for idx, msg in enumerate(history_msgs):
            msg_content = msg.get("content", "")
            msg_text = self.extract_text_from_content(msg_content)
            msg_lower = msg_text.lower()
            msg_words = set(msg_lower.split())

            # åŸºäºæ–‡æœ¬åŒ¹é…çš„ç®€å•ç›¸ä¼¼åº¦
            common_words = query_words & msg_words
            text_sim = (
                len(common_words) / max(1, len(query_words)) if query_words else 0
            )

            # é¢å¤–æƒé‡è®¡ç®—
            recency = idx / max(1, len(history_msgs) - 1)
            role = msg.get("role", "")
            role_weight = (
                1.0 if role == "user" else (0.8 if role == "assistant" else 0.6)
            )
            kw_bonus = 1.0 if self.is_high_priority_content(msg_text) else 0.0

            # ç»¼åˆåˆ†æ•°ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦60% + æ—¶é—´æƒé‡20% +è§’è‰²æƒé‡10% + å…³é”®è¯æƒé‡10%
            score = 0.6 * text_sim + 0.2 * recency + 0.1 * role_weight + 0.1 * kw_bonus

            scored.append(
                {
                    "msg": msg,
                    "score": score,
                    "tokens": self.count_message_tokens(msg),
                    "idx": idx,
                    "sim": text_sim,
                    "recency": recency,
                    "role_weight": role_weight,
                    "kw_bonus": kw_bonus,
                }
            )

        return scored

    async def _compute_vector_scores_concurrent(
        self,
        query_vector: List[float],
        msg_items: List[dict],
        progress: ProgressTracker,
    ) -> List[dict]:
        """å¹¶å‘è®¡ç®—å‘é‡åˆ†æ•°"""
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.valves.max_concurrent_requests)

        async def get_msg_embedding(item):
            async with semaphore:
                msg = item["msg"]
                msg_content = msg.get("content", "")
                msg_text = self.extract_text_from_content(msg_content)

                # å°è¯•ä»ç¼“å­˜è·å–ï¼ˆåŸºäºcontent_keyï¼‰
                content_key = msg.get("_content_key")
                if content_key and self.embedding_cache:
                    cached_embedding = self.embedding_cache.get(content_key)
                    if cached_embedding:
                        self.stats.cache_hits += 1
                        return item["idx"], cached_embedding

                self.stats.cache_misses += 1

                if self.has_images_in_content(msg_content):
                    msg_vector = await self.get_multimodal_embedding(
                        msg_content, progress.event_emitter
                    )
                    if not msg_vector:
                        msg_vector = await self.get_text_embedding(
                            msg_text, progress.event_emitter
                        )
                else:
                    msg_vector = await self.get_text_embedding(
                        msg_text, progress.event_emitter
                    )

                # ç¼“å­˜ç»“æœ
                if content_key and msg_vector and self.embedding_cache:
                    self.embedding_cache.set(content_key, msg_vector)

                return item["idx"], msg_vector

        # å¹¶å‘è·å–æ‰€æœ‰å‘é‡
        self.stats.concurrent_tasks = len(msg_items)
        embedding_tasks = [get_msg_embedding(item) for item in msg_items]
        embedding_results = await asyncio.gather(
            *embedding_tasks, return_exceptions=True
        )

        # è®¡ç®—åˆ†æ•°
        scored = []
        for item in msg_items:
            # æ‰¾åˆ°å¯¹åº”çš„å‘é‡ç»“æœ
            msg_vector = None
            for result in embedding_results:
                if isinstance(result, Exception):
                    continue
                result_idx, vector = result
                if result_idx == item["idx"]:
                    msg_vector = vector
                    break

            msg = item["msg"]
            msg_text = self.extract_text_from_content(msg.get("content", ""))

            # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
            sim = (
                self.cosine_similarity(query_vector, msg_vector)
                if (query_vector and msg_vector)
                else 0.0
            )

            # é¢å¤–æƒé‡è®¡ç®—
            recency = item["idx"] / max(1, len(msg_items) - 1)
            role = msg.get("role", "")
            role_weight = (
                1.0 if role == "user" else (0.8 if role == "assistant" else 0.6)
            )
            kw_bonus = 1.0 if self.is_high_priority_content(msg_text) else 0.0

            # ç»¼åˆåˆ†æ•°ï¼šå‘é‡ç›¸ä¼¼åº¦60% + æ—¶é—´æƒé‡20% + è§’è‰²æƒé‡10% + å…³é”®è¯æƒé‡10%
            score = 0.6 * sim + 0.2 * recency + 0.1 * role_weight + 0.1 * kw_bonus

            scored.append(
                {
                    "msg": msg,
                    "score": score,
                    "tokens": item["tokens"],
                    "idx": item["idx"],
                    "sim": sim,
                    "recency": recency,
                    "role_weight": role_weight,
                    "kw_bonus": kw_bonus,
                }
            )

        return scored

    # ========== å‡çº§ç­–ç•¥ï¼ˆé˜²é¢„ç®—è¢«åƒå…‰ï¼‰ ==========
    def select_preserve_upgrades_with_protection(
        self, scored_msgs: List[dict], coverage_entries: List[dict], total_budget: int
    ) -> Tuple[set, int]:
        """é€‰æ‹©å‡çº§çš„æ¶ˆæ¯ï¼ˆé˜²é¢„ç®—è¢«åƒå…‰ç‰ˆæœ¬ï¼‰"""
        # å…ˆé¢„ç•™å‡çº§æ± 
        upgrade_pool = int(total_budget * self.valves.upgrade_min_pct)
        if upgrade_pool <= 0 or not scored_msgs:
            return set(), 0

        self.debug_log(
            1,
            f"å‡çº§æ± ä¿æŠ¤: é¢„ç•™{upgrade_pool:,}tokens({self.valves.upgrade_min_pct:.1%})ç»™å‡çº§",
            "â¬†ï¸",
        )

        # å»ºç«‹æ¶ˆæ¯IDåˆ°æ‘˜è¦æˆæœ¬çš„æ˜ å°„
        summary_cost_map = defaultdict(int)
        for entry in coverage_entries:
            if entry["type"] == "micro":
                summary_cost_map[entry["msg_id"]] = entry.get(
                    "budget", entry.get("ideal_budget", 0)
                )

        # æ„å»ºå‡çº§å€™é€‰åˆ—è¡¨
        candidates = []
        for item in scored_msgs:
            msg = item["msg"]
            msg_id = msg.get("_order_id", f"msg_{item['idx']}")
            original_tokens = item["tokens"]
            summary_cost = summary_cost_map.get(msg_id, 0)

            if summary_cost > 0:
                upgrade_cost = max(0, original_tokens - summary_cost)
            else:
                upgrade_cost = original_tokens

            if upgrade_cost <= 0:
                continue

            score = item["score"]

            # æœ€è¿‘æ€§æƒé‡ï¼Œä½†è®¾ä¸Šé™é˜²æ­¢æé•¿æ¶ˆæ¯æŒ¤çˆ†æ± å­
            if item["recency"] > 0.8:
                recency_boost = min(
                    1.2, 1.0 + 0.2 * (2000 / max(upgrade_cost, 1))
                )  # æˆæœ¬è¶Šé«˜ï¼ŒåŠ æƒè¶Šå°‘
                score *= recency_boost

            density = score / upgrade_cost
            candidates.append(
                {
                    "density": density,
                    "score": score,
                    "upgrade_cost": upgrade_cost,
                    "item": item,
                    "msg_id": msg_id,
                }
            )

        # æŒ‰ä»·å€¼å¯†åº¦æ’åº
        candidates.sort(key=lambda x: (-x["density"], -x["score"]))

        # è´ªå¿ƒé€‰æ‹©ï¼Œä½¿ç”¨å‡çº§æ± é¢„ç®—
        preserve_set = set()
        consumed = 0

        self.debug_log(
            2, f"å‡çº§å€™é€‰: {len(candidates)}ä¸ªï¼Œå‡çº§æ± é¢„ç®—{upgrade_pool:,}tokens", "â¬†ï¸"
        )

        for cand in candidates:
            if consumed + cand["upgrade_cost"] > upgrade_pool:
                continue

            preserve_set.add(cand["msg_id"])
            consumed += cand["upgrade_cost"]

            self.debug_log(
                3,
                f"å‡çº§é€‰ä¸­: ID={cand['msg_id'][:8]}, å¯†åº¦={cand['density']:.4f}, æˆæœ¬={cand['upgrade_cost']}tokens",
                "â¬†ï¸",
            )

        self.debug_log(
            1,
            f"å‡çº§é€‰æ‹©å®Œæˆ: {len(preserve_set)}æ¡æ¶ˆæ¯å‡çº§, æ¶ˆè€—{consumed:,}/{upgrade_pool:,}tokens",
            "â¬†ï¸",
        )

        return preserve_set, consumed

    # ========== æ‘˜è¦ç”Ÿæˆï¼ˆå¹¶å‘ä¼˜åŒ–ï¼‰ ==========
    async def generate_micro_summary_with_budget_impl(
        self, msg: dict, budget: int, event_emitter
    ):
        """ç”Ÿæˆå•æ¡æ¶ˆæ¯çš„å¾®æ‘˜è¦ï¼ˆä½¿ç”¨ç¼©æ”¾åçš„é¢„ç®—ï¼‰"""
        client = self.get_api_client()
        if not client:
            return None

        content = self.extract_text_from_content(msg.get("content", ""))
        role = msg.get("role", "")
        cleaned_content = self.input_cleaner.clean_text_for_regex(content)

        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ¶ˆæ¯ç”Ÿæˆç®€æ´æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚è¦æ±‚ï¼š
1. ä¸¥æ ¼åœ¨{budget}ä¸ªtokensä»¥å†…
2. ä¿ç•™æ—¶é—´ã€ä¸»ä½“ã€åŠ¨ä½œã€æ•°æ®/ä»£ç å…³é”®è¡Œç­‰æ ¸å¿ƒè¦ç´ 
3. å¦‚æœæ˜¯æŠ€æœ¯å†…å®¹ï¼Œä¿ç•™æŠ€æœ¯æœ¯è¯­å’Œå…³é”®å‚æ•°
4. ä¿æŒå®¢è§‚ç®€æ´

æ¶ˆæ¯è§’è‰²: {role}
æ¶ˆæ¯å†…å®¹: {cleaned_content[:2000]}

æ‘˜è¦ï¼š"""

        has_multimodal = self.has_images_in_content(msg.get("content"))
        model_to_use = (
            self.valves.multimodal_model if has_multimodal else self.valves.text_model
        )

        self.stats.summary_requests += 1

        response = await client.chat.completions.create(
            model=model_to_use,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=budget,  # ä½¿ç”¨ç¼©æ”¾åçš„é¢„ç®—
            temperature=0.2,
            timeout=self.valves.request_timeout,
        )

        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            summary = self.input_cleaner.clean_text_for_regex(summary)
            return summary

        return None

    async def generate_adaptive_block_summary_impl(
        self, msgs: List[dict], idx_range: Tuple[int, int], budget: int, event_emitter
    ):
        """ç”Ÿæˆè‡ªé€‚åº”å—æ‘˜è¦ï¼ˆä½¿ç”¨ç¼©æ”¾åçš„é¢„ç®—ï¼‰"""
        client = self.get_api_client()
        if not client:
            return None

        # åˆå¹¶æ¶ˆæ¯å†…å®¹
        combined_content = ""
        has_multimodal = False
        for i, msg in enumerate(msgs):
            role = msg.get("role", "")
            content = self.extract_text_from_content(msg.get("content", ""))
            combined_content += f"[æ¶ˆæ¯{idx_range[0] + i}:{role}] {content}\n\n"
            if self.has_images_in_content(msg.get("content")):
                has_multimodal = True

        cleaned_content = self.input_cleaner.clean_text_for_regex(combined_content)

        prompt = f"""è¯·ä¸ºä»¥ä¸‹è¿ç»­æ¶ˆæ¯å—(ç¬¬{idx_range[0]}åˆ°{idx_range[1]}æ¡)ç”Ÿæˆç»¼åˆæ‘˜è¦ã€‚è¦æ±‚ï¼š
1. ä¸¥æ ¼åœ¨{budget}ä¸ªtokensä»¥å†…
2. è¦†ç›–æ‰€æœ‰è¦ç‚¹ï¼Œä¿æŒé€»è¾‘é¡ºåº
3. æŒ‡æ˜æ¶ˆæ¯ç¼–å·èŒƒå›´å’Œä¸»è¦è§’è‰²
4. ä¿ç•™å…³é”®æŠ€æœ¯ç»†èŠ‚ã€æ•°æ®ã€å‚æ•°ç­‰

æ¶ˆæ¯å—å†…å®¹ï¼š
{cleaned_content[:4000]}

å—æ‘˜è¦ï¼š"""

        model_to_use = (
            self.valves.multimodal_model if has_multimodal else self.valves.text_model
        )

        self.stats.summary_requests += 1

        response = await client.chat.completions.create(
            model=model_to_use,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=budget,  # ä½¿ç”¨ç¼©æ”¾åçš„é¢„ç®—
            temperature=0.2,
            timeout=self.valves.request_timeout,
        )

        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            summary = self.input_cleaner.clean_text_for_regex(summary)
            return summary

        return None

    async def generate_global_block_summary_impl(
        self, msgs: List[dict], idx_range: Tuple[int, int], budget: int, event_emitter
    ):
        """ç”Ÿæˆå…¨å±€å—æ‘˜è¦"""
        client = self.get_api_client()
        if not client:
            return None

        # é‡‡æ ·å…³é”®æ¶ˆæ¯ï¼Œé¿å…å†…å®¹è¿‡é•¿
        sampled_msgs = msgs[:: max(1, len(msgs) // 10)]  # æœ€å¤šé‡‡æ ·10æ¡
        combined_content = ""
        has_multimodal = False

        for i, msg in enumerate(sampled_msgs):
            role = msg.get("role", "")
            content = self.extract_text_from_content(msg.get("content", ""))
            combined_content += f"[æ¶ˆæ¯æ ·æœ¬{i}:{role}] {content[:200]}...\n\n"
            if self.has_images_in_content(msg.get("content")):
                has_multimodal = True

        cleaned_content = self.input_cleaner.clean_text_for_regex(combined_content)

        prompt = f"""è¯·ä¸ºä»¥ä¸‹å¯¹è¯å†å²ç”Ÿæˆå…¨å±€æ‘˜è¦ã€‚è¦æ±‚ï¼š
1. ä¸¥æ ¼åœ¨{budget}ä¸ªtokensä»¥å†…
2. æ¦‚æ‹¬ä¸»è¦è¯é¢˜å’Œè®¨è®ºè¦ç‚¹
3. ä¿ç•™é‡è¦çš„æŠ€æœ¯ç»†èŠ‚å’Œç»“è®º
4. æ€»å…±æ¶µç›–{len(msgs)}æ¡å†å²æ¶ˆæ¯

å¯¹è¯å†å²æ ·æœ¬ï¼š
{cleaned_content[:5000]}

å…¨å±€æ‘˜è¦ï¼š"""

        model_to_use = (
            self.valves.multimodal_model if has_multimodal else self.valves.text_model
        )

        self.stats.summary_requests += 1

        response = await client.chat.completions.create(
            model=model_to_use,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=budget,
            temperature=0.3,
            timeout=self.valves.request_timeout,
        )

        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            summary = self.input_cleaner.clean_text_for_regex(summary)
            return summary

        return None

    async def generate_coverage_summaries_with_budgets(
        self, coverage_entries: List[dict], progress: ProgressTracker
    ) -> Dict[str, str]:
        """å¹¶å‘ç”Ÿæˆè¦†ç›–æ‘˜è¦ï¼ˆä½¿ç”¨ç¼©æ”¾åçš„é¢„ç®—ï¼‰"""
        if not coverage_entries:
            return {}

        self.debug_log(1, f"å¼€å§‹å¹¶å‘ç”Ÿæˆè¦†ç›–æ‘˜è¦: {len(coverage_entries)}ä¸ªæ¡ç›®", "ğŸ“")

        summaries = {}

        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        semaphore = asyncio.Semaphore(self.valves.max_concurrent_requests)

        async def generate_single_summary(entry):
            async with semaphore:
                if entry["type"] == "micro":
                    msg = entry["msg"]
                    budget = entry.get(
                        "budget",
                        entry.get(
                            "ideal_budget", self.valves.coverage_high_summary_tokens
                        ),
                    )
                    msg_id = entry["msg_id"]

                    summary = await self.safe_api_call(
                        self.generate_micro_summary_with_budget_impl,
                        "å¾®æ‘˜è¦ç”Ÿæˆ",
                        msg,
                        budget,
                        progress.event_emitter,
                    )

                    if summary:
                        self.stats.coverage_micro_summaries += 1
                        return msg_id, summary
                    else:
                        content = self.extract_text_from_content(msg.get("content", ""))
                        fallback_summary = (
                            content[: budget * 3] + "..."
                            if len(content) > budget * 3
                            else content
                        )
                        self.stats.guard_b_fallbacks += 1
                        return msg_id, f"[ç®€åŒ–æ‘˜è¦] {fallback_summary}"

                elif entry["type"] == "adaptive_block":
                    msgs = entry["msgs"]
                    idx_range = entry["idx_range"]
                    budget = entry.get(
                        "budget",
                        entry.get(
                            "ideal_budget", self.valves.coverage_block_summary_tokens
                        ),
                    )
                    block_key = entry["block_key"]

                    summary = await self.safe_api_call(
                        self.generate_adaptive_block_summary_impl,
                        "è‡ªé€‚åº”å—æ‘˜è¦ç”Ÿæˆ",
                        msgs,
                        idx_range,
                        budget,
                        progress.event_emitter,
                    )

                    if summary:
                        self.stats.coverage_block_summaries += 1
                        self.stats.adaptive_blocks_created += 1
                        return block_key, summary
                    else:
                        combined = " ".join(
                            [
                                f"[{msg.get('role','')}]{self.extract_text_from_content(msg.get('content',''))[:100]}..."
                                for msg in msgs
                            ]
                        )
                        self.stats.guard_b_fallbacks += 1
                        return (
                            block_key,
                            f"[ç®€åŒ–å—æ‘˜è¦] ç¬¬{idx_range[0]}-{idx_range[1]}æ¡: {combined}",
                        )

                elif entry["type"] == "global_block":
                    msgs = entry["msgs"]
                    idx_range = entry["idx_range"]
                    budget = entry.get("budget", self.valves.min_block_summary_tokens)
                    block_key = entry["block_key"]

                    summary = await self.safe_api_call(
                        self.generate_global_block_summary_impl,
                        "å…¨å±€å—æ‘˜è¦ç”Ÿæˆ",
                        msgs,
                        idx_range,
                        budget,
                        progress.event_emitter,
                    )

                    if summary:
                        self.stats.coverage_block_summaries += 1
                        return block_key, summary
                    else:
                        self.stats.guard_b_fallbacks += 1
                        return (
                            block_key,
                            f"[å…¨å±€ç®€åŒ–æ‘˜è¦] åŒ…å«{len(msgs)}æ¡å†å²æ¶ˆæ¯çš„å¯¹è¯å†…å®¹",
                        )

                return None, None

        # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
        tasks = [generate_single_summary(entry) for entry in coverage_entries]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # æ”¶é›†ç»“æœ
        for result in results:
            if isinstance(result, Exception):
                self.stats.api_failures += 1
                continue

            key, summary = result
            if key and summary:
                summaries[key] = summary

        self.debug_log(1, f"å¹¶å‘æ‘˜è¦ç”Ÿæˆå®Œæˆ: {len(summaries)}ä¸ªæ‘˜è¦", "ğŸ“")

        return summaries

    # ========== ç»„è£…é˜¶æ®µåŒé‡æŠ¤æ ï¼ˆä¿®æ­£è¦†ç›–ç‡è®¡ç®—ï¼‰ ==========
    async def assemble_coverage_output_with_guards(
        self,
        history_messages: List[dict],
        preserve_set: set,
        coverage_entries: List[dict],
        summaries: Dict[str, str],
        progress: ProgressTracker,
    ) -> List[dict]:
        """ç»„è£…æœ€ç»ˆè¾“å‡ºï¼ˆåŒé‡æŠ¤æ ç‰ˆæœ¬ - ä¿®æ­£è¦†ç›–ç‡è®¡ç®—ï¼‰"""
        if not history_messages:
            return []

        self.debug_log(1, f"å¼€å§‹ç»„è£…æœ€ç»ˆè¾“å‡º: {len(history_messages)}æ¡å†å²æ¶ˆæ¯", "ğŸ”§")

        # æŠ¤æ Aï¼šæ ¡éªŒå¹¶æ—¥å¿—æ‰“å°å„ç§æ¡ç›®æ•°é‡
        micro_entries = [e for e in coverage_entries if e["type"] == "micro"]
        adaptive_block_entries = [
            e for e in coverage_entries if e["type"] == "adaptive_block"
        ]
        global_block_entries = [
            e for e in coverage_entries if e["type"] == "global_block"
        ]

        if self.valves.debug_level >= 2:
            print(f"ğŸ›¡ï¸ æŠ¤æ Aç»Ÿè®¡:")
            print(f"    â”œâ”€ åŸæ–‡ä¿ç•™é›†åˆ: {len(preserve_set)}æ¡")
            print(f"    â”œâ”€ å¾®æ‘˜è¦æ¡ç›®: {len(micro_entries)}æ¡")
            print(f"    â”œâ”€ è‡ªé€‚åº”å—æ¡ç›®: {len(adaptive_block_entries)}æ¡")
            print(f"    â”œâ”€ å…¨å±€å—æ¡ç›®: {len(global_block_entries)}æ¡")
            print(f"    â”œâ”€ ç”Ÿæˆæ‘˜è¦æ€»æ•°: {len(summaries)}ä¸ª")
            print(f"    â””â”€ å†å²æ¶ˆæ¯æ€»æ•°: {len(history_messages)}æ¡")

        # æ‰“å°å‰å‡ ä¸ªæœªå‘½ä¸­microçš„msg_id
        all_micro_msg_ids = {e["msg_id"] for e in micro_entries}
        all_msg_ids = {
            msg.get("_order_id", f"msg_{i}") for i, msg in enumerate(history_messages)
        }
        unmapped_msg_ids = all_msg_ids - all_micro_msg_ids

        if unmapped_msg_ids and self.valves.debug_level >= 2:
            unmapped_sample = list(unmapped_msg_ids)[:3]
            print(
                f"ğŸ›¡ï¸ æŠ¤æ Aè­¦å‘Š: {len(unmapped_msg_ids)}æ¡æ¶ˆæ¯æœªæ˜ å°„åˆ°å¾®æ‘˜è¦: {unmapped_sample}..."
            )
            self.stats.guard_a_warnings += 1

        # å»ºç«‹æ˜ å°„
        msg_id_to_msg = {
            msg.get("_order_id", f"msg_{i}"): msg
            for i, msg in enumerate(history_messages)
        }

        # å»ºç«‹å—æ‘˜è¦æ˜ å°„
        block_summaries = {}
        block_ranges = {}
        entry_idx_ranges = {}  # å­˜å‚¨æ¯ä¸ªblock_keyçš„idx_range

        for entry in adaptive_block_entries + global_block_entries:
            idx_range = entry["idx_range"]
            block_key = entry.get("block_key", f"block_{idx_range[0]}_{idx_range[1]}")
            entry_idx_ranges[block_key] = idx_range

            if block_key in summaries:
                block_summaries[block_key] = summaries[block_key]
                # è®°å½•è¿™ä¸ªèŒƒå›´å†…çš„æ‰€æœ‰æ¶ˆæ¯ç´¢å¼•
                for idx in range(idx_range[0], idx_range[1] + 1):
                    if idx < len(history_messages):
                        block_ranges[idx] = block_key

        # è®¡ç®—å·²è¢«micro/preserveè¦†ç›–çš„ç´¢å¼•
        covered_by_micro_or_preserve = set()
        for i, msg in enumerate(history_messages):
            mid = msg.get("_order_id", f"msg_{i}")
            if mid in preserve_set or mid in summaries:  # micro å·²ç”Ÿæˆ
                covered_by_micro_or_preserve.add(i)

        final_messages = []
        processed_block_keys = set()
        covered_messages = 0  # å®é™…è¢«è¦†ç›–çš„æ¶ˆæ¯æ•°

        # æŒ‰åŸå§‹é¡ºåºéå†å†å²æ¶ˆæ¯
        for idx, msg in enumerate(history_messages):
            msg_id = msg.get("_order_id", f"msg_{idx}")
            message_covered = False

            # æ£€æŸ¥æ˜¯å¦åœ¨preserveé›†åˆä¸­ï¼ˆå‡çº§ä¸ºåŸæ–‡ï¼‰
            if msg_id in preserve_set:
                final_messages.append(msg)
                self.stats.coverage_preserved_count += 1
                self.stats.coverage_preserved_tokens += self.count_message_tokens(msg)
                self.debug_log(3, f"ä½¿ç”¨åŸæ–‡: {msg_id[:8]}", "ğŸ“„")
                message_covered = True

            # æ£€æŸ¥æ˜¯å¦æœ‰å¾®æ‘˜è¦
            elif msg_id in summaries:
                summary_msg = {
                    "role": "assistant",
                    "content": summaries[msg_id],
                    "_is_summary": True,
                    "_original_msg_id": msg_id,
                    "_summary_type": "micro",
                }
                final_messages.append(summary_msg)
                self.stats.coverage_summary_count += 1
                self.stats.coverage_summary_tokens += self.count_message_tokens(
                    summary_msg
                )
                self.debug_log(3, f"ä½¿ç”¨å¾®æ‘˜è¦: {msg_id[:8]}", "ğŸ“„")
                message_covered = True

            # æ£€æŸ¥æ˜¯å¦åœ¨æŸä¸ªå—æ‘˜è¦ä¸­
            elif idx in block_ranges:
                block_key = block_ranges[idx]
                if (
                    block_key not in processed_block_keys
                    and block_key in block_summaries
                ):
                    # æ£€æŸ¥è¿™ä¸ªå—èŒƒå›´é‡Œæ˜¯å¦è¿˜æœ‰æœªè¢«è¦†ç›–çš„ç´¢å¼•
                    idx0, idx1 = entry_idx_ranges[block_key]
                    has_uncovered = any(
                        j not in covered_by_micro_or_preserve
                        for j in range(idx0, idx1 + 1)
                        if j < len(history_messages)
                    )

                    if has_uncovered:
                        block_summary_msg = {
                            "role": "assistant",
                            "content": block_summaries[block_key],
                            "_is_summary": True,
                            "_block_key": block_key,
                            "_summary_type": (
                                "adaptive_block"
                                if "global" not in block_key
                                else "global_block"
                            ),
                        }
                        final_messages.append(block_summary_msg)
                        processed_block_keys.add(block_key)
                        self.stats.coverage_summary_count += 1
                        self.stats.coverage_summary_tokens += self.count_message_tokens(
                            block_summary_msg
                        )
                        self.debug_log(3, f"ä½¿ç”¨å—æ‘˜è¦: {block_key}", "ğŸ“„")

                        # æ ‡è®°è¯¥å—èŒƒå›´å†…çš„æ‰€æœ‰æ¶ˆæ¯ä¸ºå·²è¦†ç›–
                        for j in range(idx0, idx1 + 1):
                            if j < len(history_messages):
                                message_covered = True
                                break

                # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªæˆ–æ²¡æœ‰æœªè¦†ç›–ç´¢å¼•ï¼Œä¹Ÿè®¤ä¸ºè¢«å—æ‘˜è¦è¦†ç›–äº†
                if idx in block_ranges:
                    message_covered = True

            else:
                # æŠ¤æ Bï¼šè®¡åˆ’microä½†æœ€ç»ˆæ²¡è½åœ°çš„æ¡ç›®ï¼Œå›é€€æ”¾ç®€åŒ–æ‘˜è¦
                self.debug_log(
                    1, f"æŠ¤æ Bè§¦å‘ï¼šæ¶ˆæ¯{msg_id[:8]}æ—¢ä¸åœ¨preserveä¹Ÿä¸åœ¨coverageä¸­", "ğŸ›¡ï¸"
                )
                content = self.extract_text_from_content(msg.get("content", ""))
                fallback_msg = {
                    "role": "assistant",
                    "content": f"[æŠ¤æ Bç®€åŒ–æ‘˜è¦] {content[:200]}...",
                    "_is_summary": True,
                    "_original_msg_id": msg_id,
                    "_summary_type": "guard_b_fallback",
                }
                final_messages.append(fallback_msg)
                self.stats.guard_b_fallbacks += 1
                self.stats.coverage_summary_count += 1
                self.stats.coverage_summary_tokens += self.count_message_tokens(
                    fallback_msg
                )
                message_covered = True

            if message_covered:
                covered_messages += 1

        # ç¡®ä¿æ¶ˆæ¯é¡ºåºæ­£ç¡®
        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )

        # ä¿®æ­£è¦†ç›–ç‡è®¡ç®—
        self.stats.coverage_total_messages = len(history_messages)
        self.stats.coverage_rate = covered_messages / max(1, len(history_messages))

        # æŠ¤æ Aæœ€ç»ˆéªŒè¯
        final_tokens = self.count_messages_tokens(final_messages)
        if self.valves.debug_level >= 2:
            print(f"ğŸ›¡ï¸ æŠ¤æ Aæœ€ç»ˆéªŒè¯:")
            print(
                f"    â”œâ”€ æœ€ç»ˆæ¶ˆæ¯æ•°: åŸæ–‡{self.stats.coverage_preserved_count}æ¡ + æ‘˜è¦{self.stats.coverage_summary_count}æ¡ = {len(final_messages)}æ¡"
            )
            print(
                f"    â”œâ”€ è¦†ç›–ç‡éªŒè¯: {self.stats.coverage_rate:.1%} ({covered_messages}/{len(history_messages)})"
            )
            print(f"    â””â”€ æœ€ç»ˆtokenç»Ÿè®¡: {final_tokens:,}tokens")

        self.debug_log(
            1,
            f"åŒé‡æŠ¤æ ç»„è£…å®Œæˆ: {len(history_messages)} -> {len(final_messages)}æ¡æ¶ˆæ¯({final_tokens:,}tokens)",
            "âœ…",
        )

        return final_messages

    # ========== Top-upçª—å£å¡«å……å™¨ï¼ˆä¿®å¤ç»Ÿè®¡é—®é¢˜ï¼‰ ==========
    def topup_fill_window(
        self,
        final_messages: List[dict],
        scored_msgs: List[dict],
        available_tokens: int,
        summaries: Dict[str, str],
        preserve_set: set,
    ) -> List[dict]:
        """Top-upå¡«å……å™¨ï¼šæŠŠçª—å£åˆ©ç”¨ç‡æå‡åˆ°ç›®æ ‡å€¼ - ç»Ÿè®¡ä¿®æ­£ç‰ˆ"""
        # ä¸€å¼€å¤´å°±å­˜åŸºçº¿
        initial_tokens = self.count_messages_tokens(final_messages)
        current_tokens = initial_tokens
        target_tokens = int(
            available_tokens * self.valves.target_window_usage
        )  # ä½¿ç”¨valvesä¸­çš„è®¾ç½®

        if current_tokens >= target_tokens:
            self.debug_log(
                1,
                f"çª—å£åˆ©ç”¨ç‡å·²è¾¾æ ‡: {current_tokens:,}/{target_tokens:,} tokens ({self.valves.target_window_usage:.1%})",
                "ğŸ”¥",
            )
            return final_messages

        self.debug_log(
            1,
            f"å¼€å§‹Top-upå¡«å……: {current_tokens:,} -> {target_tokens:,} tokens (ç›®æ ‡{self.valves.target_window_usage:.1%})",
            "ğŸ”¥",
        )
        self.stats.topup_applied += 1

        # 1) å…ˆæŠŠå·²æœ‰ micro å‡çº§ä¸ºåŸæ–‡ï¼ˆæ›¿æ¢æ‰ microï¼‰
        # æŒ‰ä»·å€¼å¯†åº¦ï¼ˆscore/tokensï¼‰ä»é«˜åˆ°ä½
        taken_micro = {
            m.get("_original_msg_id")
            for m in final_messages
            if m.get("_summary_type") == "micro"
        }
        id2msg = {
            item["msg"].get("_order_id", f"msg_{item['idx']}"): item
            for item in scored_msgs
        }

        micro_ids_sorted = sorted(
            [mid for mid in taken_micro if mid in id2msg],
            key=lambda mid: id2msg[mid]["score"] / max(1, id2msg[mid]["tokens"]),
            reverse=True,
        )

        upgraded_count = 0
        for mid in micro_ids_sorted:
            item = id2msg[mid]
            raw_msg = item["msg"]
            raw_tokens = self.count_message_tokens(raw_msg)

            # æ‰¾åˆ°å¯¹åº”çš„microæ‘˜è¦æ¶ˆæ¯
            micro_msg = None
            for i, msg in enumerate(final_messages):
                if msg.get("_original_msg_id") == mid:
                    micro_msg = msg
                    break

            if not micro_msg:
                continue

            micro_tokens = self.count_message_tokens(micro_msg)
            token_diff = raw_tokens - micro_tokens

            if current_tokens + token_diff > available_tokens:
                continue

            # åˆ é™¤è¯¥æ¡ microï¼ŒåŠ å…¥åŸæ–‡
            final_messages = [
                m for m in final_messages if m.get("_original_msg_id") != mid
            ]
            final_messages.append(raw_msg)
            current_tokens += token_diff
            upgraded_count += 1
            self.stats.topup_micro_upgraded += 1

            self.debug_log(
                3, f"å¾®æ‘˜è¦å‡çº§ä¸ºåŸæ–‡: {mid[:8]}, å¢åŠ {token_diff}tokens", "â¬†ï¸"
            )

            if current_tokens >= target_tokens:
                break

        if upgraded_count > 0:
            self.debug_log(1, f"å¾®æ‘˜è¦å‡çº§å®Œæˆ: {upgraded_count}æ¡å‡çº§", "â¬†ï¸")

        # 2) å†ä»æœªè½åœ°çš„æ¶ˆæ¯é‡Œï¼ŒæŒ‰ä»·å€¼å¯†åº¦è´ªå¿ƒåŠ å…¥åŸæ–‡
        landed_ids = {
            m.get("_order_id") or m.get("_original_msg_id") for m in final_messages
        }
        candidates = [
            it for it in scored_msgs if it["msg"].get("_order_id") not in landed_ids
        ]
        candidates.sort(key=lambda it: it["score"] / max(1, it["tokens"]), reverse=True)

        added_count = 0
        for item in candidates:
            tokens = item["tokens"]
            if current_tokens + tokens > available_tokens:
                continue

            final_messages.append(item["msg"])
            current_tokens += tokens
            added_count += 1
            self.stats.topup_raw_added += 1

            self.debug_log(
                3,
                f"æ·»åŠ æœªè½åœ°åŸæ–‡: {item['msg'].get('_order_id', 'unknown')[:8]}, å¢åŠ {tokens}tokens",
                "ğŸ“",
            )

            if current_tokens >= target_tokens:
                break

        if added_count > 0:
            self.debug_log(1, f"æœªè½åœ°åŸæ–‡æ·»åŠ å®Œæˆ: {added_count}æ¡æ·»åŠ ", "ğŸ“")

        # 3) é‡æ–°æ’åº
        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )

        # ä¿®å¤ç»Ÿè®¡è®¡ç®—ï¼šç”¨åŸºçº¿åšå·®
        final_tokens = self.count_messages_tokens(final_messages)
        tokens_added = max(0, final_tokens - initial_tokens)
        self.stats.topup_tokens_added += tokens_added

        utilization = final_tokens / available_tokens if available_tokens > 0 else 0

        self.debug_log(
            1,
            f"Top-upå¡«å……å®Œæˆ: {final_tokens:,}tokens, åˆ©ç”¨ç‡{utilization:.1%}, æ–°å¢{tokens_added:,}tokens",
            "âœ…",
        )

        return final_messages

    # ========== Coverage-Firstä¸»æµç¨‹ï¼ˆé‡æ„ç‰ˆï¼‰ ==========
    async def process_coverage_first_context_maximization_v2(
        self,
        history_messages: List[dict],
        available_tokens: int,
        progress: ProgressTracker,
        query_message: dict,
    ) -> List[dict]:
        """Coverage-Firstä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†ä¸»æµç¨‹ v2.4.5"""
        if not history_messages or not self.valves.enable_coverage_first:
            return history_messages

        await progress.start_phase("Coverage-Firstå¤„ç†", len(history_messages))
        self.debug_log(
            1,
            f"Coverage-Firstå¼€å§‹: {len(history_messages)}æ¡æ¶ˆæ¯, å¯ç”¨é¢„ç®—: {available_tokens:,}tokens",
            "ğŸ¯",
        )

        # Step 0: æ¶ˆæ¯åˆ†ç‰‡é¢„å¤„ç†
        if self.valves.enable_smart_chunking:
            await progress.update_progress(0, 8, "æ¶ˆæ¯åˆ†ç‰‡é¢„å¤„ç†")
            processed_history = self.message_chunker.preprocess_messages_with_chunking(
                history_messages, self.message_order
            )
            self.stats.chunked_messages_count = len(
                [msg for msg in processed_history if msg.get("_is_chunk")]
            )
            # ä¿®æ­£ç»Ÿè®¡è®¡ç®—
            self.stats.total_chunks_created = sum(
                1 for m in processed_history if m.get("_is_chunk")
            )
            self.debug_log(
                1,
                f"æ¶ˆæ¯åˆ†ç‰‡é¢„å¤„ç†: {len(history_messages)} -> {len(processed_history)}æ¡ ({self.stats.chunked_messages_count}æ¡è¢«åˆ†ç‰‡)",
                "ğŸ§©",
            )
        else:
            processed_history = history_messages

        # Step 1: è®¡ç®—ç›¸å…³åº¦åˆ†æ•°ï¼ˆä¸¤é˜¶æ®µå¬å› + å¹¶å‘ä¼˜åŒ–ï¼‰
        await progress.update_progress(1, 8, "è®¡ç®—ç›¸å…³åº¦åˆ†æ•°")
        scored_msgs = await self.compute_relevance_scores(
            query_message, processed_history, progress
        )
        if not scored_msgs:
            self.debug_log(1, "ç›¸å…³åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯", "âš ï¸")
            return processed_history

        # Step 2: è‡ªé€‚åº”Coverageè§„åˆ’ï¼ˆæŒ‰åŸæ–‡tokené‡åˆ†å— + ä¸€æ¬¡æ€§ç¼©æ”¾ï¼‰
        await progress.update_progress(2, 8, "è‡ªé€‚åº”Coverageè§„åˆ’")
        # ä¸ºè¦†ç›–åˆ†é…é¢„ç®—ï¼ˆå…ˆé¢„ç•™å‡çº§æ± ï¼‰
        upgrade_pool = int(available_tokens * self.valves.upgrade_min_pct)
        coverage_budget = available_tokens - upgrade_pool

        coverage_entries, coverage_cost = (
            self.coverage_planner.plan_adaptive_coverage_summaries(
                scored_msgs, coverage_budget
            )
        )

        # è®°å½•ç¼©æ”¾ç»Ÿè®¡
        if coverage_cost < coverage_budget:
            # æœ‰ä½™é¢ï¼Œå¢åŠ å‡çº§æ± 
            actual_upgrade_pool = upgrade_pool + (coverage_budget - coverage_cost)
        else:
            actual_upgrade_pool = upgrade_pool

        if coverage_cost != coverage_budget:
            self.stats.budget_scaling_applied += 1
            self.stats.scaling_factor = (
                coverage_cost / coverage_budget if coverage_budget > 0 else 1.0
            )

        self.debug_log(
            1,
            f"è‡ªé€‚åº”Coverageè§„åˆ’: {len(coverage_entries)}ä¸ªæ¡ç›®, æˆæœ¬{coverage_cost:,}tokens (å‡çº§æ± {actual_upgrade_pool:,}tokens)",
            "ğŸ“„",
        )

        # Step 3: å‡çº§ç­–ç•¥ï¼ˆé˜²é¢„ç®—è¢«åƒå…‰ï¼‰
        await progress.update_progress(3, 8, "å‡çº§ç­–ç•¥é€‰æ‹©")
        preserve_set, upgrade_consumed = self.select_preserve_upgrades_with_protection(
            scored_msgs, coverage_entries, actual_upgrade_pool
        )

        # æ›´æ–°ç»Ÿè®¡
        self.stats.coverage_upgrade_count = len(preserve_set)
        self.stats.coverage_upgrade_tokens_saved = upgrade_consumed

        # Step 4: å¹¶å‘ç”Ÿæˆæ‘˜è¦å†…å®¹ï¼ˆä½¿ç”¨ç¼©æ”¾åé¢„ç®—ï¼‰
        await progress.update_progress(4, 8, "å¹¶å‘ç”Ÿæˆæ‘˜è¦å†…å®¹")
        summaries = await self.generate_coverage_summaries_with_budgets(
            coverage_entries, progress
        )

        # Step 5: åŒé‡æŠ¤æ ç»„è£…ï¼ˆä¿®æ­£è¦†ç›–ç‡è®¡ç®—ï¼‰
        await progress.update_progress(5, 8, "åŒé‡æŠ¤æ ç»„è£…")
        final_messages = await self.assemble_coverage_output_with_guards(
            processed_history, preserve_set, coverage_entries, summaries, progress
        )

        # Step 6: Top-upçª—å£å¡«å……ï¼ˆä¿®æ­£ç»Ÿè®¡ï¼‰
        await progress.update_progress(6, 8, "Top-upçª—å£å¡«å……")
        final_messages = self.topup_fill_window(
            final_messages, scored_msgs, available_tokens, summaries, preserve_set
        )

        # Step 7: æœ€ç»ˆç»Ÿè®¡
        await progress.update_progress(7, 8, "æœ€ç»ˆç»Ÿè®¡è®¡ç®—")
        final_tokens = self.count_messages_tokens(final_messages)
        self.stats.coverage_budget_usage = (
            final_tokens / available_tokens if available_tokens > 0 else 0
        )

        # ç¡®ä¿æ¶ˆæ¯é¡ºåºæ­£ç¡®
        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )

        await progress.update_progress(8, 8, "å¤„ç†å®Œæˆ")

        self.debug_log(
            1,
            f"Coverage-Firstå®Œæˆ: {len(processed_history)} -> {len(final_messages)}æ¡æ¶ˆæ¯",
            "âœ…",
        )
        self.debug_log(
            1,
            f"ç»Ÿè®¡: è¦†ç›–ç‡{self.stats.coverage_rate:.1%}, é¢„ç®—ä½¿ç”¨{self.stats.coverage_budget_usage:.1%}",
            "âœ…",
        )

        await progress.complete_phase(
            f"è¦†ç›–ç‡{self.stats.coverage_rate:.1%} é¢„ç®—ä½¿ç”¨{self.stats.coverage_budget_usage:.1%}"
        )

        return final_messages

    # ========== è§†è§‰å¤„ç† ==========
    def validate_base64_image_data(self, image_data: str) -> bool:
        """éªŒè¯base64å›¾ç‰‡æ•°æ®çš„æœ‰æ•ˆæ€§"""
        return self.input_cleaner.validate_and_clean_image_url(image_data)[0]

    async def describe_image_impl(self, image_data: str, event_emitter):
        """å®é™…çš„å›¾ç‰‡æè¿°å®ç°"""
        client = self.get_api_client()
        if not client:
            return None

        # ä½¿ç”¨InputCleaneréªŒè¯ï¼ˆæ”¯æŒHTTPå’Œdata URIï¼‰
        is_valid, cleaned_data = self.input_cleaner.validate_and_clean_image_url(
            image_data
        )
        if not is_valid:
            self.debug_log(1, "å›¾ç‰‡æ•°æ®éªŒè¯å¤±è´¥", "âš ï¸")
            self.stats.image_processing_errors += 1
            return "å›¾ç‰‡æ ¼å¼é”™è¯¯ï¼šä¸æ˜¯æœ‰æ•ˆçš„URLæˆ–data URI"

        try:
            response = await client.chat.completions.create(
                model=self.valves.multimodal_model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": self.valves.vision_prompt_template,
                            },
                            {"type": "image_url", "image_url": {"url": cleaned_data}},
                        ],
                    }
                ],
                max_tokens=self.valves.vision_max_tokens,
                temperature=0.2,
                timeout=self.valves.request_timeout,
            )

            if response.choices and response.choices[0].message.content:
                description = response.choices[0].message.content.strip()
                description = self.input_cleaner.clean_text_for_regex(description)
                return description
            else:
                self.stats.image_processing_errors += 1
                return "å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼šAPIè¿”å›ç©ºå“åº”"
        except Exception as e:
            self.debug_log(1, f"å›¾ç‰‡è¯†åˆ«å¼‚å¸¸: {str(e)[:100]}", "âŒ")
            self.stats.image_processing_errors += 1
            return f"å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼š{str(e)[:100]}"

    async def describe_image(self, image_data: str, event_emitter) -> str:
        """æè¿°å•å¼ å›¾ç‰‡"""
        if not image_data:
            return "å›¾ç‰‡æ•°æ®ä¸ºç©º"

        description = await self.safe_api_call(
            self.describe_image_impl,
            "å›¾ç‰‡è¯†åˆ«",
            image_data,
            event_emitter,
        )

        if description:
            if len(description) > 3000:
                description = description[:3000] + "..."
            return description
        else:
            self.stats.image_processing_errors += 1
            return "å›¾ç‰‡å¤„ç†å¤±è´¥ï¼šæ— æ³•è·å–æè¿°"

    async def process_message_images(
        self, message: dict, progress: ProgressTracker
    ) -> dict:
        """å¤„ç†å•æ¡æ¶ˆæ¯ä¸­çš„å›¾ç‰‡"""
        content = message.get("content", "")
        if not isinstance(content, list):
            return message

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        images = [item for item in content if item.get("type") == "image_url"]
        if not images:
            return message

        self.debug_log(2, f"å¤„ç†æ¶ˆæ¯ä¸­çš„å›¾ç‰‡: {len(images)}å¼ ", "ğŸ–¼ï¸")

        # å¤„ç†å›¾ç‰‡
        processed_content = []
        image_count = 0

        for item in content:
            if item.get("type") == "text":
                text = item.get("text", "")
                if text.strip():
                    processed_content.append(text)
            elif item.get("type") == "image_url":
                image_count += 1
                image_data = item.get("image_url", {}).get("url", "")
                # æ ¡éªŒ URL / data URI
                is_valid, cleaned = self.input_cleaner.validate_and_clean_image_url(
                    image_data
                )
                if not is_valid:
                    self.stats.image_processing_errors += 1
                    processed_content.append(f"[å›¾ç‰‡{image_count}æ— æ³•è¯†åˆ«]")
                    continue
                if progress:
                    await progress.update_progress(
                        image_count,
                        len(images),
                        f"å¤„ç†å›¾ç‰‡ {image_count}/{len(images)}",
                    )
                # describe_image å†…éƒ¨ä¹Ÿä¼šå†æ¬¡æ ¡éªŒï¼Œè¿™é‡Œä¼  cleaned æˆ–åŸå§‹éƒ½å¯
                description = await self.describe_image(
                    cleaned, progress.event_emitter if progress else None
                )
                image_description = f"[å›¾ç‰‡{image_count}æè¿°] {description}"
                processed_content.append(image_description)

        # åˆ›å»ºæ–°æ¶ˆæ¯
        processed_message = copy.deepcopy(message)
        processed_message["content"] = (
            "\n".join(processed_content) if processed_content else ""
        )
        processed_message["_images_processed"] = image_count

        # æ›´æ–°ç»Ÿè®¡
        self.stats.multimodal_processed += image_count

        return processed_message

    # ========== å¤šæ¨¡æ€å¤„ç†ç­–ç•¥ ==========
    def calculate_multimodal_budget_sufficient(
        self, messages: List[dict], target_tokens: int
    ) -> bool:
        """è®¡ç®—å¤šæ¨¡æ€æ¨¡å‹çš„Tokené¢„ç®—æ˜¯å¦å……è¶³"""
        current_tokens = self.count_messages_tokens(messages)
        usage_ratio = current_tokens / target_tokens if target_tokens > 0 else 1.0
        threshold = self.valves.multimodal_direct_threshold
        is_sufficient = usage_ratio <= threshold

        self.debug_log(
            1,
            f"å¤šæ¨¡æ€é¢„ç®—æ£€æŸ¥: {current_tokens:,}/{target_tokens:,} = {usage_ratio:.2%} {'â‰¤' if is_sufficient else '>'} {threshold:.1%}",
            "ğŸ’°",
        )

        return is_sufficient

    async def determine_multimodal_processing_strategy(
        self, messages: List[dict], model_name: str, target_tokens: int
    ) -> Tuple[str, str]:
        """ç¡®å®šå¤šæ¨¡æ€å¤„ç†ç­–ç•¥"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return "text_only", "æ— å›¾ç‰‡å†…å®¹ï¼ŒæŒ‰æ–‡æœ¬å¤„ç†"

        # åˆ¤æ–­æ¨¡å‹ç±»å‹
        is_multimodal = self.is_multimodal_model(model_name)
        self.debug_log(1, f"æ¨¡å‹åˆ†æ: {model_name} | å¤šæ¨¡æ€æ”¯æŒ: {is_multimodal}", "ğŸ¤–")

        if is_multimodal:
            budget_sufficient = self.calculate_multimodal_budget_sufficient(
                messages, target_tokens
            )
            if budget_sufficient:
                return "direct_multimodal", "å¤šæ¨¡æ€æ¨¡å‹ï¼ŒTokené¢„ç®—å……è¶³ï¼Œç›´æ¥è¾“å…¥"
            else:
                return "multimodal_rag", "å¤šæ¨¡æ€æ¨¡å‹ï¼ŒTokené¢„ç®—ä¸è¶³ï¼Œä½¿ç”¨å¤šæ¨¡æ€å‘é‡RAG"
        else:
            return "vision_to_text", "çº¯æ–‡æœ¬æ¨¡å‹ï¼Œå…ˆè¯†åˆ«å›¾ç‰‡å†å¤„ç†"

    async def process_multimodal_content(
        self,
        messages: List[dict],
        model_name: str,
        target_tokens: int,
        progress: ProgressTracker,
    ) -> List[dict]:
        """å¤šæ¨¡æ€å†…å®¹å¤„ç†"""
        if not self.valves.enable_multimodal:
            return messages

        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return messages

        # ç¡®å®šç­–ç•¥
        strategy, strategy_desc = await self.determine_multimodal_processing_strategy(
            messages, model_name, target_tokens
        )

        self.debug_log(1, f"å¤šæ¨¡æ€ç­–ç•¥: {strategy} - {strategy_desc}", "ğŸ¯")

        if strategy == "text_only":
            return messages
        elif strategy == "direct_multimodal":
            return messages
        elif strategy == "vision_to_text":
            await progress.start_phase("è§†è§‰è¯†åˆ«è½¬æ–‡æœ¬", len(messages))

            # å¹¶å‘å¤„ç†
            semaphore = asyncio.Semaphore(self.valves.max_concurrent_requests)

            async def process_single_message(i, message):
                if self.has_images_in_content(message.get("content")):
                    async with semaphore:
                        processed_message = await self.process_message_images(
                            message, progress
                        )
                        return processed_message
                else:
                    return message

            process_tasks = []
            for i, message in enumerate(messages):
                task = process_single_message(i, message)
                process_tasks.append(task)

            processed_messages = await asyncio.gather(*process_tasks)

            if self.message_order:
                processed_messages = self.message_order.sort_messages_preserve_user(
                    processed_messages, self.current_user_message
                )

            await progress.complete_phase("è§†è§‰è¯†åˆ«å®Œæˆ")
            return processed_messages
        else:
            return messages

    # ========== æ™ºèƒ½æˆªæ–­ ==========
    def smart_truncate_messages(
        self, messages: List[dict], target_tokens: int, preserve_priority: bool = True
    ) -> List[dict]:
        """æ™ºèƒ½æˆªæ–­ç®—æ³•"""
        if not messages:
            return messages

        current_tokens = self.count_messages_tokens(messages)
        if current_tokens <= target_tokens:
            return messages

        self.debug_log(
            1, f"å¼€å§‹æ™ºèƒ½æˆªæ–­: {current_tokens:,} -> {target_tokens:,}tokens", "âœ‚ï¸"
        )
        self.stats.smart_truncation_applied += 1

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        if preserve_priority:
            message_priorities = []
            for i, msg in enumerate(messages):
                priority_score = self._calculate_message_priority(msg, i, len(messages))
                message_priorities.append((i, msg, priority_score))

            message_priorities.sort(key=lambda x: x[2], reverse=True)
        else:
            message_priorities = [(i, msg, 1.0) for i, msg in enumerate(messages)]

        # æ™ºèƒ½é€‰æ‹©
        selected_messages = []
        used_tokens = 0
        skipped_messages = []

        for original_idx, msg, priority in message_priorities:
            msg_tokens = self.count_message_tokens(msg)
            if used_tokens + msg_tokens <= target_tokens:
                selected_messages.append((original_idx, msg, priority))
                used_tokens += msg_tokens
            else:
                skipped_messages.append((original_idx, msg, priority, msg_tokens))
                self.stats.truncation_skip_count += 1

        # å¡«è¡¥ç©ºéš™
        remaining_budget = target_tokens - used_tokens
        if remaining_budget > 100 and skipped_messages:
            skipped_messages.sort(key=lambda x: x[3])
            recovered_count = 0

            for original_idx, msg, priority, msg_tokens in skipped_messages:
                if msg_tokens <= remaining_budget:
                    selected_messages.append((original_idx, msg, priority))
                    used_tokens += msg_tokens
                    remaining_budget -= msg_tokens
                    recovered_count += 1
                    if remaining_budget < 100:
                        break

            self.stats.truncation_recovered_messages += recovered_count

        # æŒ‰åŸå§‹ç´¢å¼•æ’åº
        selected_messages.sort(key=lambda x: x[0])
        final_messages = [msg for _, msg, _ in selected_messages]

        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )

        final_tokens = self.count_messages_tokens(final_messages)
        retention_ratio = len(final_messages) / len(messages) if messages else 0

        self.debug_log(
            1,
            f"æ™ºèƒ½æˆªæ–­å®Œæˆ: {len(messages)} -> {len(final_messages)}æ¡æ¶ˆæ¯ ä¿ç•™ç‡{retention_ratio:.1%}",
            "âœ…",
        )

        return final_messages

    def _calculate_message_priority(
        self, msg: dict, index: int, total_count: int
    ) -> float:
        """è®¡ç®—æ¶ˆæ¯ä¼˜å…ˆçº§åˆ†æ•° - å¢åŠ é•¿åº¦æƒ©ç½š"""
        priority = 1.0

        # è§’è‰²ä¼˜å…ˆçº§
        role = msg.get("role", "")
        if role == "user":
            priority += 2.0
        elif role == "assistant":
            priority += 1.5
        elif role == "system":
            priority += 3.0

        # ä½ç½®ä¼˜å…ˆçº§
        position_score = index / total_count if total_count > 0 else 0
        priority += position_score * 2.0

        # å†…å®¹ä¼˜å…ˆçº§
        content_text = self.extract_text_from_content(msg.get("content", ""))
        if self.is_high_priority_content(content_text):
            priority += 1.5

        # é•¿åº¦å› å­ - å¢å¼ºé•¿åº¦æƒ©ç½š
        content_length = len(content_text)
        if 100 < content_length < 2000:
            priority += 0.5
        elif content_length > 5000:
            priority -= 1.0  # å¢åŠ æƒ©ç½š
        elif content_length > 10000:
            priority -= 2.0  # æé•¿æ–‡æœ¬é‡åº¦æƒ©ç½š

        # å¤šæ¨¡æ€å†…å®¹ä¼˜å…ˆçº§
        if self.has_images_in_content(msg.get("content")):
            priority += 1.0

        # æ‘˜è¦æ¶ˆæ¯çš„ä¼˜å…ˆçº§
        if msg.get("_is_summary"):
            priority += 0.8

        # åˆ†ç‰‡æ¶ˆæ¯çš„ä¼˜å…ˆçº§
        if msg.get("_is_chunk"):
            priority += 0.3

        return priority

    # ========== ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤ ==========
    def ensure_current_user_message_preserved(
        self, final_messages: List[dict]
    ) -> List[dict]:
        """ç¡®ä¿å½“å‰ç”¨æˆ·æ¶ˆæ¯è¢«æ­£ç¡®ä¿ç•™åœ¨æœ€åä½ç½®"""
        if not self.current_user_message:
            return final_messages

        # æ£€æŸ¥å½“å‰ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦åœ¨æœ€åä½ç½®
        if final_messages and final_messages[-1].get("role") == "user":
            current_id = self.current_user_message.get("_order_id")
            last_id = final_messages[-1].get("_order_id")
            if current_id == last_id:
                return final_messages

        # ä¿®å¤ä½ç½®
        self.debug_log(1, "æ£€æµ‹åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½ç½®é”™è¯¯ï¼Œå¼€å§‹ä¿®å¤", "ğŸ›¡ï¸")

        current_id = self.current_user_message.get("_order_id")
        filtered_messages = []

        for msg in final_messages:
            if msg.get("_order_id") != current_id:
                filtered_messages.append(msg)

        filtered_messages.append(self.current_user_message)

        self.stats.user_message_recovery_count += 1
        self.debug_log(1, "å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½ç½®ä¿®å¤å®Œæˆ", "ğŸ›¡ï¸")

        return filtered_messages

    # ========== ä¸»è¦å¤„ç†é€»è¾‘ ==========
    def should_force_maximize_content(
        self, messages: List[dict], target_tokens: int
    ) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼ºåˆ¶è¿›è¡Œå†…å®¹æœ€å¤§åŒ–å¤„ç†"""
        current_tokens = self.count_messages_tokens(messages)
        utilization = current_tokens / target_tokens if target_tokens > 0 else 0

        should_maximize = (
            utilization < self.valves.max_window_utilization
            or current_tokens > target_tokens
        )

        self.debug_log(
            1,
            f"å†…å®¹æœ€å¤§åŒ–åˆ¤æ–­: {current_tokens:,}tokens / {target_tokens:,}tokens = {utilization:.1%}",
            "ğŸ”¥",
        )
        self.debug_log(1, f"éœ€è¦æœ€å¤§åŒ–: {should_maximize}", "ğŸ”¥")

        return should_maximize

    async def maximize_content_comprehensive_processing_v2(
        self, messages: List[dict], target_tokens: int, progress: ProgressTracker
    ) -> List[dict]:
        """å†…å®¹æœ€å¤§åŒ–ç»¼åˆå¤„ç† v2.4.5"""
        start_time = time.time()

        # è·å–çœŸå®æ¨¡å‹é™åˆ¶
        current_model_name = getattr(self, "_current_model_name", "unknown")
        if hasattr(self, "current_model_info") and self.current_model_info:
            model_limit = self.current_model_info.get(
                "limit", self.valves.default_token_limit
            )
            safe_limit = int(model_limit * self.valves.token_safety_ratio)
        else:
            safe_limit = self.get_model_token_limit(current_model_name)

        # åˆå§‹åŒ–ç»Ÿè®¡
        self.stats.original_tokens = self.count_messages_tokens(messages)
        self.stats.original_messages = len(messages)
        self.stats.token_limit = safe_limit
        self.stats.target_tokens = target_tokens
        current_tokens = self.stats.original_tokens

        self.debug_log(
            1,
            f"Coverage-Firstå¤„ç†å¼€å§‹: {current_tokens:,} tokens, {len(messages)} æ¡æ¶ˆæ¯",
            "ğŸ¯",
        )

        await progress.start_phase("Coverage-Firstå¤„ç†", 10)

        # 1. æ¶ˆæ¯åˆ†ç¦»
        await progress.update_progress(1, 10, "åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯")
        current_user_message, history_messages = (
            self.separate_current_and_history_messages(messages)
        )
        self.current_user_message = current_user_message

        # ç³»ç»Ÿæ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.get("role") == "system"]

        if current_user_message:
            self.stats.current_user_tokens = self.count_message_tokens(
                current_user_message
            )

        # 2. æ£€æµ‹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚
        need_context_max = False
        if current_user_message and self.valves.enable_context_maximization:
            query_text = self.extract_text_from_content(
                current_user_message.get("content", "")
            )
            need_context_max = await self.detect_context_max_need(
                query_text, progress.event_emitter
            )
            if need_context_max:
                self.debug_log(
                    1, f"æ£€æµ‹åˆ°éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ï¼Œå¯ç”¨Coverage-Firstç­–ç•¥", "ğŸ“š"
                )

        # 3. è®¡ç®—ä¿æŠ¤æ¶ˆæ¯çš„token
        protected_messages = system_messages[:]
        protected_tokens = self.count_messages_tokens(protected_messages)
        available_for_processing = (
            target_tokens - protected_tokens - self.stats.current_user_tokens
        )

        self.debug_log(
            1, f"å†å²æ¶ˆæ¯å¯ç”¨å¤„ç†ç©ºé—´: {available_for_processing:,}tokens", "ğŸ’°"
        )

        # 4. å¤„ç†å†å²æ¶ˆæ¯
        if not history_messages:
            final_messages = system_messages[:]
            if current_user_message:
                final_messages.append(current_user_message)
            await progress.complete_phase("æ— å†å²æ¶ˆæ¯éœ€è¦å¤„ç†")
            return final_messages

        # 5. ä½¿ç”¨Coverage-First v2.4.5ç­–ç•¥
        if (
            need_context_max
            and self.valves.enable_context_maximization
            and self.valves.enable_coverage_first
        ):
            await progress.update_progress(2, 10, "Coverage-Firstä¸“ç”¨å¤„ç†")
            processed_history = (
                await self.process_coverage_first_context_maximization_v2(
                    history_messages,
                    available_for_processing,
                    progress,
                    current_user_message,
                )
            )
        else:
            await progress.update_progress(2, 10, "æ ‡å‡†æˆªæ–­å¤„ç†")
            if available_for_processing > 0:
                processed_history = self.smart_truncate_messages(
                    history_messages, available_for_processing, True
                )
            else:
                processed_history = []

        # 6. é›¶ä¸¢å¤±ä¿éšœæ£€æŸ¥
        await progress.update_progress(6, 10, "é›¶ä¸¢å¤±ä¿éšœæ£€æŸ¥")
        final_history = processed_history
        final_tokens = self.count_messages_tokens(final_history)

        if (
            final_tokens > available_for_processing
            and self.valves.disable_insurance_truncation
        ):
            self.debug_log(1, f"é¢„ç®—è¶…é™ä½†ç¦ç”¨æˆªæ–­ï¼Œä¿è¯é›¶ä¸¢å¤±", "ğŸ›¡ï¸")
            self.stats.insurance_truncation_avoided += 1
        elif final_tokens > available_for_processing:
            self.debug_log(1, f"è¶…å‡ºé¢„ç®—ï¼Œå¯ç”¨ä¿é™©æˆªæ–­", "âœ‚ï¸")
            final_history = self.smart_truncate_messages(
                final_history, available_for_processing, True
            )
            final_tokens = self.count_messages_tokens(final_history)
            self.stats.zero_loss_guarantee = False

        # 7. ç»„åˆæœ€ç»ˆç»“æœ
        await progress.update_progress(8, 10, "ç»„åˆæœ€ç»ˆç»“æœ")
        current_result = system_messages + final_history

        if self.message_order:
            current_result = self.message_order.sort_messages_preserve_user(
                current_result, self.current_user_message
            )

        final_messages = []
        for msg in current_result:
            final_messages.append(msg)

        if current_user_message:
            final_messages.append(current_user_message)

        # 8. ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤
        await progress.update_progress(9, 10, "ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤éªŒè¯")
        final_messages = self.ensure_current_user_message_preserved(final_messages)

        # 9. æ›´æ–°ç»Ÿè®¡
        await progress.update_progress(10, 10, "æ›´æ–°ç»Ÿè®¡")
        self.stats.final_tokens = self.count_messages_tokens(final_messages)
        self.stats.final_messages = len(final_messages)
        self.stats.processing_time = time.time() - start_time
        self.stats.iterations = 1

        if self.stats.original_tokens > 0:
            self.stats.content_loss_ratio = max(
                0,
                (self.stats.original_tokens - self.stats.final_tokens)
                / self.stats.original_tokens,
            )

        if target_tokens > 0:
            self.stats.window_utilization = self.stats.final_tokens / target_tokens

        if current_user_message:
            self.stats.current_user_preserved = any(
                msg.get("_order_id") == current_user_message.get("_order_id")
                for msg in final_messages
            )

        retention_ratio = self.stats.calculate_retention_ratio()
        window_usage = self.stats.calculate_window_usage_ratio()

        self.debug_log(
            1,
            f"Coverage-Firstå¤„ç†å®Œæˆ: ä¿ç•™{retention_ratio:.1%} çª—å£ä½¿ç”¨{window_usage:.1%} é›¶ä¸¢å¤±{'ä¿éšœæˆåŠŸ' if self.stats.zero_loss_guarantee else 'éƒ¨åˆ†å¤±æ•ˆ'}",
            "ğŸ¯",
        )

        await progress.complete_phase(
            f"è¦†ç›–ç‡{self.stats.coverage_rate:.1%} é¢„ç®—ä½¿ç”¨{window_usage:.1%} "
            f"é›¶ä¸¢å¤±{'æˆåŠŸ' if self.stats.zero_loss_guarantee else 'å¤±æ•ˆ'} "
            f"{'[ä¸Šä¸‹æ–‡æœ€å¤§åŒ–]' if need_context_max else '[å…·ä½“æŸ¥è¯¢]'}"
        )

        return final_messages

    def print_detailed_stats(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.valves.enable_detailed_stats:
            return

        print("\n" + "=" * 60)
        print(self.stats.get_summary())
        print("=" * 60)

    # ========== ä¸»è¦å…¥å£å‡½æ•° ==========
    async def inlet(
        self,
        body: dict,
        user: Optional[dict] = None,
        __event_emitter__: Optional[Callable] = None,
    ) -> dict:
        """å…¥å£å‡½æ•° v2.4.5 - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        if self.valves.debug_level >= 1:
            print("ğŸš€ Coverage-Firstä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯åŠ¨")

        if not self.valves.enable_processing:
            return body

        messages = body.get("messages", [])
        if not messages:
            return body

        model_name = body.get("model", "æœªçŸ¥")
        if self.is_model_excluded(model_name):
            return body

        # é‡ç½®å¤„ç†çŠ¶æ€
        self.reset_processing_state()

        # ä¿å­˜å½“å‰æ¨¡å‹å
        self._current_model_name = model_name

        # åˆ†ææ¨¡å‹ä¿¡æ¯
        self.current_model_info = self.analyze_model(model_name)

        # åˆ›å»ºè¿›åº¦è¿½è¸ªå™¨
        progress = ProgressTracker(__event_emitter__)

        # åˆå§‹åŒ–æ¶ˆæ¯é¡ºåºç®¡ç†å™¨ï¼ˆä¸å†deepcopyï¼Œç›´æ¥åœ¨åŸæ¶ˆæ¯ä¸Šæ‰“æ ‡ç­¾ï¼‰
        self.message_order = MessageOrder(messages)

        # ã€å…³é”®ä¿®å¤ã€‘ï¼šä½¿ç”¨å¸¦_order_idçš„æ¶ˆæ¯åˆ—è¡¨
        messages = self.message_order.original_messages

        # æ¶ˆæ¯åˆ†ç¦»ï¼ˆä½¿ç”¨å¸¦IDçš„æ¶ˆæ¯ï¼‰
        current_user_message, history_messages = (
            self.separate_current_and_history_messages(messages)
        )
        self.current_user_message = current_user_message

        # Tokenåˆ†æ
        original_tokens = self.count_messages_tokens(messages)
        model_token_limit = self.get_model_token_limit(model_name)
        current_user_tokens = (
            self.count_message_tokens(current_user_message)
            if current_user_message
            else 0
        )
        target_tokens = self.calculate_target_tokens(model_name, current_user_tokens)

        # æ›´æ–°ç»Ÿè®¡
        self.stats.token_limit = model_token_limit
        self.stats.target_tokens = target_tokens
        self.stats.current_user_tokens = current_user_tokens

        if self.valves.debug_level >= 1:
            print(
                f"æ¨¡å‹: {self.current_model_info['family']} | tokens: {original_tokens:,}/{model_token_limit:,} | å†å²: {len(history_messages)}æ¡"
            )

        # ç”Ÿæˆå¤„ç†ID
        if current_user_message:
            content_preview = self.message_order.get_message_preview(
                current_user_message
            )
            processing_id = hashlib.md5(
                f"{current_user_message.get('_order_id', '')}{content_preview}{time.time()}".encode()
            ).hexdigest()[:8]
            self.current_processing_id = processing_id

            # AIæ£€æµ‹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–
            query_text = self.extract_text_from_content(
                current_user_message.get("content", "")
            )
            if self.valves.enable_ai_context_max_detection:
                try:
                    need_context_max = await self.detect_context_max_need(
                        query_text, __event_emitter__
                    )
                    if self.valves.debug_level >= 1:
                        print(
                            f"ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹: {'éœ€è¦' if need_context_max else 'ä¸éœ€è¦'}"
                        )
                except Exception as e:
                    if self.valves.debug_level >= 1:
                        print(f"AIæ£€æµ‹å¤±è´¥: {e}")
                    need_context_max = self.is_context_max_need_simple(query_text)

        # åˆ¤æ–­æ˜¯å¦éœ€è¦æœ€å¤§åŒ–
        should_maximize = self.should_force_maximize_content(messages, target_tokens)

        try:
            # 1. å¤šæ¨¡æ€å¤„ç†
            if self.valves.enable_detailed_progress:
                await progress.start_phase("å¤šæ¨¡æ€å¤„ç†", 1)

            processed_messages = await self.process_multimodal_content(
                messages, model_name, target_tokens, progress
            )
            processed_tokens = self.count_messages_tokens(processed_messages)

            # 2. Coverage-First v2.4.5å†…å®¹æœ€å¤§åŒ–å¤„ç†
            if should_maximize:
                final_messages = (
                    await self.maximize_content_comprehensive_processing_v2(
                        processed_messages, target_tokens, progress
                    )
                )

                # æ‰“å°è¯¦ç»†ç»Ÿè®¡
                self.print_detailed_stats()

                body["messages"] = copy.deepcopy(final_messages)

                # æœ€ç»ˆç»Ÿè®¡
                final_tokens = self.count_messages_tokens(final_messages)
                window_utilization = (
                    final_tokens / target_tokens if target_tokens > 0 else 0
                )

                if self.valves.debug_level >= 1:
                    print(
                        f"å¤„ç†å®Œæˆ: {len(final_messages)}æ¡æ¶ˆæ¯, {final_tokens:,}tokens, åˆ©ç”¨ç‡{window_utilization:.1%}, é›¶ä¸¢å¤±{'âœ…' if self.stats.zero_loss_guarantee else 'âš ï¸'}"
                    )

                # éªŒè¯å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤
                if current_user_message and final_messages:
                    last_msg = final_messages[-1]
                    if last_msg.get("role") == "user" and self.valves.debug_level >= 1:
                        print(f"å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤: âœ…")

            else:
                # ç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯
                self.stats.original_tokens = self.count_messages_tokens(messages)
                self.stats.original_messages = len(messages)
                self.stats.final_tokens = processed_tokens
                self.stats.final_messages = len(processed_messages)

                if self.valves.enable_detailed_progress:
                    await progress.complete_phase("æ— éœ€æœ€å¤§åŒ–å¤„ç†")

                body["messages"] = copy.deepcopy(processed_messages)

                if self.valves.debug_level >= 1:
                    print(f"ç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯")

        except Exception as e:
            print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
            self.stats.api_failures += 1
            import traceback

            if self.valves.debug_level >= 2:
                traceback.print_exc()

            if self.valves.enable_detailed_progress:
                await progress.update_status(f"å¤„ç†å¤±è´¥: {str(e)[:50]}", True)

        if self.valves.debug_level >= 1:
            print("ğŸ Coverage-Firstå¤„ç†å®Œæˆ")

        return body

    async def outlet(
        self,
        body: dict,
        user: Optional[dict] = None,
        __event_emitter__: Optional[Callable] = None,
    ) -> dict:
        """å‡ºå£å‡½æ•°"""
        return body

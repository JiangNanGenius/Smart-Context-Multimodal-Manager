"""
title: üöÄ Advanced Multimodal Context Manager
author: JiangNanGenius
version: 1.1.0
license: MIT
required_open_webui_version: 0.5.17
description: Êô∫ËÉΩÈïø‰∏ä‰∏ãÊñáÂíåÂ§öÊ®°ÊÄÅÂÜÖÂÆπÂ§ÑÁêÜÂô®ÔºåÊîØÊåÅÂêëÈáèÂåñÊ£ÄÁ¥¢„ÄÅËØ≠‰πâÈáçÊéíÂ∫è„ÄÅÈÄíÂΩíÊÄªÁªìÁ≠âÂäüËÉΩ
"""
import json
import hashlib
import asyncio
import re
from typing import Optional, List, Dict, Callable, Any, Tuple, Union
from pydantic import BaseModel, Field
from enum import Enum

try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    tiktoken = None

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None

MULTIMODAL_MODELS = {
    "gpt-4o", "gpt-4o-mini", "gpt-4-vision-preview",
    "doubao-1.5-vision-pro", "doubao-1.5-vision-lite",
    "claude-3", "gemini-pro-vision"
}

MODEL_TOKEN_LIMITS = {
    "gpt-4o": 128000, "gpt-4o-mini": 128000, "gpt-4": 8192, "gpt-3.5-turbo": 16385,
    "doubao-1.5-thinking-pro": 128000, "doubao-1.5-vision-pro": 128000,
    "claude-3": 200000, "gemini-pro": 128000,
}

class VectorStrategy(str, Enum):
    AUTO = "auto"
    MULTIMODAL_FIRST = "multimodal_first"
    TEXT_FIRST = "text_first"
    MIXED = "mixed"
    FALLBACK = "fallback"
    VISION_TO_TEXT = "vision_to_text"

class Filter:
    class Valves(BaseModel):
        # Âü∫Á°ÄÈÖçÁΩÆ
        enable_processing: bool = Field(default=True, description="üîÑ ÂêØÁî®Êèí‰ª∂ÂäüËÉΩ")
        enable_multimodal: bool = Field(default=True, description="üñºÔ∏è ÂêØÁî®Â§öÊ®°ÊÄÅÂ§ÑÁêÜ")
        enable_vision_preprocessing: bool = Field(default=True, description="üëÅÔ∏è ÂêØÁî®ÂõæÁâáÈ¢ÑÂ§ÑÁêÜ")
        force_truncate_first: bool = Field(default=True, description="‚úÇÔ∏è Âº∫Âà∂ÂÖàÊ£ÄÊü•Êà™Êñ≠")
        
        # Ë∞ÉËØïÈÖçÁΩÆ
        debug_level: int = Field(default=1, description="üêõ Ë∞ÉËØïÁ∫ßÂà´ 0-3")
        show_frontend_progress: bool = Field(default=True, description="üì± ÊòæÁ§∫Â§ÑÁêÜËøõÂ∫¶")
        
        # TokenÁÆ°ÁêÜ
        default_token_limit: int = Field(default=120000, description="‚öñÔ∏è ÈªòËÆ§tokenÈôêÂà∂")
        token_safety_ratio: float = Field(default=0.85, description="üõ°Ô∏è TokenÂÆâÂÖ®ÊØî‰æã")
        preserve_last_messages: int = Field(default=2, description="üíæ ‰øùÁïôÊúÄÂêéÊ∂àÊÅØÊï∞")
        context_preserve_ratio: float = Field(default=0.6, description="üìù ‰∏ä‰∏ãÊñá‰øùÁïôÊØî‰æã")
        
        # VisionÈÖçÁΩÆ
        vision_api_base: str = Field(default="https://ark.cn-beijing.volces.com/api/v3", description="üëÅÔ∏è Vision APIÂú∞ÂùÄ")
        vision_api_key: str = Field(default="", description="üîë Vision APIÂØÜÈí•")
        vision_model: str = Field(default="doubao-1.5-vision-pro-250328", description="üß† VisionÊ®°Âûã")
        vision_prompt_template: str = Field(default="ËØ∑ËØ¶ÁªÜÊèèËø∞ËøôÂº†ÂõæÁâáÁöÑÂÜÖÂÆπÔºåÂåÖÊã¨‰∏ªË¶ÅÂØπË±°„ÄÅÂú∫ÊôØ„ÄÅÊñáÂ≠ó„ÄÅÈ¢úËâ≤„ÄÅÂ∏ÉÂ±ÄÁ≠âÊâÄÊúâÂèØËßÅ‰ø°ÊÅØ„ÄÇÊèèËø∞Ë¶ÅÂáÜÁ°Æ„ÄÅÂÖ∑‰Ωì„ÄÅÂÆåÊï¥Ôºå‰æø‰∫éÂêéÁª≠ÁöÑËØ≠‰πâÊ£ÄÁ¥¢„ÄÇ", description="üëÅÔ∏è VisionÊèêÁ§∫ËØç")
        
        # Â§öÊ®°ÊÄÅÂêëÈáè
        enable_multimodal_vector: bool = Field(default=True, description="üñºÔ∏è ÂêØÁî®Â§öÊ®°ÊÄÅÂêëÈáè")
        multimodal_vector_api_base: str = Field(default="https://ark.cn-beijing.volces.com/api/v3", description="üîó Â§öÊ®°ÊÄÅÂêëÈáèAPI")
        multimodal_vector_api_key: str = Field(default="", description="üîë Â§öÊ®°ÊÄÅÂêëÈáèÂØÜÈí•")
        multimodal_vector_model: str = Field(default="doubao-embedding-vision-250615", description="üß† Â§öÊ®°ÊÄÅÂêëÈáèÊ®°Âûã")
        
        # ÊñáÊú¨ÂêëÈáè
        enable_text_vector: bool = Field(default=True, description="üìù ÂêØÁî®ÊñáÊú¨ÂêëÈáè")
        text_vector_api_base: str = Field(default="https://ark.cn-beijing.volces.com/api/v3", description="üîó ÊñáÊú¨ÂêëÈáèAPI")
        text_vector_api_key: str = Field(default="", description="üîë ÊñáÊú¨ÂêëÈáèÂØÜÈí•")
        text_vector_model: str = Field(default="doubao-embedding-large-text-250515", description="üß† ÊñáÊú¨ÂêëÈáèÊ®°Âûã")
        
        # ÂêëÈáèÁ≠ñÁï•
        vector_strategy: VectorStrategy = Field(default=VectorStrategy.AUTO, description="üéØ ÂêëÈáèÂåñÁ≠ñÁï•")
        vector_similarity_threshold: float = Field(default=0.5, description="üéØ Âü∫Á°ÄÁõ∏‰ººÂ∫¶ÈòàÂÄº")
        multimodal_similarity_threshold: float = Field(default=0.45, description="üñºÔ∏è Â§öÊ®°ÊÄÅÁõ∏‰ººÂ∫¶ÈòàÂÄº")
        text_similarity_threshold: float = Field(default=0.55, description="üìù ÊñáÊú¨Áõ∏‰ººÂ∫¶ÈòàÂÄº")
        
        # ÈáçÊéíÂ∫è
        enable_reranking: bool = Field(default=True, description="üîÑ ÂêØÁî®ÈáçÊéíÂ∫è")
        rerank_api_base: str = Field(default="https://api.bochaai.com", description="üîÑ ÈáçÊéíÂ∫èAPI")
        rerank_api_key: str = Field(default="", description="üîë ÈáçÊéíÂ∫èÂØÜÈí•")
        rerank_model: str = Field(default="gte-rerank", description="üß† ÈáçÊéíÂ∫èÊ®°Âûã")
        rerank_top_k: int = Field(default=10, description="üîù ÈáçÊéíÂ∫èËøîÂõûÊï∞Èáè")
        
        # ÊëòË¶ÅÈÖçÁΩÆ
        summary_api_base: str = Field(default="https://ark.cn-beijing.volces.com/api/v3", description="üìù ÊëòË¶ÅAPI")
        summary_api_key: str = Field(default="", description="üîë ÊëòË¶ÅÂØÜÈí•")
        summary_model: str = Field(default="doubao-1.5-thinking-pro-250415", description="üß† ÊëòË¶ÅÊ®°Âûã")
        max_summary_length: int = Field(default=3000, description="üìè ÊëòË¶ÅÊúÄÂ§ßÈïøÂ∫¶")
        max_recursion_depth: int = Field(default=3, description="üîÑ ÊúÄÂ§ßÈÄíÂΩíÊ∑±Â∫¶")
        
        # ÊÄßËÉΩÈÖçÁΩÆ
        max_concurrent_requests: int = Field(default=3, description="‚ö° ÊúÄÂ§ßÂπ∂ÂèëÊï∞")
        request_timeout: int = Field(default=60, description="‚è±Ô∏è ËØ∑Ê±ÇË∂ÖÊó∂(Áßí)")
        chunk_size: int = Field(default=1000, description="üìÑ ÂàÜÁâáÂ§ßÂ∞è")
        overlap_size: int = Field(default=100, description="üîó ÈáçÂè†Â§ßÂ∞è")

    def __init__(self):
        self.valves = self.Valves()
        self.toggle = True
        self.icon = """xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGZpbGw9Im5vbmUiIHZpZXdCb3g9IjAgMCAyNCAyNCIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZT0iY3VycmVudENvbG9yIj4KICA8cGF0aCBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiIGQ9Ik0xMiAzdjE4bTktOWwtOS05LTkgOSIgLz4KPC9zdmc+"""
        
        self._vision_client = None
        self._encoding = None
        self.vision_cache = {}
        self.vector_cache = {}

    def debug_log(self, level: int, message: str, emoji: str = "üîß"):
        if self.valves.debug_level >= level:
            prefix = ["", "üêõ", "üîç", "üìã"][min(level, 3)]
            print(f"{prefix} {emoji} {message}")

    def get_encoding(self):
        if not TIKTOKEN_AVAILABLE or self._encoding:
            return self._encoding
        try:
            self._encoding = tiktoken.get_encoding("cl100k_base")
        except:
            pass
        return self._encoding

    def count_tokens(self, text: str) -> int:
        if not text:
            return 0
        encoding = self.get_encoding()
        if encoding:
            try:
                return len(encoding.encode(text))
            except:
                pass
        return len(text) // 4

    def count_message_tokens(self, message: dict) -> int:
        content = message.get("content", "")
        total_tokens = 0
        
        if isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    total_tokens += self.count_tokens(item.get("text", ""))
                elif item.get("type") == "image_url":
                    total_tokens += 1000
        else:
            total_tokens = self.count_tokens(str(content))
        
        return total_tokens + self.count_tokens(message.get("role", "")) + 4

    def count_messages_tokens(self, messages: List[dict]) -> int:
        return sum(self.count_message_tokens(msg) for msg in messages)

    def get_model_token_limit(self, model_name: str) -> int:
        for model_key, limit in MODEL_TOKEN_LIMITS.items():
            if model_key in model_name.lower():
                return int(limit * self.valves.token_safety_ratio)
        return int(self.valves.default_token_limit * self.valves.token_safety_ratio)

    def is_multimodal_model(self, model_name: str) -> bool:
        return any(mm in model_name.lower() for mm in MULTIMODAL_MODELS)

    def has_images_in_content(self, content) -> bool:
        if isinstance(content, list):
            return any(item.get("type") == "image_url" for item in content)
        return False

    def has_images_in_messages(self, messages: List[dict]) -> bool:
        return any(self.has_images_in_content(msg.get("content")) for msg in messages)

    async def send_status(self, __event_emitter__, message: str, done: bool = True, emoji: str = "üîÑ"):
        if __event_emitter__ and self.valves.show_frontend_progress:
            try:
                await __event_emitter__({
                    "type": "status",
                    "data": {"description": f"{emoji} {message}", "done": done}
                })
            except:
                pass

    # VisionÂ§ÑÁêÜ
    def get_vision_client(self):
        if not OPENAI_AVAILABLE or self._vision_client:
            return self._vision_client
        
        api_key = self.valves.vision_api_key
        if not api_key:
            if self.valves.multimodal_vector_api_key:
                api_key = self.valves.multimodal_vector_api_key
            elif self.valves.text_vector_api_key:
                api_key = self.valves.text_vector_api_key
        
        if api_key:
            self._vision_client = AsyncOpenAI(
                base_url=self.valves.vision_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout
            )
        return self._vision_client

    async def describe_image(self, image_url: str, __event_emitter__) -> str:
        image_hash = hashlib.md5(image_url.encode()).hexdigest()
        if image_hash in self.vision_cache:
            return self.vision_cache[image_hash]
        
        client = self.get_vision_client()
        if not client:
            return "Êó†Ê≥ïÂ§ÑÁêÜÂõæÁâáÔºöVisionÊúçÂä°Êú™ÈÖçÁΩÆ"
        
        try:
            response = await client.chat.completions.create(
                model=self.valves.vision_model,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self.valves.vision_prompt_template},
                        {"type": "image_url", "image_url": {"url": image_url}}
                    ]
                }],
                max_tokens=800,
                temperature=0.2
            )
            
            if response.choices:
                description = response.choices[0].message.content.strip()
                self.vision_cache[image_hash] = description
                return description
            return "ÂõæÁâáÊèèËø∞ÁîüÊàêÂ§±Ë¥•"
        except Exception as e:
            error_msg = f"ÂõæÁâáÂ§ÑÁêÜÈîôËØØ: {str(e)[:100]}"
            self.debug_log(1, error_msg, "‚ùå")
            return error_msg

    async def process_multimodal_content(self, messages: List[dict], model_name: str, __event_emitter__) -> List[dict]:
        """Â§ÑÁêÜÂ§öÊ®°ÊÄÅÂÜÖÂÆπÔºö‰∏∫‰∏çÊîØÊåÅÂõæÁâáÁöÑÊ®°ÂûãÊ∑ªÂä†ËßÜËßâËÉΩÂäõ"""
        if not self.valves.enable_multimodal:
            return messages
        
        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return messages
        
        is_target_multimodal = self.is_multimodal_model(model_name)
        if is_target_multimodal:
            self.debug_log(2, f"ÁõÆÊ†áÊ®°Âûã {model_name} ÊîØÊåÅÂ§öÊ®°ÊÄÅÔºåË∑≥ËøáÈ¢ÑÂ§ÑÁêÜ", "‚úÖ")
            return messages
        
        total_images = sum(len([item for item in msg.get("content", []) if isinstance(msg.get("content"), list) and item.get("type") == "image_url"]) for msg in messages)
        
        await self.send_status(__event_emitter__, f"Ê£ÄÊµãÂà∞ {total_images} Âº†ÂõæÁâáÔºåÂºÄÂßãÂ§ÑÁêÜ...", False, "üñºÔ∏è")
        
        processed_messages = []
        processed_count = 0
        
        for message in messages:
            content = message.get("content", "")
            
            if isinstance(content, list):
                text_parts = []
                for item in content:
                    if item.get("type") == "text":
                        text_parts.append(item.get("text", ""))
                    elif item.get("type") == "image_url":
                        processed_count += 1
                        image_url = item.get("image_url", {}).get("url", "")
                        if image_url:
                            description = await self.describe_image(image_url, __event_emitter__)
                            text_parts.append(f"[ÂõæÁâá{processed_count}] {description}")
                
                processed_message = message.copy()
                processed_message["content"] = " ".join(text_parts) if text_parts else ""
                processed_messages.append(processed_message)
            else:
                processed_messages.append(message)
        
        await self.send_status(__event_emitter__, f"Â§öÊ®°ÊÄÅÂ§ÑÁêÜÂÆåÊàêÔºö{processed_count} Âº†ÂõæÁâá", True, "‚úÖ")
        return processed_messages

    # ÂêëÈáèÂåñÂäüËÉΩ
    def choose_vector_model(self, has_images: bool = False) -> Tuple[str, str, str, str]:
        strategy = self.valves.vector_strategy
        
        if has_images and not self.valves.enable_multimodal_vector and self.valves.enable_text_vector:
            strategy = VectorStrategy.VISION_TO_TEXT
        
        if strategy == VectorStrategy.AUTO:
            if has_images and self.valves.enable_multimodal_vector:
                return (self.valves.multimodal_vector_api_base, self.valves.multimodal_vector_api_key, 
                       self.valves.multimodal_vector_model, "multimodal")
            elif self.valves.enable_text_vector:
                return (self.valves.text_vector_api_base, self.valves.text_vector_api_key, 
                       self.valves.text_vector_model, "text")
        
        elif strategy in [VectorStrategy.TEXT_FIRST, VectorStrategy.VISION_TO_TEXT]:
            if self.valves.enable_text_vector:
                return (self.valves.text_vector_api_base, self.valves.text_vector_api_key, 
                       self.valves.text_vector_model, "text")
        
        elif strategy == VectorStrategy.MULTIMODAL_FIRST:
            if self.valves.enable_multimodal_vector:
                return (self.valves.multimodal_vector_api_base, self.valves.multimodal_vector_api_key, 
                       self.valves.multimodal_vector_model, "multimodal")
        
        if self.valves.enable_text_vector:
            return (self.valves.text_vector_api_base, self.valves.text_vector_api_key, 
                   self.valves.text_vector_model, "text")
        elif self.valves.enable_multimodal_vector:
            return (self.valves.multimodal_vector_api_base, self.valves.multimodal_vector_api_key, 
                   self.valves.multimodal_vector_model, "multimodal")
        
        raise ValueError("Ê≤°ÊúâÂèØÁî®ÁöÑÂêëÈáèÊ®°Âûã")

    async def preprocess_for_text_vector(self, content, __event_emitter__) -> str:
        if isinstance(content, str):
            return content
        
        if isinstance(content, list):
            parts = []
            for item in content:
                if item.get("type") == "text":
                    parts.append(item.get("text", ""))
                elif item.get("type") == "image_url":
                    image_url = item.get("image_url", {}).get("url", "")
                    if image_url and self.valves.enable_vision_preprocessing:
                        description = await self.describe_image(image_url, __event_emitter__)
                        parts.append(f"[ÂõæÁâáÊèèËø∞] {description}")
            return " ".join(parts)
        
        return str(content)

    async def vectorize_content(self, content, __event_emitter__, has_images: bool = False) -> Optional[List[float]]:
        if not HTTPX_AVAILABLE:
            return None
        
        content_str = str(content)[:50]
        cache_key = hashlib.md5(f"{content_str}_{has_images}".encode()).hexdigest()
        if cache_key in self.vector_cache:
            return self.vector_cache[cache_key]
        
        try:
            api_base, api_key, model_name, model_type = self.choose_vector_model(has_images)
        except:
            return None
        
        if not api_key:
            return None
        
        text_content = content
        if model_type == "text" and (has_images or self.has_images_in_content(content)):
            text_content = await self.preprocess_for_text_vector(content, __event_emitter__)
        elif isinstance(content, list):
            text_parts = [item.get("text", "") for item in content if item.get("type") == "text"]
            text_content = " ".join(text_parts)
        
        try:
            async with httpx.AsyncClient(timeout=self.valves.request_timeout) as client:
                response = await client.post(f"{api_base}/embeddings", 
                    headers={"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"},
                    json={"model": model_name, "input": str(text_content), "encoding_format": "float"})
                
                response.raise_for_status()
                result = response.json()
                
                if "data" in result and result["data"]:
                    embedding = result["data"][0]["embedding"]
                    self.vector_cache[cache_key] = embedding
                    return embedding
        except Exception as e:
            self.debug_log(1, f"ÂêëÈáèÂåñÂ§±Ë¥•: {e}", "‚ùå")
        
        return None

    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        try:
            import math
            dot_product = sum(a * b for a, b in zip(vec1, vec2))
            magnitude1 = math.sqrt(sum(a * a for a in vec1))
            magnitude2 = math.sqrt(sum(b * b for b in vec2))
            return dot_product / (magnitude1 * magnitude2) if magnitude1 and magnitude2 else 0
        except:
            return 0

    # ÂàÜÁâáÂíåÊ£ÄÁ¥¢
    async def chunk_messages(self, messages: List[dict]) -> List[Dict]:
        chunks = []
        current_content, current_tokens, current_messages = "", 0, []
        
        for message in messages:
            content = message.get("content", "")
            if isinstance(content, list):
                text_parts = [item.get("text", "") for item in content if item.get("type") == "text"]
                content = " ".join(text_parts)
            
            message_tokens = self.count_tokens(str(content))
            
            if current_tokens + message_tokens > self.valves.chunk_size and current_content:
                chunks.append({
                    "content": current_content,
                    "messages": current_messages.copy(),
                    "tokens": current_tokens,
                    "has_images": any(self.has_images_in_content(msg.get("content")) for msg in current_messages)
                })
                
                sentences = re.split(r'[.!?„ÄÇÔºÅÔºü]+', current_content)
                if len(sentences) > 1:
                    overlap = " ".join(sentences[-2:])
                    current_content = overlap + " " + content
                    current_tokens = self.count_tokens(current_content)
                else:
                    current_content = content
                    current_tokens = message_tokens
                current_messages = [message]
            else:
                current_content += " " + content if current_content else content
                current_tokens += message_tokens
                current_messages.append(message)
        
        if current_content:
            chunks.append({
                "content": current_content,
                "messages": current_messages,
                "tokens": current_tokens,
                "has_images": any(self.has_images_in_content(msg.get("content")) for msg in current_messages)
            })
        
        return chunks

    async def semantic_search(self, query, chunks: List[Dict], __event_emitter__) -> List[Dict]:
        if not chunks:
            return []
        
        await self.send_status(__event_emitter__, f"ËØ≠‰πâÊ£ÄÁ¥¢ {len(chunks)} ‰∏™ÂàÜÁâá...", False, "üîç")
        
        query_text = query
        query_has_images = False
        if isinstance(query, list):
            text_parts = [item.get("text", "") for item in query if item.get("type") == "text"]
            query_text = " ".join(text_parts)
            query_has_images = any(item.get("type") == "image_url" for item in query)
        
        query_vector = await self.vectorize_content(query, __event_emitter__, query_has_images)
        if not query_vector:
            return chunks[:self.valves.rerank_top_k]
        
        scored_chunks = []
        for chunk in chunks:
            chunk_vector = await self.vectorize_content(chunk["content"], __event_emitter__, chunk.get("has_images", False))
            if chunk_vector:
                similarity = self.cosine_similarity(query_vector, chunk_vector)
                threshold = self.valves.multimodal_similarity_threshold if chunk.get("has_images") else self.valves.text_similarity_threshold
                if similarity >= threshold:
                    chunk["similarity_score"] = similarity
                    scored_chunks.append(chunk)
        
        scored_chunks.sort(key=lambda x: x["similarity_score"], reverse=True)
        
        if self.valves.enable_reranking and self.valves.rerank_api_key:
            return await self.rerank_chunks(query_text, scored_chunks[:20], __event_emitter__)
        
        return scored_chunks[:self.valves.rerank_top_k]

    async def rerank_chunks(self, query: str, chunks: List[Dict], __event_emitter__) -> List[Dict]:
        if not HTTPX_AVAILABLE:
            return chunks
        
        try:
            async with httpx.AsyncClient(timeout=self.valves.request_timeout) as client:
                response = await client.post(f"{self.valves.rerank_api_base}/v1/rerank",
                    headers={"Content-Type": "application/json", "Authorization": f"Bearer {self.valves.rerank_api_key}"},
                    json={
                        "model": self.valves.rerank_model,
                        "query": query,
                        "documents": [chunk["content"] for chunk in chunks],
                        "top_n": min(self.valves.rerank_top_k, len(chunks))
                    })
                
                result = response.json()
                if "data" in result and "results" in result["data"]:
                    reranked = []
                    for item in result["data"]["results"]:
                        chunk = chunks[item["index"]].copy()
                        chunk["rerank_score"] = item.get("relevance_score", 0)
                        reranked.append(chunk)
                    return reranked
        except Exception as e:
            self.debug_log(1, f"ÈáçÊéíÂ∫èÂ§±Ë¥•: {e}", "‚ùå")
        
        return chunks

    # ÊëòË¶ÅÂäüËÉΩ
    async def recursive_summarize(self, messages: List[dict], target_tokens: int, __event_emitter__, depth: int = 0) -> List[dict]:
        if depth >= self.valves.max_recursion_depth:
            return self.preserve_essential_messages(messages)
        
        current_tokens = self.count_messages_tokens(messages)
        if current_tokens <= target_tokens:
            return messages
        
        await self.send_status(__event_emitter__, f"Á¨¨{depth+1}ËΩÆÊëòË¶Å ({current_tokens}‚Üí{target_tokens})", False, "üìù")
        
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        other_messages = [msg for msg in messages if msg.get("role") != "system"]
        
        protected_count = self.valves.preserve_last_messages
        protected = other_messages[-protected_count:] if len(other_messages) > protected_count else other_messages
        to_summarize = other_messages[:-protected_count] if len(other_messages) > protected_count else []
        
        if not to_summarize:
            return system_messages + protected
        
        summary_text = await self.summarize_messages(to_summarize, depth)
        summary_message = {"role": "system", "content": f"=== ÂéÜÂè≤ÊëòË¶Å (Á¨¨{depth+1}ËΩÆ) ===\n{summary_text}"}
        
        new_messages = system_messages + [summary_message] + protected
        new_tokens = self.count_messages_tokens(new_messages)
        
        if new_tokens > target_tokens:
            return await self.recursive_summarize(new_messages, target_tokens, __event_emitter__, depth + 1)
        
        return new_messages

    def preserve_essential_messages(self, messages: List[dict]) -> List[dict]:
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        other_messages = [msg for msg in messages if msg.get("role") != "system"]
        return system_messages + (other_messages[-1:] if other_messages else [])

    async def summarize_messages(self, messages: List[dict], depth: int = 0) -> str:
        if not OPENAI_AVAILABLE or not self.valves.summary_api_key:
            return f"ÊëòË¶ÅÂ§±Ë¥•ÔºöAPI‰∏çÂèØÁî® | Ê∂àÊÅØÊï∞: {len(messages)}"
        
        conversation_text = ""
        for i, msg in enumerate(messages):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            if isinstance(content, list):
                text_parts = [item.get("text", "") for item in content if item.get("type") == "text"]
                content = " ".join(text_parts)
            conversation_text += f"[{i+1}] {role}: {content}\n"
        
        try:
            client = AsyncOpenAI(base_url=self.valves.summary_api_base, api_key=self.valves.summary_api_key, timeout=self.valves.request_timeout)
            
            response = await client.chat.completions.create(
                model=self.valves.summary_model,
                messages=[
                    {"role": "system", "content": f"ËØ∑ÁÆÄÊ¥ÅÊëòË¶Å‰ª•‰∏ãÂØπËØùÔºå‰øùÁïôÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇÈïøÂ∫¶ÈôêÂà∂{self.valves.max_summary_length}Â≠óÁ¨¶„ÄÇÊ∑±Â∫¶:{depth}"},
                    {"role": "user", "content": conversation_text}
                ],
                max_tokens=self.valves.max_summary_length,
                temperature=0.2
            )
            
            return response.choices[0].message.content.strip() if response.choices else "ÊëòË¶ÅÁîüÊàêÂ§±Ë¥•"
        except Exception as e:
            return f"ÊëòË¶ÅÂ§±Ë¥•: {str(e)[:100]} | Ê∂àÊÅØÊï∞: {len(messages)}"

    # Ê†∏ÂøÉÂ§ÑÁêÜÈÄªËæë
    async def process_context_with_retrieval(self, messages: List[dict], model_name: str, __event_emitter__) -> List[dict]:
        token_limit = self.get_model_token_limit(model_name)
        current_tokens = self.count_messages_tokens(messages)
        
        if current_tokens <= token_limit:
            return messages
        
        # Ê£ÄÊü•ÂêëÈáèÈÖçÁΩÆ
        if not self.valves.enable_multimodal_vector and not self.valves.enable_text_vector:
            return await self.recursive_summarize(messages, token_limit, __event_emitter__)
        
        # ÊèêÂèñÊü•ËØ¢
        user_query = None
        for msg in reversed(messages):
            if msg.get("role") == "user":
                user_query = msg.get("content", "")
                break
        
        if not user_query:
            return await self.recursive_summarize(messages, token_limit, __event_emitter__)
        
        # ÂàÜÁâáÂíåÊ£ÄÁ¥¢
        protected_count = self.valves.preserve_last_messages * 2
        protected_messages = messages[-protected_count:] if len(messages) > protected_count else messages
        history_messages = messages[:-protected_count] if len(messages) > protected_count else []
        
        if not history_messages:
            return await self.recursive_summarize(messages, token_limit, __event_emitter__)
        
        chunks = await self.chunk_messages(history_messages)
        relevant_chunks = await self.semantic_search(user_query, chunks, __event_emitter__)
        
        # ÊûÑÂª∫Â¢ûÂº∫‰∏ä‰∏ãÊñá
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        enhanced_context = []
        
        if relevant_chunks:
            context_content = ""
            used_tokens = 0
            available_tokens = int(token_limit * self.valves.context_preserve_ratio)
            
            for i, chunk in enumerate(relevant_chunks):
                if used_tokens + chunk["tokens"] <= available_tokens:
                    context_content += f"\n### Áõ∏ÂÖ≥‰∏ä‰∏ãÊñá {i+1}\n{chunk['content']}\n"
                    used_tokens += chunk["tokens"]
                else:
                    break
            
            if context_content:
                enhanced_message = {
                    "role": "system",
                    "content": f"=== Ê£ÄÁ¥¢Â¢ûÂº∫‰∏ä‰∏ãÊñá ===\n{context_content}\n\nËØ∑Âü∫‰∫é‰∏äËø∞‰∏ä‰∏ãÊñáÂõûÁ≠îÁî®Êà∑ÈóÆÈ¢ò„ÄÇ"
                }
                enhanced_context.append(enhanced_message)
        
        final_messages = system_messages + enhanced_context + protected_messages
        final_tokens = self.count_messages_tokens(final_messages)
        
        if final_tokens > token_limit:
            return await self.recursive_summarize(final_messages, token_limit, __event_emitter__)
        
        return final_messages

    # ‰∏ªÂÖ•Âè£
    async def inlet(self, body: dict, user: Optional[dict] = None, __event_emitter__: Optional[Callable] = None) -> dict:
        if not self.toggle or not self.valves.enable_processing:
            return body
        
        messages = body.get("messages", [])
        if not messages:
            return body
        
        model_name = body.get("model", "")
        self.debug_log(1, f"Â§ÑÁêÜÂºÄÂßã: {len(messages)}Êù°Ê∂àÊÅØ, Ê®°Âûã: {model_name}", "üöÄ")
        
        try:
            # 1. Â§öÊ®°ÊÄÅÂ§ÑÁêÜ (Êó†ËÆ∫ÊòØÂê¶Ë∂ÖÈôêÈÉΩËøõË°å)
            processed_messages = await self.process_multimodal_content(messages, model_name, __event_emitter__)
            
            # 2. Ë∂ÖÈôêÊ£ÄÊü•ÂíåÂ§ÑÁêÜ
            if self.valves.force_truncate_first:
                final_messages = await self.process_context_with_retrieval(processed_messages, model_name, __event_emitter__)
                body["messages"] = final_messages
                
                final_tokens = self.count_messages_tokens(final_messages)
                token_limit = self.get_model_token_limit(model_name)
                
                if final_tokens <= token_limit:
                    await self.send_status(__event_emitter__, f"Â§ÑÁêÜÂÆåÊàê ({final_tokens}/{token_limit} tokens)", True, "‚úÖ")
                else:
                    await self.send_status(__event_emitter__, "Â§ÑÁêÜÂêé‰ªçË∂ÖÈôêÔºå‰ΩøÁî®ÂéüÂßãÊ∂àÊÅØ", True, "‚ö†Ô∏è")
                    body["messages"] = processed_messages
            else:
                body["messages"] = processed_messages
        
        except Exception as e:
            self.debug_log(1, f"Â§ÑÁêÜÂ§±Ë¥•: {e}", "‚ùå")
            await self.send_status(__event_emitter__, f"Â§ÑÁêÜÂ§±Ë¥•: {str(e)[:50]}", True, "‚ùå")
        
        return body

    async def outlet(self, body: dict, user: Optional[dict] = None, __event_emitter__: Optional[Callable] = None) -> dict:
        return body
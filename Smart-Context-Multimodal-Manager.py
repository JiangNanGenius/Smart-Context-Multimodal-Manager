"""
title: ğŸš€ Advanced Multimodal Context Manager
author: JiangNanGenius
version: 1.5.2
license: MIT
required_open_webui_version: 0.5.17
description: æ™ºèƒ½é•¿ä¸Šä¸‹æ–‡å’Œå¤šæ¨¡æ€å†…å®¹å¤„ç†å™¨ï¼Œæ”¯æŒå‘é‡åŒ–æ£€ç´¢ã€è¯­ä¹‰é‡æ’åºã€é€’å½’æ€»ç»“ç­‰åŠŸèƒ½ - ä¿®å¤JSONè§£æé”™è¯¯
"""

import json
import hashlib
import asyncio
import re
import base64
import math
from typing import Optional, List, Dict, Callable, Any, Tuple, Union
from pydantic import BaseModel, Field
from enum import Enum

try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    tiktoken = None

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None

class VectorStrategy(str, Enum):
    AUTO = "auto"
    MULTIMODAL_FIRST = "multimodal_first"
    TEXT_FIRST = "text_first"
    MIXED = "mixed"
    FALLBACK = "fallback"
    VISION_TO_TEXT = "vision_to_text"

class MultimodalStrategy(str, Enum):
    ALL_MODELS = "all_models"
    NON_MULTIMODAL_ONLY = "non_multimodal_only"
    CUSTOM_LIST = "custom_list"
    SMART_ADAPTIVE = "smart_adaptive"

class Filter:
    class Valves(BaseModel):
        # åŸºç¡€æ§åˆ¶
        enable_processing: bool = Field(default=True, description="ğŸ”„ å¯ç”¨æ‰€æœ‰å¤„ç†åŠŸèƒ½")
        excluded_models: str = Field(
            default="", description="ğŸš« æ’é™¤æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)"
        )
        
        # å¤šæ¨¡æ€æ¨¡å‹é…ç½®
        multimodal_models: str = Field(
            default="gpt-4o,gpt-4o-mini,gpt-4-vision-preview,doubao-1.5-vision-pro,doubao-1.5-vision-lite,claude-3,gemini-pro-vision,qwen-vl",
            description="ğŸ–¼ï¸ å¤šæ¨¡æ€æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)",
        )
        
        # æ¨¡å‹Tokené™åˆ¶é…ç½®
        model_token_limits: str = Field(
            default="gpt-4o:128000,gpt-4o-mini:128000,gpt-4:8192,gpt-3.5-turbo:16385,doubao-1.5-thinking-pro:128000,doubao-1.5-vision-pro:128000,doubao-seed:50000,doubao:50000,claude-3:200000,gemini-pro:128000",
            description="âš–ï¸ æ¨¡å‹Tokené™åˆ¶é…ç½®(model:limitæ ¼å¼ï¼Œé€—å·åˆ†éš”)",
        )
        
        # å¤šæ¨¡æ€å¤„ç†ç­–ç•¥
        multimodal_processing_strategy: str = Field(
            default="smart_adaptive",
            description="ğŸ–¼ï¸ å¤šæ¨¡æ€å¤„ç†ç­–ç•¥ (all_models|non_multimodal_only|custom_list|smart_adaptive)",
        )
        force_vision_processing_models: str = Field(
            default="gpt-4,gpt-3.5-turbo,doubao-1.5-thinking-pro",
            description="ğŸ” å¼ºåˆ¶è¿›è¡Œè§†è§‰å¤„ç†çš„æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)",
        )
        preserve_images_in_multimodal: bool = Field(
            default=True, description="ğŸ“¸ å¤šæ¨¡æ€æ¨¡å‹æ˜¯å¦ä¿ç•™åŸå§‹å›¾ç‰‡"
        )
        always_process_images_before_summary: bool = Field(
            default=True, description="ğŸ“ æ‘˜è¦å‰æ€»æ˜¯å…ˆå¤„ç†å›¾ç‰‡"
        )
        
        # åŠŸèƒ½å¼€å…³
        enable_multimodal: bool = Field(default=True, description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å¤„ç†")
        enable_vision_preprocessing: bool = Field(
            default=True, description="ğŸ‘ï¸ å¯ç”¨å›¾ç‰‡é¢„å¤„ç†"
        )
        enable_smart_truncation: bool = Field(
            default=True, description="âœ‚ï¸ å¯ç”¨æ™ºèƒ½æˆªæ–­"
        )
        enable_vector_retrieval: bool = Field(
            default=True, description="ğŸ” å¯ç”¨å‘é‡æ£€ç´¢"
        )
        enable_content_maximization: bool = Field(
            default=True, description="ğŸ“ˆ å¯ç”¨å†…å®¹æœ€å¤§åŒ–ä¿ç•™"
        )
        
        # è°ƒè¯•å’Œé”™è¯¯å¤„ç†
        debug_level: int = Field(default=2, description="ğŸ› è°ƒè¯•çº§åˆ« 0-3")
        show_frontend_progress: bool = Field(
            default=True, description="ğŸ“± æ˜¾ç¤ºå¤„ç†è¿›åº¦"
        )
        api_error_retry_times: int = Field(
            default=2, description="ğŸ”„ APIé”™è¯¯é‡è¯•æ¬¡æ•°"
        )
        api_error_retry_delay: float = Field(
            default=1.0, description="â±ï¸ APIé”™è¯¯é‡è¯•å»¶è¿Ÿ(ç§’)"
        )
        
        # Tokenç®¡ç† - æœ€å¤§åŒ–ä¿ç•™ç­–ç•¥
        default_token_limit: int = Field(default=100000, description="âš–ï¸ é»˜è®¤tokené™åˆ¶")
        token_safety_ratio: float = Field(
            default=0.95, description="ğŸ›¡ï¸ Tokenå®‰å…¨æ¯”ä¾‹"
        )
        max_processing_iterations: int = Field(
            default=3, description="ğŸ”„ æœ€å¤§å¤„ç†è¿­ä»£æ¬¡æ•°"
        )
        min_reduction_threshold: int = Field(
            default=2000, description="ğŸ“‰ æœ€å°å‡å°‘é˜ˆå€¼"
        )
        
        # ä¿æŠ¤ç­–ç•¥
        force_preserve_last_user_message: bool = Field(
            default=True, description="ğŸ”’ å¼ºåˆ¶ä¿ç•™ç”¨æˆ·æœ€åæ¶ˆæ¯"
        )
        preserve_recent_exchanges: int = Field(
            default=3, description="ğŸ’¬ ä¿æŠ¤æœ€è¿‘å®Œæ•´å¯¹è¯è½®æ¬¡"
        )
        max_preserve_ratio: float = Field(
            default=0.75, description="ğŸ”’ ä¿æŠ¤æ¶ˆæ¯æœ€å¤§tokenæ¯”ä¾‹"
        )
        max_single_message_tokens: int = Field(
            default=20000, description="ğŸ“ å•æ¡æ¶ˆæ¯æœ€å¤§token"
        )
        
        # Visioné…ç½®
        vision_api_base: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸ‘ï¸ Vision APIåœ°å€",
        )
        vision_api_key: str = Field(default="", description="ğŸ”‘ Vision APIå¯†é’¥")
        vision_model: str = Field(
            default="doubao-1.5-vision-pro-250328", description="ğŸ§  Visionæ¨¡å‹"
        )
        vision_prompt_template: str = Field(
            default="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€æ–‡å­—ã€é¢œè‰²ã€å¸ƒå±€ç­‰æ‰€æœ‰å¯è§ä¿¡æ¯ã€‚ä¿æŒå®¢è§‚å‡†ç¡®ï¼Œé‡ç‚¹çªå‡ºå…³é”®ä¿¡æ¯ã€‚",
            description="ğŸ‘ï¸ Visionæç¤ºè¯",
        )
        vision_max_tokens: int = Field(
            default=1200, description="ğŸ‘ï¸ Visionæœ€å¤§è¾“å‡ºtokens"
        )
        
        # å¤šæ¨¡æ€å‘é‡
        enable_multimodal_vector: bool = Field(
            default=True, description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å‘é‡"
        )
        multimodal_vector_api_base: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸ”— å¤šæ¨¡æ€å‘é‡API",
        )
        multimodal_vector_api_key: str = Field(
            default="", description="ğŸ”‘ å¤šæ¨¡æ€å‘é‡å¯†é’¥"
        )
        multimodal_vector_model: str = Field(
            default="doubao-embedding-vision-250615", description="ğŸ§  å¤šæ¨¡æ€å‘é‡æ¨¡å‹"
        )
        
        # æ–‡æœ¬å‘é‡
        enable_text_vector: bool = Field(default=True, description="ğŸ“ å¯ç”¨æ–‡æœ¬å‘é‡")
        text_vector_api_base: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸ”— æ–‡æœ¬å‘é‡API",
        )
        text_vector_api_key: str = Field(default="", description="ğŸ”‘ æ–‡æœ¬å‘é‡å¯†é’¥")
        text_vector_model: str = Field(
            default="doubao-embedding-large-text-250515", description="ğŸ§  æ–‡æœ¬å‘é‡æ¨¡å‹"
        )
        
        # å‘é‡ç­–ç•¥
        vector_strategy: str = Field(
            default="auto",
            description="ğŸ¯ å‘é‡åŒ–ç­–ç•¥ (auto|multimodal_first|text_first|mixed|fallback|vision_to_text)",
        )
        vector_similarity_threshold: float = Field(
            default=0.4, description="ğŸ¯ åŸºç¡€ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        multimodal_similarity_threshold: float = Field(
            default=0.35, description="ğŸ–¼ï¸ å¤šæ¨¡æ€ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        text_similarity_threshold: float = Field(
            default=0.45, description="ğŸ“ æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        vector_top_k: int = Field(default=25, description="ğŸ” å‘é‡æ£€ç´¢Top-Kæ•°é‡")
        
        # é‡æ’åº
        enable_reranking: bool = Field(default=True, description="ğŸ”„ å¯ç”¨é‡æ’åº")
        rerank_api_base: str = Field(
            default="https://api.bochaai.com", description="ğŸ”„ é‡æ’åºAPI"
        )
        rerank_api_key: str = Field(default="", description="ğŸ”‘ é‡æ’åºå¯†é’¥")
        rerank_model: str = Field(default="gte-rerank", description="ğŸ§  é‡æ’åºæ¨¡å‹")
        rerank_top_k: int = Field(default=20, description="ğŸ” é‡æ’åºè¿”å›æ•°é‡")
        
        # æ‘˜è¦é…ç½®
        summary_api_base: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3", description="ğŸ“ æ‘˜è¦API"
        )
        summary_api_key: str = Field(default="", description="ğŸ”‘ æ‘˜è¦å¯†é’¥")
        summary_model: str = Field(
            default="doubao-1.5-thinking-pro-250415", description="ğŸ§  æ‘˜è¦æ¨¡å‹"
        )
        max_summary_length: int = Field(
            default=4000, description="ğŸ“ æ‘˜è¦æœ€å¤§é•¿åº¦"
        )
        max_recursion_depth: int = Field(
            default=3, description="ğŸ”„ æœ€å¤§é€’å½’æ·±åº¦"
        )
        
        # æ€§èƒ½é…ç½®
        max_concurrent_requests: int = Field(default=3, description="âš¡ æœ€å¤§å¹¶å‘æ•°")
        request_timeout: int = Field(default=60, description="â±ï¸ è¯·æ±‚è¶…æ—¶(ç§’)")
        chunk_size: int = Field(default=1500, description="ğŸ“„ åˆ†ç‰‡å¤§å°")
        overlap_size: int = Field(default=150, description="ğŸ”— é‡å å¤§å°")

    def __init__(self):
        print("\n" + "=" * 60)
        print("ğŸš€ Advanced Multimodal Context Manager v1.5.2")
        print("ğŸ“ æ’ä»¶æ­£åœ¨åˆå§‹åŒ–...")
        print("ğŸ”§ ä¿®å¤JSONè§£æé”™è¯¯...")
        
        self.valves = self.Valves()
        self._vision_client = None
        self._text_vector_client = None
        self._multimodal_vector_client = None
        self._rerank_client = None
        self._encoding = None
        
        self.vision_cache = {}
        self.vector_cache = {}
        self.processing_cache = {}
        self.api_error_cache = {}
        
        # è§£æå¤šæ¨¡æ€æ¨¡å‹é…ç½®
        self.multimodal_models = set()
        if self.valves.multimodal_models:
            self.multimodal_models = {
                model.strip().lower()
                for model in self.valves.multimodal_models.split(",")
                if model.strip()
            }
        
        # è§£ææ¨¡å‹Tokené™åˆ¶é…ç½®
        self.model_token_limits = {}
        if self.valves.model_token_limits:
            for limit_config in self.valves.model_token_limits.split(","):
                if ":" in limit_config:
                    model, limit = limit_config.split(":", 1)
                    try:
                        self.model_token_limits[model.strip().lower()] = int(
                            limit.strip()
                        )
                    except ValueError:
                        pass
        
        print(f"âœ… æ’ä»¶åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ”§ é”™è¯¯é‡è¯•æ¬¡æ•°: {self.valves.api_error_retry_times}")
        print(f"ğŸ”§ é”™è¯¯é‡è¯•å»¶è¿Ÿ: {self.valves.api_error_retry_delay}ç§’")
        print("=" * 60 + "\n")

    def debug_log(self, level: int, message: str, emoji: str = "ğŸ”§"):
        if self.valves.debug_level >= level:
            prefix = ["", "ğŸ›[DEBUG]", "ğŸ”[DETAIL]", "ğŸ“‹[VERBOSE]"][min(level, 3)]
            print(f"{prefix} {emoji} {message}")

    def is_model_excluded(self, model_name: str) -> bool:
        if not self.valves.excluded_models or not model_name:
            return False
        
        excluded_list = [
            model.strip().lower()
            for model in self.valves.excluded_models.split(",")
            if model.strip()
        ]
        
        if not excluded_list:
            return False
        
        model_lower = model_name.lower()
        for excluded_model in excluded_list:
            if excluded_model in model_lower:
                self.debug_log(1, f"æ¨¡å‹ {model_name} åœ¨æ’é™¤åˆ—è¡¨ä¸­", "ğŸš«")
                return True
        
        return False

    def get_encoding(self):
        if not TIKTOKEN_AVAILABLE:
            return None
        
        if self._encoding is None:
            try:
                self._encoding = tiktoken.get_encoding("cl100k_base")
                self.debug_log(3, "tiktokenç¼–ç å™¨å·²åˆå§‹åŒ–", "ğŸ”§")
            except Exception as e:
                self.debug_log(1, f"tiktokenåˆå§‹åŒ–å¤±è´¥: {e}", "âš ï¸")
        
        return self._encoding

    def count_tokens(self, text: str) -> int:
        if not text:
            return 0
        
        text = str(text)
        encoding = self.get_encoding()
        
        if encoding:
            try:
                return len(encoding.encode(text))
            except Exception as e:
                self.debug_log(2, f"tokenè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ä¼°ç®—: {e}", "âš ï¸")
        
        return max(len(text) // 3, len(text.encode("utf-8")) // 4)

    def count_message_tokens(self, message: dict) -> int:
        if not message:
            return 0
        
        content = message.get("content", "")
        role = message.get("role", "")
        total_tokens = 0
        
        if isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    total_tokens += self.count_tokens(item.get("text", ""))
                elif item.get("type") == "image_url":
                    total_tokens += 1500  # å›¾ç‰‡tokenä¼°ç®—
        else:
            total_tokens = self.count_tokens(str(content))
        
        # è§’è‰²å’Œæ ¼å¼å¼€é”€
        total_tokens += self.count_tokens(role) + 10
        return total_tokens

    def count_messages_tokens(self, messages: List[dict]) -> int:
        if not messages:
            return 0
        return sum(self.count_message_tokens(msg) for msg in messages)

    def get_model_token_limit(self, model_name: str) -> int:
        model_lower = model_name.lower()
        
        # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„é™åˆ¶
        for model_key, limit in self.model_token_limits.items():
            if model_key in model_lower:
                safe_limit = int(limit * self.valves.token_safety_ratio)
                self.debug_log(
                    2, f"æ¨¡å‹ {model_name} é™åˆ¶: {limit} -> {safe_limit}", "âš–ï¸"
                )
                return safe_limit
        
        # ä½¿ç”¨é»˜è®¤é™åˆ¶
        safe_limit = int(
            self.valves.default_token_limit * self.valves.token_safety_ratio
        )
        self.debug_log(1, f"æœªçŸ¥æ¨¡å‹ {model_name}, ä½¿ç”¨é»˜è®¤é™åˆ¶: {safe_limit}", "âš ï¸")
        return safe_limit

    def is_multimodal_model(self, model_name: str) -> bool:
        model_lower = model_name.lower()
        return any(mm in model_lower for mm in self.multimodal_models)

    def should_process_images_for_model(self, model_name: str) -> bool:
        if not self.valves.enable_multimodal:
            return False
        
        model_lower = model_name.lower()
        
        # æ£€æŸ¥å¼ºåˆ¶å¤„ç†åˆ—è¡¨
        force_list = [
            m.strip().lower()
            for m in self.valves.force_vision_processing_models.split(",")
            if m.strip()
        ]
        
        if any(force_model in model_lower for force_model in force_list):
            self.debug_log(2, f"æ¨¡å‹ {model_name} åœ¨å¼ºåˆ¶å¤„ç†åˆ—è¡¨ä¸­", "ğŸ”")
            return True
        
        # æ ¹æ®ç­–ç•¥åˆ¤æ–­
        is_multimodal = self.is_multimodal_model(model_name)
        strategy = self.valves.multimodal_processing_strategy.lower()
        
        if strategy == "all_models":
            return True
        elif strategy == "non_multimodal_only":
            return not is_multimodal
        elif strategy == "smart_adaptive":
            return True
        else:
            return not is_multimodal

    def has_images_in_content(self, content) -> bool:
        if isinstance(content, list):
            return any(item.get("type") == "image_url" for item in content)
        return False

    def has_images_in_messages(self, messages: List[dict]) -> bool:
        return any(self.has_images_in_content(msg.get("content")) for msg in messages)

    async def send_status(
        self, __event_emitter__, message: str, done: bool = True, emoji: str = "ğŸ”„"
    ):
        self.debug_log(2, f"çŠ¶æ€: {message}", emoji)
        if __event_emitter__ and self.valves.show_frontend_progress:
            try:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {"description": f"{emoji} {message}", "done": done},
                    }
                )
            except:
                pass

    # ========== å¢å¼ºçš„APIè°ƒç”¨æ–¹æ³• ==========
    def is_json_response(self, content: str) -> bool:
        """æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºJSONæ ¼å¼"""
        if not content:
            return False
        
        content = content.strip()
        return content.startswith('{') or content.startswith('[')

    def extract_error_info(self, content: str) -> str:
        """ä»é”™è¯¯å“åº”ä¸­æå–å…³é”®ä¿¡æ¯"""
        if not content:
            return "ç©ºå“åº”"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºHTMLé”™è¯¯é¡µé¢
        if content.strip().startswith('<!DOCTYPE') or '<html' in content:
            # å°è¯•æå–title
            title_match = re.search(r'<title>(.*?)</title>', content, re.IGNORECASE)
            if title_match:
                return f"HTMLé”™è¯¯é¡µé¢: {title_match.group(1)}"
            return "HTMLé”™è¯¯é¡µé¢"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºJSONé”™è¯¯
        try:
            if self.is_json_response(content):
                error_data = json.loads(content)
                if isinstance(error_data, dict):
                    error_msg = error_data.get('error', error_data.get('message', ''))
                    if error_msg:
                        return f"APIé”™è¯¯: {error_msg}"
            return f"å“åº”å†…å®¹: {content[:200]}..."
        except:
            return f"å“åº”å†…å®¹: {content[:200]}..."

    async def safe_api_call(self, call_func, call_name: str, *args, **kwargs):
        """å®‰å…¨çš„APIè°ƒç”¨åŒ…è£…å™¨"""
        error_key = f"{call_name}_{hash(str(args) + str(kwargs))}"
        
        # æ£€æŸ¥é”™è¯¯ç¼“å­˜
        if error_key in self.api_error_cache:
            cache_time, error_msg = self.api_error_cache[error_key]
            if asyncio.get_event_loop().time() - cache_time < 300:  # 5åˆ†é’Ÿç¼“å­˜
                self.debug_log(1, f"ä½¿ç”¨ç¼“å­˜çš„é”™è¯¯ç»“æœ: {error_msg}", "âš ï¸")
                return None
        
        for attempt in range(self.valves.api_error_retry_times + 1):
            try:
                result = await call_func(*args, **kwargs)
                # æ¸…é™¤é”™è¯¯ç¼“å­˜
                if error_key in self.api_error_cache:
                    del self.api_error_cache[error_key]
                return result
                
            except Exception as e:
                error_msg = str(e)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºJSONè§£æé”™è¯¯
                if "is not valid JSON" in error_msg or "Unexpected token" in error_msg:
                    self.debug_log(1, f"{call_name} JSONè§£æé”™è¯¯: {error_msg}", "âŒ")
                    # è®°å½•åˆ°é”™è¯¯ç¼“å­˜
                    self.api_error_cache[error_key] = (asyncio.get_event_loop().time(), error_msg)
                    return None
                
                if attempt < self.valves.api_error_retry_times:
                    self.debug_log(
                        1, f"{call_name} ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{self.valves.api_error_retry_delay}ç§’åé‡è¯•: {error_msg}", "ğŸ”„"
                    )
                    await asyncio.sleep(self.valves.api_error_retry_delay)
                else:
                    self.debug_log(1, f"{call_name} æœ€ç»ˆå¤±è´¥: {error_msg}", "âŒ")
                    # è®°å½•åˆ°é”™è¯¯ç¼“å­˜
                    self.api_error_cache[error_key] = (asyncio.get_event_loop().time(), error_msg)
                    return None
        
        return None

    # ========== å‘é‡åŒ–åŠŸèƒ½ ==========
    def get_text_vector_client(self):
        if not OPENAI_AVAILABLE:
            return None
        
        if self._text_vector_client:
            return self._text_vector_client
        
        api_key = self.valves.text_vector_api_key
        if not api_key:
            api_key = (
                self.valves.multimodal_vector_api_key or self.valves.vision_api_key
            )
        
        if api_key:
            self._text_vector_client = AsyncOpenAI(
                base_url=self.valves.text_vector_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout,
            )
            self.debug_log(2, "æ–‡æœ¬å‘é‡å®¢æˆ·ç«¯å·²åˆ›å»º", "ğŸ“")
        
        return self._text_vector_client

    def get_multimodal_vector_client(self):
        if not OPENAI_AVAILABLE:
            return None
        
        if self._multimodal_vector_client:
            return self._multimodal_vector_client
        
        api_key = self.valves.multimodal_vector_api_key
        if not api_key:
            api_key = self.valves.text_vector_api_key or self.valves.vision_api_key
        
        if api_key:
            self._multimodal_vector_client = AsyncOpenAI(
                base_url=self.valves.multimodal_vector_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout,
            )
            self.debug_log(2, "å¤šæ¨¡æ€å‘é‡å®¢æˆ·ç«¯å·²åˆ›å»º", "ğŸ–¼ï¸")
        
        return self._multimodal_vector_client

    async def _get_text_embedding_impl(self, text: str, __event_emitter__):
        """å®é™…çš„æ–‡æœ¬å‘é‡è·å–å®ç°"""
        client = self.get_text_vector_client()
        if not client:
            return None
        
        response = await client.embeddings.create(
            model=self.valves.text_vector_model,
            input=[text[:8000]],
            encoding_format="float",
        )
        
        if response.data:
            return response.data[0].embedding
        return None

    async def get_text_embedding(
        self, text: str, __event_emitter__
    ) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬å‘é‡"""
        if not text or not self.valves.enable_text_vector:
            return None
        
        text_hash = hashlib.md5(text.encode()).hexdigest()
        cache_key = f"text_emb_{text_hash}"
        
        if cache_key in self.vector_cache:
            return self.vector_cache[cache_key]
        
        embedding = await self.safe_api_call(
            self._get_text_embedding_impl, "æ–‡æœ¬å‘é‡", text, __event_emitter__
        )
        
        if embedding:
            self.vector_cache[cache_key] = embedding
            self.debug_log(3, f"æ–‡æœ¬å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ“")
        
        return embedding

    async def _get_multimodal_embedding_impl(self, content, __event_emitter__):
        """å®é™…çš„å¤šæ¨¡æ€å‘é‡è·å–å®ç°"""
        client = self.get_multimodal_vector_client()
        if not client:
            return None
        
        # å¤„ç†è¾“å…¥æ ¼å¼
        if isinstance(content, list):
            input_data = content
        else:
            input_data = [{"type": "text", "text": str(content)[:8000]}]
        
        response = await client.embeddings.create(
            model=self.valves.multimodal_vector_model, input=input_data
        )
        
        if response.data:
            return response.data[0].embedding
        return None

    async def get_multimodal_embedding(
        self, content, __event_emitter__
    ) -> Optional[List[float]]:
        """è·å–å¤šæ¨¡æ€å‘é‡"""
        if not content or not self.valves.enable_multimodal_vector:
            return None
        
        # ç”Ÿæˆç¼“å­˜key
        if isinstance(content, list):
            content_str = json.dumps(content, sort_keys=True)
        else:
            content_str = str(content)
        
        content_hash = hashlib.md5(content_str.encode()).hexdigest()
        cache_key = f"multimodal_emb_{content_hash}"
        
        if cache_key in self.vector_cache:
            return self.vector_cache[cache_key]
        
        embedding = await self.safe_api_call(
            self._get_multimodal_embedding_impl, "å¤šæ¨¡æ€å‘é‡", content, __event_emitter__
        )
        
        if embedding:
            self.vector_cache[cache_key] = embedding
            self.debug_log(3, f"å¤šæ¨¡æ€å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ–¼ï¸")
        
        return embedding

    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)

    async def vector_retrieve_relevant_messages(
        self, query_message: dict, candidate_messages: List[dict], __event_emitter__
    ) -> List[dict]:
        """åŸºäºå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³æ¶ˆæ¯"""
        if not candidate_messages or not self.valves.enable_vector_retrieval:
            return candidate_messages
        
        self.debug_log(
            1, f"å¼€å§‹å‘é‡æ£€ç´¢: æŸ¥è¯¢1æ¡ï¼Œå€™é€‰{len(candidate_messages)}æ¡", "ğŸ”"
        )
        
        await self.send_status(
            __event_emitter__,
            f"å‘é‡æ£€ç´¢ {len(candidate_messages)} æ¡æ¶ˆæ¯...",
            False,
            "ğŸ”",
        )
        
        # è·å–æŸ¥è¯¢å‘é‡
        query_content = query_message.get("content", "")
        query_vector = None
        strategy = self.valves.vector_strategy.lower()
        
        # æ ¹æ®ç­–ç•¥é€‰æ‹©å‘é‡åŒ–æ–¹æ³•
        if self.has_images_in_content(query_content):
            if strategy in ["auto", "multimodal_first"]:
                query_vector = await self.get_multimodal_embedding(
                    query_content, __event_emitter__
                )
            if not query_vector and strategy in ["auto", "fallback"]:
                # è½¬æ¢ä¸ºæ–‡æœ¬å†å‘é‡åŒ–
                text_content = self.extract_text_from_content(query_content)
                if text_content:
                    query_vector = await self.get_text_embedding(
                        text_content, __event_emitter__
                    )
        else:
            text_content = self.extract_text_from_content(query_content)
            if text_content:
                query_vector = await self.get_text_embedding(
                    text_content, __event_emitter__
                )
        
        if not query_vector:
            self.debug_log(1, "æŸ¥è¯¢å‘é‡è·å–å¤±è´¥ï¼Œè¿”å›åŸå§‹æ¶ˆæ¯", "âš ï¸")
            return candidate_messages
        
        # è®¡ç®—å€™é€‰æ¶ˆæ¯çš„ç›¸ä¼¼åº¦
        similarities = []
        for i, msg in enumerate(candidate_messages):
            msg_content = msg.get("content", "")
            msg_vector = None
            
            # ä¸ºå€™é€‰æ¶ˆæ¯è·å–å‘é‡
            if self.has_images_in_content(msg_content):
                msg_vector = await self.get_multimodal_embedding(
                    msg_content, __event_emitter__
                )
                if not msg_vector:
                    text_content = self.extract_text_from_content(msg_content)
                    if text_content:
                        msg_vector = await self.get_text_embedding(
                            text_content, __event_emitter__
                        )
            else:
                text_content = self.extract_text_from_content(msg_content)
                if text_content:
                    msg_vector = await self.get_text_embedding(
                        text_content, __event_emitter__
                    )
            
            if msg_vector:
                similarity = self.cosine_similarity(query_vector, msg_vector)
                similarities.append((i, similarity, msg))
                self.debug_log(3, f"æ¶ˆæ¯{i}ç›¸ä¼¼åº¦: {similarity:.3f}", "ğŸ“Š")
            else:
                # æ²¡æœ‰å‘é‡çš„æ¶ˆæ¯ç»™ä¸­ç­‰åˆ†æ•°ä½†ä»ä¿ç•™
                similarities.append((i, 0.4, msg))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # æ ¹æ®é˜ˆå€¼è¿‡æ»¤
        threshold = self.valves.vector_similarity_threshold
        filtered_similarities = [item for item in similarities if item[1] >= threshold]
        
        # å¦‚æœè¿‡æ»¤åå¤ªå°‘ï¼Œä¿ç•™æ›´å¤šæ¶ˆæ¯
        if len(filtered_similarities) < len(candidate_messages) * 0.5:
            filtered_similarities = similarities[
                : max(len(similarities) // 2, self.valves.vector_top_k)
            ]
        
        # é™åˆ¶æ•°é‡
        top_similarities = filtered_similarities[: self.valves.vector_top_k]
        
        # æå–æ¶ˆæ¯å¹¶ä¿æŒåŸå§‹é¡ºåº
        relevant_messages = []
        selected_indices = sorted([item[0] for item in top_similarities])
        
        for idx in selected_indices:
            relevant_messages.append(candidate_messages[idx])
        
        self.debug_log(
            1,
            f"å‘é‡æ£€ç´¢å®Œæˆ: {len(candidate_messages)} -> {len(relevant_messages)}æ¡",
            "âœ…",
        )
        
        await self.send_status(
            __event_emitter__,
            f"å‘é‡æ£€ç´¢å®Œæˆ: {len(relevant_messages)}æ¡ç›¸å…³æ¶ˆæ¯",
            True,
            "âœ…",
        )
        
        return relevant_messages

    def extract_text_from_content(self, content) -> str:
        """ä»å†…å®¹ä¸­æå–æ–‡æœ¬"""
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
            return " ".join(text_parts)
        else:
            return str(content)

    # ========== é‡æ’åºåŠŸèƒ½ ==========
    async def _rerank_messages_impl(self, query_text: str, documents: List[str], __event_emitter__):
        """å®é™…çš„é‡æ’åºå®ç°"""
        if not HTTPX_AVAILABLE:
            return None
        
        async with httpx.AsyncClient(timeout=self.valves.request_timeout) as client:
            headers = {
                "Authorization": f"Bearer {self.valves.rerank_api_key}",
                "Content-Type": "application/json",
            }
            
            data = {
                "model": self.valves.rerank_model,
                "query": query_text,
                "documents": documents,
                "top_n": min(self.valves.rerank_top_k, len(documents)),
                "return_documents": True,
            }
            
            response = await client.post(
                f"{self.valves.rerank_api_base}/v1/rerank",
                headers=headers,
                json=data,
            )
            
            if response.status_code != 200:
                raise Exception(f"HTTP {response.status_code}: {response.text}")
            
            # æ£€æŸ¥å“åº”æ ¼å¼
            response_text = response.text
            if not self.is_json_response(response_text):
                error_info = self.extract_error_info(response_text)
                raise Exception(f"éJSONå“åº”: {error_info}")
            
            result = response.json()
            
            if result.get("code") != 200:
                error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                raise Exception(f"APIé”™è¯¯: {error_msg}")
            
            return result.get("data", {}).get("results", [])

    async def rerank_messages(
        self, query_message: dict, candidate_messages: List[dict], __event_emitter__
    ) -> List[dict]:
        """é‡æ’åºæ¶ˆæ¯"""
        if not candidate_messages or not self.valves.enable_reranking:
            return candidate_messages
        
        self.debug_log(1, f"å¼€å§‹é‡æ’åº: æŸ¥è¯¢1æ¡ï¼Œå€™é€‰{len(candidate_messages)}æ¡", "ğŸ”„")
        
        await self.send_status(
            __event_emitter__,
            f"é‡æ’åº {len(candidate_messages)} æ¡æ¶ˆæ¯...",
            False,
            "ğŸ”„",
        )
        
        # å‡†å¤‡æŸ¥è¯¢æ–‡æœ¬
        query_text = self.extract_text_from_content(query_message.get("content", ""))
        if not query_text:
            return candidate_messages
        
        # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
        documents = []
        for msg in candidate_messages:
            text = self.extract_text_from_content(msg.get("content", ""))
            if text:
                # æé«˜æ–‡æ¡£é•¿åº¦é™åˆ¶
                if len(text) > 3000:
                    text = text[:3000] + "..."
                documents.append(text)
            else:
                documents.append("ç©ºæ¶ˆæ¯")
        
        if not documents:
            return candidate_messages
        
        # è°ƒç”¨é‡æ’åºAPI
        rerank_results = await self.safe_api_call(
            self._rerank_messages_impl, "é‡æ’åº", query_text, documents, __event_emitter__
        )
        
        if rerank_results:
            # æŒ‰é‡æ’åºç»“æœé‡æ–°æ’åˆ—æ¶ˆæ¯
            reranked_messages = []
            for item in rerank_results:
                original_index = item.get("index", 0)
                if 0 <= original_index < len(candidate_messages):
                    reranked_messages.append(candidate_messages[original_index])
                    score = item.get("relevance_score", 0)
                    self.debug_log(
                        3,
                        f"é‡æ’åºç»“æœ: index={original_index}, score={score:.3f}",
                        "ğŸ“Š",
                    )
            
            self.debug_log(
                1,
                f"é‡æ’åºå®Œæˆ: {len(candidate_messages)} -> {len(reranked_messages)}æ¡",
                "âœ…",
            )
            
            await self.send_status(
                __event_emitter__,
                f"é‡æ’åºå®Œæˆ: {len(reranked_messages)}æ¡æ¶ˆæ¯",
                True,
                "âœ…",
            )
            
            return reranked_messages
        
        return candidate_messages

    # ========== Visionå¤„ç† ==========
    def get_vision_client(self):
        if not OPENAI_AVAILABLE:
            return None
        
        if self._vision_client:
            return self._vision_client
        
        api_key = self.valves.vision_api_key
        if not api_key:
            api_key = (
                self.valves.multimodal_vector_api_key or self.valves.text_vector_api_key
            )
        
        if api_key:
            self._vision_client = AsyncOpenAI(
                base_url=self.valves.vision_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout,
            )
            self.debug_log(2, "Visionå®¢æˆ·ç«¯å·²åˆ›å»º", "ğŸ‘ï¸")
        
        return self._vision_client

    async def _describe_image_impl(self, image_url: str, __event_emitter__):
        """å®é™…çš„å›¾ç‰‡æè¿°å®ç°"""
        client = self.get_vision_client()
        if not client:
            return None
        
        response = await client.chat.completions.create(
            model=self.valves.vision_model,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": self.valves.vision_prompt_template,
                        },
                        {"type": "image_url", "image_url": {"url": image_url}},
                    ],
                }
            ],
            max_tokens=self.valves.vision_max_tokens,
            temperature=0.2,
        )
        
        if response.choices:
            return response.choices[0].message.content.strip()
        return None

    async def describe_image(self, image_url: str, __event_emitter__) -> str:
        """æè¿°å•å¼ å›¾ç‰‡"""
        image_hash = hashlib.md5(image_url.encode()).hexdigest()
        
        if image_hash in self.vision_cache:
            self.debug_log(3, f"ä½¿ç”¨ç¼“å­˜çš„å›¾ç‰‡æè¿°: {image_hash[:8]}", "ğŸ“‹")
            return self.vision_cache[image_hash]
        
        self.debug_log(2, f"å¼€å§‹è¯†åˆ«å›¾ç‰‡: {image_hash[:8]}", "ğŸ‘ï¸")
        
        description = await self.safe_api_call(
            self._describe_image_impl, "å›¾ç‰‡è¯†åˆ«", image_url, __event_emitter__
        )
        
        if description:
            # æé«˜æè¿°é•¿åº¦é™åˆ¶
            if len(description) > 1200:
                description = description[:1200] + "..."
            
            self.vision_cache[image_hash] = description
            self.debug_log(2, f"å›¾ç‰‡è¯†åˆ«å®Œæˆ: {len(description)}å­—ç¬¦", "âœ…")
            return description
        
        return "å›¾ç‰‡å¤„ç†å¤±è´¥ï¼šæ— æ³•è·å–æè¿°"

    async def process_message_images(self, message: dict, __event_emitter__) -> dict:
        """å¤„ç†å•æ¡æ¶ˆæ¯ä¸­çš„å›¾ç‰‡"""
        content = message.get("content", "")
        if not isinstance(content, list):
            return message
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        has_images = any(item.get("type") == "image_url" for item in content)
        if not has_images:
            return message
        
        # å¤„ç†å›¾ç‰‡
        processed_content = []
        image_count = 0
        
        for item in content:
            if item.get("type") == "text":
                processed_content.append(item.get("text", ""))
            elif item.get("type") == "image_url":
                image_count += 1
                image_url = item.get("image_url", {}).get("url", "")
                if image_url:
                    description = await self.describe_image(
                        image_url, __event_emitter__
                    )
                    processed_content.append(f"[å›¾ç‰‡{image_count}æè¿°] {description}")
        
        # åˆ›å»ºæ–°æ¶ˆæ¯
        processed_message = message.copy()
        processed_message["content"] = (
            "\n".join(processed_content) if processed_content else ""
        )
        
        self.debug_log(2, f"æ¶ˆæ¯å›¾ç‰‡å¤„ç†å®Œæˆ: {image_count}å¼ å›¾ç‰‡", "ğŸ–¼ï¸")
        return processed_message

    async def process_multimodal_content(
        self, messages: List[dict], model_name: str, __event_emitter__
    ) -> List[dict]:
        """å¤„ç†å¤šæ¨¡æ€å†…å®¹"""
        if not self.valves.enable_multimodal:
            return messages
        
        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return messages
        
        should_process = self.should_process_images_for_model(model_name)
        is_multimodal = self.is_multimodal_model(model_name)
        
        self.debug_log(
            1,
            f"å¤šæ¨¡æ€å¤„ç†æ£€æŸ¥: æ¨¡å‹={model_name}, å¤šæ¨¡æ€={is_multimodal}, éœ€è¦å¤„ç†={should_process}",
            "ğŸ–¼ï¸",
        )
        
        if (
            is_multimodal
            and self.valves.preserve_images_in_multimodal
            and not should_process
        ):
            self.debug_log(2, f"å¤šæ¨¡æ€æ¨¡å‹ {model_name} ä¿ç•™åŸå§‹å›¾ç‰‡", "ğŸ“¸")
            return messages
        
        # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
        total_images = 0
        for msg in messages:
            if isinstance(msg.get("content"), list):
                total_images += len(
                    [
                        item
                        for item in msg.get("content", [])
                        if item.get("type") == "image_url"
                    ]
                )
        
        if total_images == 0:
            return messages
        
        self.debug_log(1, f"å¼€å§‹å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼š{total_images} å¼ å›¾ç‰‡", "ğŸ–¼ï¸")
        
        await self.send_status(
            __event_emitter__,
            f"å¤„ç† {total_images} å¼ å›¾ç‰‡...",
            False,
            "ğŸ–¼ï¸",
        )
        
        # å¤„ç†æ‰€æœ‰æ¶ˆæ¯
        processed_messages = []
        processed_count = 0
        
        for message in messages:
            if self.has_images_in_content(message.get("content")):
                processed_message = await self.process_message_images(
                    message, __event_emitter__
                )
                processed_messages.append(processed_message)
                if isinstance(message.get("content"), list):
                    processed_count += len(
                        [
                            item
                            for item in message.get("content", [])
                            if item.get("type") == "image_url"
                        ]
                    )
            else:
                processed_messages.append(message)
        
        self.debug_log(1, f"å¤šæ¨¡æ€å¤„ç†å®Œæˆï¼š{processed_count} å¼ å›¾ç‰‡", "âœ…")
        await self.send_status(__event_emitter__, "å›¾ç‰‡å¤„ç†å®Œæˆ", True, "âœ…")
        
        return processed_messages

    # ========== å†…å®¹æœ€å¤§åŒ–ä¿ç•™ç­–ç•¥ ==========
    def get_summary_client(self):
        """è·å–æ‘˜è¦å®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE:
            return None
        
        api_key = self.valves.summary_api_key
        if not api_key:
            api_key = (
                self.valves.multimodal_vector_api_key
                or self.valves.text_vector_api_key
                or self.valves.vision_api_key
            )
        
        if api_key:
            return AsyncOpenAI(
                base_url=self.valves.summary_api_base,
                api_key=api_key,
                timeout=self.valves.request_timeout,
            )
        
        return None

    def smart_message_selection_v2(
        self, messages: List[dict], target_tokens: int, iteration: int = 0
    ) -> Tuple[List[dict], List[dict]]:
        """
        å†…å®¹æœ€å¤§åŒ–ä¿ç•™çš„æ™ºèƒ½æ¶ˆæ¯é€‰æ‹©ç­–ç•¥
        æ ¸å¿ƒæ€æƒ³ï¼šå¼ºåˆ¶ä¿ç•™ç”¨æˆ·æœ€åæ¶ˆæ¯ï¼Œæœ€å¤§åŒ–ä¿ç•™å…¶ä»–å†…å®¹
        """
        if not messages:
            return [], []
        
        # åˆ†ç¦»ä¸åŒç±»å‹çš„æ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        assistant_messages = [msg for msg in messages if msg.get("role") == "assistant"]
        
        protected = []
        current_tokens = 0
        
        # 1. å¼ºåˆ¶ä¿ç•™ç”¨æˆ·æœ€åæ¶ˆæ¯
        last_user_message = None
        if self.valves.force_preserve_last_user_message and user_messages:
            last_user_message = user_messages[-1]
            protected.append(last_user_message)
            current_tokens += self.count_message_tokens(last_user_message)
            self.debug_log(1, f"ğŸ”’ å¼ºåˆ¶ä¿ç•™ç”¨æˆ·æœ€åæ¶ˆæ¯: {current_tokens}tokens", "ğŸ’¾")
        
        # 2. ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
        for msg in system_messages:
            msg_tokens = self.count_message_tokens(msg)
            if current_tokens + msg_tokens <= target_tokens:
                protected.append(msg)
                current_tokens += msg_tokens
        
        # 3. åŠ¨æ€è°ƒæ•´ä¿æŠ¤ç­–ç•¥ï¼ˆæ›´ä¿å®ˆçš„è°ƒæ•´ï¼‰
        preserve_exchanges = max(2, self.valves.preserve_recent_exchanges - iteration)
        max_preserve_tokens = int(
            target_tokens * max(0.5, self.valves.max_preserve_ratio - iteration * 0.05)
        )
        
        self.debug_log(
            1,
            f"ğŸ”„ ç¬¬{iteration+1}æ¬¡è¿­ä»£: ä¿æŠ¤{preserve_exchanges}è½®å¯¹è¯, æœ€å¤§{max_preserve_tokens}tokens",
            "ğŸ“Š",
        )
        
        # 4. ä¿æŠ¤æœ€è¿‘çš„å¯¹è¯è½®æ¬¡
        remaining_messages = [msg for msg in messages if msg not in protected]
        
        # æŒ‰æ—¶é—´é¡ºåºæ‰¾åˆ°æœ€è¿‘çš„å¯¹è¯è½®æ¬¡
        exchanges_protected = 0
        i = len(remaining_messages) - 1
        
        while (
            i >= 0
            and exchanges_protected < preserve_exchanges
            and current_tokens < max_preserve_tokens
        ):
            msg = remaining_messages[i]
            msg_tokens = self.count_message_tokens(msg)
            
            if current_tokens + msg_tokens <= max_preserve_tokens:
                if msg.get("role") == "assistant" and i > 0:
                    # å°è¯•ä¿æŠ¤å®Œæ•´çš„å¯¹è¯è½®æ¬¡
                    prev_msg = remaining_messages[i - 1]
                    if prev_msg.get("role") == "user":
                        prev_tokens = self.count_message_tokens(prev_msg)
                        if (
                            current_tokens + msg_tokens + prev_tokens
                            <= max_preserve_tokens
                        ):
                            protected.insert(-1, prev_msg)  # æ’å…¥åˆ°æœ€åç”¨æˆ·æ¶ˆæ¯å‰
                            protected.insert(-1, msg)
                            current_tokens += msg_tokens + prev_tokens
                            exchanges_protected += 1
                            i -= 2
                            continue
                
                # å•ç‹¬ä¿æŠ¤è¿™æ¡æ¶ˆæ¯
                protected.insert(-1, msg)
                current_tokens += msg_tokens
            
            i -= 1
        
        # 5. ç¡®å®šéœ€è¦å¤„ç†çš„æ¶ˆæ¯
        to_process = [msg for msg in messages if msg not in protected]
        
        self.debug_log(
            1,
            f"ğŸ“‹ ç¬¬{iteration+1}æ¬¡é€‰æ‹©: ä¿æŠ¤{len(protected)}æ¡({current_tokens}tokens), å¤„ç†{len(to_process)}æ¡",
            "ğŸ“",
        )
        
        return protected, to_process

    async def _summarize_messages_impl(self, conversation_text: str, iteration: int):
        """å®é™…çš„æ‘˜è¦å®ç°"""
        client = self.get_summary_client()
        if not client:
            return None
        
        # å¢å¼ºçš„æ‘˜è¦æç¤º
        system_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¯¹è¯æ‘˜è¦åŠ©æ‰‹ã€‚è¯·ä¸ºä»¥ä¸‹å¯¹è¯åˆ›å»ºè¯¦ç»†çš„ç»“æ„åŒ–æ‘˜è¦ï¼Œ**å¿…é¡»æœ€å¤§åŒ–ä¿ç•™ä¿¡æ¯**ã€‚

æ‘˜è¦è¦æ±‚ï¼š
1. ä¿æŒå¯¹è¯çš„å®Œæ•´é€»è¾‘è„‰ç»œå’Œæ—¶é—´é¡ºåº
2. ä¿ç•™æ‰€æœ‰å…³é”®ä¿¡æ¯ã€æŠ€æœ¯ç»†èŠ‚ã€å‚æ•°é…ç½®ã€æ•°æ®
3. ä¿ç•™é‡è¦çš„é—®ç­”å†…å®¹å’Œè®¨è®ºè¦ç‚¹
4. å¦‚æœ‰å›¾ç‰‡æè¿°ï¼Œå®Œæ•´ä¿ç•™è§†è§‰ä¿¡æ¯
5. ä½¿ç”¨æ¸…æ™°çš„ç»“æ„ï¼šé—®é¢˜ â†’ å›ç­” â†’ åç»­è®¨è®º
6. ä¼˜å…ˆçº§ï¼šå†…å®¹å®Œæ•´æ€§ > é•¿åº¦é™åˆ¶
7. å¦‚æœå†…å®¹å¾ˆé‡è¦ï¼Œ**å¿…é¡»**ä¿ç•™ï¼Œå¯ä»¥é€‚å½“è¶…å‡ºé•¿åº¦é™åˆ¶
8. ä¿ç•™å…·ä½“çš„é…ç½®ã€ä»£ç ã€æ•°æ®ã€å‚æ•°ç­‰æŠ€æœ¯ç»†èŠ‚

å¤„ç†ä¿¡æ¯ï¼š
- ç¬¬{iteration+1}æ¬¡æ‘˜è¦å¤„ç†
- ç›®æ ‡ï¼šæœ€å¤§åŒ–ä¿¡æ¯ä¿ç•™ï¼Œé¿å…é‡è¦ä¿¡æ¯ä¸¢å¤±

å¯¹è¯å†…å®¹ï¼š"""
        
        response = await client.chat.completions.create(
            model=self.valves.summary_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": conversation_text},
            ],
            max_tokens=self.valves.max_summary_length,
            temperature=0.05,
            timeout=self.valves.request_timeout,
        )
        
        if response.choices and response.choices[0].message.content:
            return response.choices[0].message.content.strip()
        return None

    async def summarize_messages_v2(
        self, messages: List[dict], __event_emitter__, iteration: int = 0
    ) -> str:
        """å¢å¼ºçš„æ‘˜è¦åŠŸèƒ½ - æœ€å¤§åŒ–ä¿¡æ¯ä¿ç•™"""
        if not messages:
            return ""
        
        # å…ˆå¤„ç†å›¾ç‰‡
        processed_messages = messages
        if self.valves.always_process_images_before_summary:
            has_images = any(
                self.has_images_in_content(msg.get("content")) for msg in messages
            )
            if has_images:
                self.debug_log(2, f"æ‘˜è¦å‰å¤„ç†å›¾ç‰‡: {len(messages)}æ¡æ¶ˆæ¯", "ğŸ–¼ï¸")
                processed_messages = []
                for msg in messages:
                    if self.has_images_in_content(msg.get("content")):
                        processed_msg = await self.process_message_images(
                            msg, __event_emitter__
                        )
                        processed_messages.append(processed_msg)
                    else:
                        processed_messages.append(msg)
        
        # æŒ‰è§’è‰²åˆ†ç»„å¤„ç†
        conversation_parts = []
        current_exchange = []
        
        for msg in processed_messages:
            role = msg.get("role", "unknown")
            content = self.extract_text_from_content(msg.get("content", ""))
            
            if len(content) > 6000:
                content = content[:6000] + "...(é•¿å†…å®¹å·²æˆªæ–­)"
            
            if role == "user":
                if current_exchange:
                    conversation_parts.append(self.format_exchange(current_exchange))
                    current_exchange = []
                current_exchange.append(f"ğŸ‘¤ ç”¨æˆ·: {content}")
            elif role == "assistant":
                current_exchange.append(f"ğŸ¤– åŠ©æ‰‹: {content}")
            else:
                current_exchange.append(f"[{role}]: {content}")
        
        if current_exchange:
            conversation_parts.append(self.format_exchange(current_exchange))
        
        conversation_text = "\n\n".join(conversation_parts)
        
        # è°ƒç”¨æ‘˜è¦API
        summary = await self.safe_api_call(
            self._summarize_messages_impl, "æ‘˜è¦ç”Ÿæˆ", conversation_text, iteration
        )
        
        if summary and len(summary) >= 200:
            self.debug_log(1, f"ğŸ“ æ‘˜è¦ç”ŸæˆæˆåŠŸ: {len(summary)}å­—ç¬¦", "ğŸ“")
            return summary
        elif summary:
            self.debug_log(1, f"âš ï¸ æ‘˜è¦è¿‡çŸ­({len(summary)}å­—ç¬¦)ï¼Œä½¿ç”¨åŸå§‹å†…å®¹", "ğŸ“")
        
        # æ‘˜è¦å¤±è´¥æˆ–è¿‡çŸ­æ—¶ï¼Œè¿”å›åŸå§‹å†…å®¹çš„æˆªæ–­ç‰ˆæœ¬
        if len(conversation_text) > 2000:
            return conversation_text[:2000] + "...(åŸå§‹å†…å®¹æˆªæ–­)"
        return conversation_text

    def format_exchange(self, exchange: List[str]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯è½®æ¬¡"""
        return "\n".join(exchange)

    async def content_maximization_processing(
        self, messages: List[dict], target_tokens: int, __event_emitter__
    ) -> List[dict]:
        """å†…å®¹æœ€å¤§åŒ–å¤„ç†çš„æ ¸å¿ƒé€»è¾‘"""
        current_tokens = self.count_messages_tokens(messages)
        if current_tokens <= target_tokens:
            return messages
        
        self.debug_log(
            1,
            f"ğŸš€ å¼€å§‹å†…å®¹æœ€å¤§åŒ–å¤„ç†: {current_tokens} -> {target_tokens} tokens",
            "ğŸ“ˆ",
        )
        
        iteration = 0
        processed_messages = messages
        
        while iteration < self.valves.max_processing_iterations:
            current_tokens = self.count_messages_tokens(processed_messages)
            
            if current_tokens <= target_tokens:
                self.debug_log(
                    1,
                    f"âœ… å†…å®¹æœ€å¤§åŒ–å®Œæˆ: è¿­ä»£{iteration+1}æ¬¡, æœ€ç»ˆ{current_tokens}tokens",
                    "ğŸ“ˆ",
                )
                break
            
            await self.send_status(
                __event_emitter__,
                f"å†…å®¹æœ€å¤§åŒ–å¤„ç† ç¬¬{iteration+1}è½® ({current_tokens}â†’{target_tokens})",
                False,
                "ğŸ“ˆ",
            )
            
            # æ™ºèƒ½é€‰æ‹©æ¶ˆæ¯
            protected_messages, to_process = self.smart_message_selection_v2(
                processed_messages, target_tokens, iteration
            )
            
            if not to_process:
                self.debug_log(1, f"âš ï¸ æ²¡æœ‰å¯å¤„ç†çš„æ¶ˆæ¯ï¼Œåœæ­¢å¤„ç†", "ğŸ“")
                break
            
            # å‘é‡æ£€ç´¢ç›¸å…³æ¶ˆæ¯
            if self.valves.enable_vector_retrieval and len(to_process) > 3:
                # ä½¿ç”¨ç”¨æˆ·æœ€åæ¶ˆæ¯ä½œä¸ºæŸ¥è¯¢
                query_msg = None
                for msg in reversed(processed_messages):
                    if msg.get("role") == "user":
                        query_msg = msg
                        break
                
                if query_msg:
                    self.debug_log(2, f"ğŸ” å¯¹{len(to_process)}æ¡æ¶ˆæ¯è¿›è¡Œå‘é‡æ£€ç´¢", "ğŸ”")
                    relevant_messages = await self.vector_retrieve_relevant_messages(
                        query_msg, to_process, __event_emitter__
                    )
                    
                    # é‡æ’åº
                    if self.valves.enable_reranking and len(relevant_messages) > 2:
                        self.debug_log(
                            2, f"ğŸ”„ å¯¹{len(relevant_messages)}æ¡æ¶ˆæ¯è¿›è¡Œé‡æ’åº", "ğŸ”„"
                        )
                        relevant_messages = await self.rerank_messages(
                            query_msg, relevant_messages, __event_emitter__
                        )
                    
                    to_process = relevant_messages
            
            # å¤„ç†æ¶ˆæ¯
            new_messages = protected_messages.copy()
            
            if to_process:
                # æŒ‰é‡è¦æ€§åˆ†ç»„å¤„ç†
                important_messages = []
                normal_messages = []
                
                for msg in to_process:
                    msg_tokens = self.count_message_tokens(msg)
                    if msg_tokens > self.valves.max_single_message_tokens:
                        important_messages.append(msg)
                    else:
                        normal_messages.append(msg)
                
                # å¤„ç†è¶…å¤§æ¶ˆæ¯
                for msg in important_messages:
                    summarized = await self.summarize_single_message_v2(
                        msg, __event_emitter__, iteration
                    )
                    if summarized:
                        new_messages.append(summarized)
                
                # æ‰¹é‡å¤„ç†æ™®é€šæ¶ˆæ¯
                if normal_messages:
                    summary_text = await self.summarize_messages_v2(
                        normal_messages, __event_emitter__, iteration
                    )
                    if summary_text and len(summary_text) > 50:
                        summary_message = {
                            "role": "system",
                            "content": f"=== ğŸ“‹ æ™ºèƒ½æ‘˜è¦ (ç¬¬{iteration+1}è½®å¤„ç†) ===\n{summary_text}\n{'='*60}",
                        }
                        new_messages.append(summary_message)
                    else:
                        # æ‘˜è¦å¤±è´¥æˆ–è¿‡çŸ­ï¼Œä¿ç•™æ›´å¤šåŸå§‹æ¶ˆæ¯
                        self.debug_log(1, f"âŒ æ‘˜è¦å¤±è´¥æˆ–è¿‡çŸ­ï¼Œä¿ç•™åŸå§‹æ¶ˆæ¯", "ğŸ“")
                        # ä¿ç•™æœ€é‡è¦çš„æ¶ˆæ¯
                        keep_count = max(len(normal_messages) // 2, 3)
                        new_messages.extend(normal_messages[-keep_count:])
            
            processed_messages = new_messages
            iteration += 1
            
            # æ£€æŸ¥è¿›åº¦
            new_tokens = self.count_messages_tokens(processed_messages)
            reduction = current_tokens - new_tokens
            
            self.debug_log(
                1,
                f"ğŸ“Š ç¬¬{iteration}è½®å¤„ç†: {current_tokens} -> {new_tokens} tokens (å‡å°‘{reduction})",
                "ğŸ“Š",
            )
            
            # æ›´ä¸¥æ ¼çš„åœæ­¢æ¡ä»¶
            if reduction < self.valves.min_reduction_threshold:
                self.debug_log(1, f"âš ï¸ å‡å°‘å¹…åº¦è¿‡å°({reduction}tokens)ï¼Œåœæ­¢å¤„ç†", "ğŸ“")
                break
        
        final_tokens = self.count_messages_tokens(processed_messages)
        
        # æ›´ä¿å®ˆçš„ç´§æ€¥æˆªæ–­
        if final_tokens > target_tokens * 1.1:  # å…è®¸10%çš„è¶…å‡º
            self.debug_log(1, f"âš ï¸ ä»è¶…å‡ºé™åˆ¶ï¼Œå¯ç”¨ç´§æ€¥ç­–ç•¥", "ğŸ†˜")
            processed_messages = self.emergency_truncate_v2(
                processed_messages, target_tokens
            )
        
        await self.send_status(
            __event_emitter__,
            f"å†…å®¹æœ€å¤§åŒ–å®Œæˆ: {final_tokens}/{target_tokens} tokens",
            True,
            "âœ…",
        )
        
        return processed_messages

    async def _summarize_single_message_impl(self, content: str, iteration: int):
        """å®é™…çš„å•æ¡æ¶ˆæ¯æ‘˜è¦å®ç°"""
        client = self.get_summary_client()
        if not client:
            return None
        
        system_prompt = f"""è¯·å°†ä»¥ä¸‹å†…å®¹è¿›è¡Œè¯¦ç»†æ‘˜è¦ï¼Œ**å¿…é¡»æœ€å¤§åŒ–ä¿ç•™å…³é”®ä¿¡æ¯**ï¼š

è¦æ±‚ï¼š
- ä¿ç•™æ‰€æœ‰é‡è¦ç»†èŠ‚ã€å‚æ•°ã€é…ç½®ã€æ•°æ®
- ä¿ç•™å›¾ç‰‡æè¿°ä¿¡æ¯
- ä¿æŒé€»è¾‘ç»“æ„å®Œæ•´
- ä¼˜å…ˆçº§ï¼šå†…å®¹å®Œæ•´æ€§ > é•¿åº¦é™åˆ¶
- å¦‚æœå†…å®¹å¾ˆé‡è¦ï¼Œå¯ä»¥é€‚å½“è¶…å‡ºé•¿åº¦é™åˆ¶
- è¿™æ˜¯ç¬¬{iteration+1}æ¬¡å¤„ç†ï¼Œä½†ä»éœ€ä¿ç•™æ ¸å¿ƒæŠ€æœ¯ä¿¡æ¯

ç›®æ ‡é•¿åº¦ï¼š{self.valves.max_summary_length}å­—ç¬¦ï¼ˆå¯é€‚å½“è¶…å‡ºï¼‰"""
        
        response = await client.chat.completions.create(
            model=self.valves.summary_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": content[:8000]},
            ],
            max_tokens=self.valves.max_summary_length,
            temperature=0.05,
            timeout=self.valves.request_timeout,
        )
        
        if response.choices and response.choices[0].message.content:
            return response.choices[0].message.content.strip()
        return None

    async def summarize_single_message_v2(
        self, message: dict, __event_emitter__, iteration: int = 0
    ) -> Optional[dict]:
        """å¢å¼ºçš„å•æ¡æ¶ˆæ¯æ‘˜è¦"""
        # å…ˆå¤„ç†å›¾ç‰‡
        processed_message = message
        if self.has_images_in_content(message.get("content")):
            processed_message = await self.process_message_images(
                message, __event_emitter__
            )
        
        content = self.extract_text_from_content(processed_message.get("content", ""))
        if not content:
            return None
        
        # å°è¯•APIæ‘˜è¦
        summary = await self.safe_api_call(
            self._summarize_single_message_impl, "å•æ¡æ¶ˆæ¯æ‘˜è¦", content, iteration
        )
        
        if summary and len(summary) > 100:
            result = processed_message.copy()
            result["content"] = f"[æ™ºèƒ½æ‘˜è¦] {summary}"
            return result
        
        # å¤±è´¥æ—¶æ›´ä¿å®ˆçš„æˆªæ–­
        if len(content) > 2000:
            result = processed_message.copy()
            result["content"] = content[:2000] + "...(å†…å®¹å·²æˆªæ–­)"
            return result
        
        return processed_message

    def emergency_truncate_v2(
        self, messages: List[dict], target_tokens: int
    ) -> List[dict]:
        """å¢å¼ºçš„ç´§æ€¥æˆªæ–­ç­–ç•¥ - æ›´ä¿å®ˆçš„å¤„ç†"""
        self.debug_log(1, f"ğŸ†˜ å¯ç”¨å¢å¼ºç´§æ€¥æˆªæ–­ç­–ç•¥", "ğŸ“")
        
        # åˆ†ç±»æ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        assistant_messages = [msg for msg in messages if msg.get("role") == "assistant"]
        
        result = []
        current_tokens = 0
        
        # 1. ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
        for msg in system_messages:
            msg_tokens = self.count_message_tokens(msg)
            if current_tokens + msg_tokens <= target_tokens:
                result.append(msg)
                current_tokens += msg_tokens
        
        # 2. å¼ºåˆ¶ä¿ç•™ç”¨æˆ·æœ€åæ¶ˆæ¯
        if user_messages:
            last_user_msg = user_messages[-1]
            msg_tokens = self.count_message_tokens(last_user_msg)
            
            if current_tokens + msg_tokens <= target_tokens:
                result.append(last_user_msg)
                current_tokens += msg_tokens
            else:
                # æ›´ä¿å®ˆçš„æˆªæ–­ç”¨æˆ·æ¶ˆæ¯å†…å®¹
                content = self.extract_text_from_content(
                    last_user_msg.get("content", "")
                )
                if content:
                    max_content_length = min(1000, len(content) // 2)
                    truncated_content = content[:max_content_length] + "...(ç´§æ€¥æˆªæ–­)"
                    truncated_msg = last_user_msg.copy()
                    truncated_msg["content"] = truncated_content
                    result.append(truncated_msg)
                    current_tokens += self.count_message_tokens(truncated_msg)
        
        # 3. å°½å¯èƒ½ä¿ç•™æœ€è¿‘çš„assistantæ¶ˆæ¯
        for msg in reversed(assistant_messages):
            msg_tokens = self.count_message_tokens(msg)
            if current_tokens + msg_tokens <= target_tokens:
                result.insert(-1, msg)  # æ’å…¥åˆ°æœ€åç”¨æˆ·æ¶ˆæ¯å‰
                current_tokens += msg_tokens
            else:
                break
        
        # 4. è¡¥å……å…¶ä»–ç”¨æˆ·æ¶ˆæ¯
        remaining_tokens = target_tokens - current_tokens
        if remaining_tokens > 200:  # æé«˜æœ€å°å‰©ä½™tokenè¦æ±‚
            for msg in reversed(user_messages[:-1]):  # é™¤äº†æœ€åä¸€æ¡
                msg_tokens = self.count_message_tokens(msg)
                if msg_tokens <= remaining_tokens:
                    result.insert(-1, msg)
                    remaining_tokens -= msg_tokens
                else:
                    break
        
        final_tokens = self.count_messages_tokens(result)
        self.debug_log(
            1, f"ğŸ†˜ å¢å¼ºç´§æ€¥æˆªæ–­å®Œæˆ: {len(result)}æ¡æ¶ˆæ¯, {final_tokens}tokens", "ğŸ“"
        )
        
        return result

    async def inlet(
        self,
        body: dict,
        user: Optional[dict] = None,
        __event_emitter__: Optional[Callable] = None,
    ) -> dict:
        """å…¥å£å‡½æ•° - å¤„ç†è¯·æ±‚"""
        print("\nğŸš€ ===== INLET CALLED =====")
        print(f"ğŸ“¨ æ”¶åˆ°è¯·æ±‚: {list(body.keys())}")
        
        if not self.valves.enable_processing:
            print("âŒ å¤„ç†åŠŸèƒ½å·²ç¦ç”¨")
            return body
        
        messages = body.get("messages", [])
        if not messages:
            print("âŒ æ— æ¶ˆæ¯å†…å®¹")
            return body
        
        model_name = body.get("model", "æœªçŸ¥")
        print(f"ğŸ“‹ æ¨¡å‹: {model_name}, æ¶ˆæ¯æ•°: {len(messages)}")
        
        if self.is_model_excluded(model_name):
            print(f"ğŸš« æ¨¡å‹å·²æ’é™¤")
            return body
        
        # Tokenåˆ†æ
        original_tokens = self.count_messages_tokens(messages)
        token_limit = self.get_model_token_limit(model_name)
        
        print(f"ğŸ“Š Token: {original_tokens}/{token_limit}")
        print(f"ğŸ”§ é”™è¯¯é‡è¯•: {self.valves.api_error_retry_times}æ¬¡")
        
        try:
            # 1. å¤šæ¨¡æ€å¤„ç†
            processed_messages = await self.process_multimodal_content(
                messages, model_name, __event_emitter__
            )
            processed_tokens = self.count_messages_tokens(processed_messages)
            print(f"ğŸ“Š å¤šæ¨¡æ€å¤„ç†å: {processed_tokens} tokens")
            
            # 2. å†…å®¹æœ€å¤§åŒ–å¤„ç†
            if (
                self.valves.enable_content_maximization
                and processed_tokens > token_limit
            ):
                print(f"ğŸš€ Tokenè¶…é™ï¼Œå¼€å§‹å†…å®¹æœ€å¤§åŒ–å¤„ç†...")
                final_messages = await self.content_maximization_processing(
                    processed_messages, token_limit, __event_emitter__
                )
                final_tokens = self.count_messages_tokens(final_messages)
                print(f"ğŸ“Š å†…å®¹æœ€å¤§åŒ–å¤„ç†å: {final_tokens} tokens")
                
                # è®¡ç®—ä¿ç•™æ¯”ä¾‹
                retention_ratio = final_tokens / original_tokens if original_tokens > 0 else 0
                print(f"ğŸ“ˆ å†…å®¹ä¿ç•™æ¯”ä¾‹: {retention_ratio:.2%}")
                
                if retention_ratio < 0.3:
                    print(f"âš ï¸ å†…å®¹ä¿ç•™æ¯”ä¾‹è¿‡ä½({retention_ratio:.2%})ï¼Œå»ºè®®è°ƒæ•´å‚æ•°")
                
                body["messages"] = final_messages
                print("âœ… ä½¿ç”¨å†…å®¹æœ€å¤§åŒ–å¤„ç†åçš„æ¶ˆæ¯")
            else:
                body["messages"] = processed_messages
                print("âœ… ç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯")
                
        except Exception as e:
            print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            
            await self.send_status(
                __event_emitter__,
                f"å¤„ç†å¤±è´¥: {str(e)[:50]}",
                True,
                "âŒ",
            )
        
        print("ğŸ ===== INLET DONE =====\n")
        return body

    async def outlet(
        self,
        body: dict,
        user: Optional[dict] = None,
        __event_emitter__: Optional[Callable] = None,
    ) -> dict:
        """å‡ºå£å‡½æ•° - è¿”å›å“åº”"""
        return body

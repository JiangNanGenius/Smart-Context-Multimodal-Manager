"""
title: ğŸš€ Advanced Context Manager - Content Maximization Only v2.3.1
author: JiangNanGenius
version: 2.3.1
license: MIT
required_open_webui_version: 0.5.17
description: ä¿®å¤RAGæœç´¢æ— ç»“æœæ—¶çš„ç”¨æˆ·æ¶ˆæ¯è¯†åˆ«é—®é¢˜ï¼Œå¢å¼ºå®¹é”™æœºåˆ¶
"""
import json
import hashlib
import asyncio
import re
import base64
import math
import time
import copy
import html
from typing import Optional, List, Dict, Callable, Any, Tuple, Union
from pydantic import BaseModel, Field
from enum import Enum

# å¯¼å…¥ä¾èµ–åº“
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    tiktoken = None

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None


class MessageOrder:
    """æ¶ˆæ¯é¡ºåºç®¡ç†å™¨ - ç¡®ä¿æ¶ˆæ¯å¤„ç†è¿‡ç¨‹ä¸­é¡ºåºçš„æ­£ç¡®æ€§"""
    
    def __init__(self, original_messages: List[dict]):
        """
        åˆå§‹åŒ–æ¶ˆæ¯é¡ºåºç®¡ç†å™¨
        ä¸ºæ¯æ¡æ¶ˆæ¯åˆ†é…å”¯ä¸€IDå’Œé¡ºåºæ ‡è®°
        """
        self.original_messages = copy.deepcopy(original_messages)
        self.order_map = {}  # æ¶ˆæ¯IDåˆ°åŸå§‹ç´¢å¼•çš„æ˜ å°„
        self.message_ids = {}  # åŸå§‹ç´¢å¼•åˆ°æ¶ˆæ¯IDçš„æ˜ å°„
        self.content_map = {}  # å†…å®¹æ ‡è¯†åˆ°åŸå§‹ç´¢å¼•çš„æ˜ å°„
        
        # ä¸ºæ¯æ¡æ¶ˆæ¯åˆ†é…å”¯ä¸€IDå’Œé¡ºåº
        for i, msg in enumerate(self.original_messages):
            content_key = self._generate_content_key(msg)
            msg_id = hashlib.md5(f"{i}_{content_key}".encode()).hexdigest()
            
            self.order_map[msg_id] = i
            self.message_ids[i] = msg_id
            self.content_map[content_key] = i
            
            # åœ¨æ¶ˆæ¯ä¸­æ·»åŠ é¡ºåºæ ‡è®°
            msg["_order_id"] = msg_id
            msg["_original_index"] = i
            msg["_content_key"] = content_key

    def _generate_content_key(self, msg: dict) -> str:
        """ç”Ÿæˆæ¶ˆæ¯å†…å®¹çš„å”¯ä¸€æ ‡è¯†"""
        role = msg.get("role", "")
        content = msg.get("content", "")
        
        # å¤„ç†å¤šæ¨¡æ€å†…å®¹
        if isinstance(content, list):
            content_parts = []
            for item in content:
                if item.get("type") == "text":
                    content_parts.append(item.get("text", ""))
                elif item.get("type") == "image_url":
                    # å¯¹äºbase64å›¾ç‰‡ï¼Œåªå–å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ ‡è¯†
                    image_data = item.get("image_url", {}).get("url", "")
                    if image_data.startswith("data:"):
                        content_parts.append(f"[IMAGE:base64:{image_data[:50]}]")
                    else:
                        content_parts.append(f"[IMAGE:url:{image_data[:50]}]")
            content_str = " ".join(content_parts)
        else:
            content_str = str(content)
        
        return f"{role}:{content_str[:200]}"

    def find_current_user_message_index(self, messages: List[dict]) -> int:
        """æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•ï¼ˆæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰"""
        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if msg.get("role") == "user":
                return i
        return -1

    def sort_messages_preserve_user(
        self, messages: List[dict], current_user_message: dict = None
    ) -> List[dict]:
        """
        æ ¹æ®åŸå§‹é¡ºåºæ’åºæ¶ˆæ¯ï¼Œä½†ä¿æŠ¤å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„ä½ç½®
        ç¡®ä¿å½“å‰ç”¨æˆ·æ¶ˆæ¯å§‹ç»ˆåœ¨æœ€å
        """
        if not messages:
            return messages

        # åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå…¶ä»–æ¶ˆæ¯
        other_messages = []
        current_user_in_list = None
        
        for msg in messages:
            if current_user_message and msg.get("_order_id") == current_user_message.get("_order_id"):
                current_user_in_list = msg
            else:
                other_messages.append(msg)

        # æŒ‰åŸå§‹é¡ºåºæ’åºå…¶ä»–æ¶ˆæ¯
        def get_order(msg):
            return msg.get("_original_index", 999999)
        
        other_messages.sort(key=get_order)

        # å¦‚æœæ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼Œå°†å…¶æ”¾åœ¨æœ€å
        if current_user_in_list:
            return other_messages + [current_user_in_list]
        else:
            return other_messages

    def get_message_preview(self, msg: dict) -> str:
        """è·å–æ¶ˆæ¯é¢„è§ˆç”¨äºè°ƒè¯•"""
        if isinstance(msg.get("content"), list):
            text_parts = []
            for item in msg.get("content", []):
                if item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
                elif item.get("type") == "image_url":
                    text_parts.append("[å›¾ç‰‡]")
            content = " ".join(text_parts)
        else:
            content = str(msg.get("content", ""))
        
        # åªåšæœ€åŸºæœ¬çš„æ¸…ç†
        content = content.replace("\n", " ").replace("\r", " ")
        content = re.sub(r"\s+", " ", content).strip()
        return content[:100] + "..." if len(content) > 100 else content


class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯è®°å½•å™¨"""
    
    def __init__(self):
        # åŸºç¡€ç»Ÿè®¡
        self.original_tokens = 0
        self.original_messages = 0
        self.final_tokens = 0
        self.final_messages = 0
        self.token_limit = 0
        self.target_tokens = 0
        self.current_user_tokens = 0
        
        # å¤„ç†ç»Ÿè®¡
        self.iterations = 0
        self.chunked_messages = 0
        self.summarized_messages = 0
        self.vector_retrievals = 0
        self.rerank_operations = 0
        self.multimodal_processed = 0
        self.processing_time = 0.0
        
        # å†…å®¹ä¿ç•™ç»Ÿè®¡
        self.current_user_preserved = False
        self.preserved_messages = 0
        self.processed_messages = 0
        self.summary_messages = 0
        self.emergency_truncations = 0
        self.content_loss_ratio = 0.0
        self.discarded_messages = 0
        self.recovered_messages = 0
        self.window_utilization = 0.0
        
        # å°½é‡ä¿ç•™ç»Ÿè®¡
        self.try_preserve_tokens = 0
        self.try_preserve_messages = 0
        self.try_preserve_summary_messages = 0
        
        # æ–°å¢ç»Ÿè®¡
        self.keyword_generations = 0
        self.context_maximization_detections = 0
        self.chunk_created = 0
        self.chunk_processed = 0
        self.recursive_summaries = 0
        
        # ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†ç»Ÿè®¡
        self.context_max_direct_preserve = 0
        self.context_max_chunked = 0
        self.context_max_summarized = 0
        self.multimodal_extracted = 0
        
        # å®¹é”™æœºåˆ¶ç»Ÿè®¡
        self.fallback_preserve_applied = 0
        self.user_message_recovery_count = 0
        self.rag_no_results_count = 0

    def calculate_retention_ratio(self) -> float:
        """è®¡ç®—å†…å®¹ä¿ç•™æ¯”ä¾‹"""
        if self.original_tokens == 0:
            return 0.0
        return self.final_tokens / self.original_tokens

    def calculate_window_usage_ratio(self) -> float:
        """è®¡ç®—å¯¹è¯çª—å£ä½¿ç”¨ç‡"""
        if self.target_tokens == 0:
            return 0.0
        return self.final_tokens / self.target_tokens

    def calculate_compression_ratio(self) -> float:
        """è®¡ç®—å‹ç¼©æ¯”ä¾‹"""
        if self.original_tokens == 0:
            return 0.0
        return (self.original_tokens - self.final_tokens) / self.original_tokens

    def calculate_processing_efficiency(self) -> float:
        """è®¡ç®—å¤„ç†æ•ˆç‡"""
        if self.processing_time == 0:
            return 0.0
        return self.final_tokens / self.processing_time

    def get_summary(self) -> str:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        retention = self.calculate_retention_ratio()
        window_usage = self.calculate_window_usage_ratio()
        compression = self.calculate_compression_ratio()
        efficiency = self.calculate_processing_efficiency()
        
        return f"""
ğŸ“Š å†…å®¹æœ€å¤§åŒ–å¤„ç†ç»Ÿè®¡æŠ¥å‘Š:
â”œâ”€ ğŸ“¥ è¾“å…¥: {self.original_messages}æ¡æ¶ˆæ¯, {self.original_tokens:,}tokens
â”œâ”€ ğŸ“¤ è¾“å‡º: {self.final_messages}æ¡æ¶ˆæ¯, {self.final_tokens:,}tokens
â”œâ”€ ğŸ¯ æ¨¡å‹é™åˆ¶: {self.token_limit:,}tokens
â”œâ”€ ğŸªŸ ç›®æ ‡çª—å£: {self.target_tokens:,}tokens
â”œâ”€ ğŸ‘¤ å½“å‰ç”¨æˆ·: {self.current_user_tokens:,}tokens
â”œâ”€ ğŸ“ˆ å†…å®¹ä¿ç•™ç‡: {retention:.2%}
â”œâ”€ ğŸªŸ çª—å£ä½¿ç”¨ç‡: {window_usage:.2%}
â”œâ”€ ğŸ“‰ å‹ç¼©æ¯”ä¾‹: {compression:.2%}
â”œâ”€ âš¡ å¤„ç†æ•ˆç‡: {efficiency:.0f}tokens/s
â”œâ”€ ğŸ”„ è¿­ä»£æ¬¡æ•°: {self.iterations}
â”œâ”€ ğŸ§© åˆ†ç‰‡å¤„ç†: {self.chunk_created}ä¸ªåˆ†ç‰‡ï¼Œ{self.chunk_processed}ä¸ªå¤„ç†
â”œâ”€ ğŸ“ æ‘˜è¦å‹ç¼©: {self.summarized_messages}æ¡
â”œâ”€ ğŸ”„ é€’å½’æ‘˜è¦: {self.recursive_summaries}æ¬¡
â”œâ”€ ğŸ” å‘é‡æ£€ç´¢: {self.vector_retrievals}æ¬¡
â”œâ”€ ğŸ”„ é‡æ’åº: {self.rerank_operations}æ¬¡
â”œâ”€ ğŸ–¼ï¸ å¤šæ¨¡æ€å¤„ç†: {self.multimodal_processed}å¼ å›¾ç‰‡
â”œâ”€ ğŸ”‘ å…³é”®å­—ç”Ÿæˆ: {self.keyword_generations}æ¬¡
â”œâ”€ ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹: {self.context_maximization_detections}æ¬¡
â”œâ”€ ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†: ç›´æ¥ä¿ç•™{self.context_max_direct_preserve}æ¡, åˆ†ç‰‡{self.context_max_chunked}æ¡, æ‘˜è¦{self.context_max_summarized}æ¡
â”œâ”€ ğŸ¨ å¤šæ¨¡æ€æå–: {self.multimodal_extracted}ä¸ªå¤šæ¨¡æ€æ¶ˆæ¯
â”œâ”€ ğŸ’¬ å½“å‰ç”¨æˆ·: {'âœ…å·²ä¿ç•™' if self.current_user_preserved else 'âŒæœªä¿ç•™'}
â”œâ”€ ğŸ”’ å°½é‡ä¿ç•™: {self.try_preserve_messages}æ¡æ¶ˆæ¯({self.try_preserve_tokens:,}tokens)
â”œâ”€ ğŸ“ å°½é‡ä¿ç•™æ‘˜è¦: {self.try_preserve_summary_messages}æ¡
â”œâ”€ ğŸ”„ åˆå¹¶å†…å®¹: {self.recovered_messages}æ¡
â”œâ”€ ğŸ†˜ ç´§æ€¥æˆªæ–­: {self.emergency_truncations}æ¬¡
â”œâ”€ ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤: åå¤‡ä¿ç•™{self.fallback_preserve_applied}æ¬¡, ç”¨æˆ·æ¶ˆæ¯æ¢å¤{self.user_message_recovery_count}æ¬¡
â”œâ”€ ğŸ” RAGæ— ç»“æœ: {self.rag_no_results_count}æ¬¡
â””â”€ â±ï¸ å¤„ç†æ—¶é—´: {self.processing_time:.2f}ç§’"""


class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨ - ç”¨äºæ˜¾ç¤ºå¤„ç†è¿›åº¦"""
    
    def __init__(self, event_emitter):
        self.event_emitter = event_emitter
        self.current_step = 0
        self.total_steps = 0
        self.current_phase = ""
        self.phase_progress = 0
        self.phase_total = 0
        self.logged_phases = set()  # é˜²æ­¢é‡å¤æ—¥å¿—

    def create_progress_bar(self, percentage: float, width: int = 15) -> str:
        """åˆ›å»ºç¾è§‚çš„è¿›åº¦æ¡"""
        filled = int(percentage * width / 100)
        if percentage >= 100:
            bar = "â–ˆ" * width
        else:
            bar = "â–ˆ" * filled + "â–“" * max(0, 1) + "â–‘" * max(0, width - filled - 1)
        return f"[{bar}] {percentage:.1f}%"

    async def start_phase(self, phase_name: str, total_items: int = 0):
        """å¼€å§‹æ–°é˜¶æ®µ"""
        self.current_phase = phase_name
        self.phase_progress = 0
        self.phase_total = total_items
        self.logged_phases.add(phase_name)
        await self.update_status(f"ğŸš€ å¼€å§‹ {phase_name}")

    async def update_progress(self, completed: int, total: int = None, detail: str = ""):
        """æ›´æ–°è¿›åº¦"""
        if total is None:
            total = self.phase_total
        
        self.phase_progress = completed
        if total > 0:
            percentage = (completed / total) * 100
            progress_bar = self.create_progress_bar(percentage)
            status = f"ğŸ”„ {self.current_phase} {progress_bar} ({completed}/{total})"
            if detail:
                status += f" - {detail}"
        else:
            status = f"ğŸ”„ {self.current_phase}"
            if detail:
                status += f" - {detail}"
        
        await self.update_status(status, False)

    async def complete_phase(self, message: str = ""):
        """å®Œæˆå½“å‰é˜¶æ®µ"""
        final_message = f"âœ… {self.current_phase} å®Œæˆ"
        if message:
            final_message += f" - {message}"
        await self.update_status(final_message, True)

    async def update_status(self, message: str, done: bool = False):
        """æ›´æ–°çŠ¶æ€"""
        if self.event_emitter:
            try:
                # åŸºæœ¬æ¸…ç†ï¼Œä¿ç•™ä¸»è¦å†…å®¹
                message = message.replace("\n", " ").replace("\r", " ")
                message = re.sub(r"\s+", " ", message).strip()
                await self.event_emitter({
                    "type": "status",
                    "data": {"description": message, "done": done},
                })
            except Exception as e:
                # é¿å…é‡å¤æ—¥å¿—
                if str(e) not in self.logged_phases:
                    print(f"âš ï¸ è¿›åº¦æ›´æ–°å¤±è´¥: {e}")
                    self.logged_phases.add(str(e))


class ModelMatcher:
    """æ™ºèƒ½æ¨¡å‹åŒ¹é…å™¨ - æ”¯æŒæ¨¡ç³ŠåŒ¹é…ä½†é¿å…thinkingæ¨¡å‹è¯¯åŒ¹é…"""
    
    def __init__(self):
        # å®šä¹‰æ¨¡å‹åŒ¹é…è§„åˆ™
        self.exact_matches = {
            # GPTç³»åˆ—
            "gpt-4o": {"family": "gpt", "multimodal": True, "limit": 128000},
            "gpt-4o-mini": {"family": "gpt", "multimodal": True, "limit": 128000},
            "gpt-4": {"family": "gpt", "multimodal": False, "limit": 8192},
            "gpt-4-turbo": {"family": "gpt", "multimodal": True, "limit": 128000},
            "gpt-4-vision-preview": {"family": "gpt", "multimodal": True, "limit": 128000},
            "gpt-3.5-turbo": {"family": "gpt", "multimodal": False, "limit": 16385},
            
            # Claudeç³»åˆ—
            "claude-3-5-sonnet": {"family": "claude", "multimodal": True, "limit": 200000},
            "claude-3-opus": {"family": "claude", "multimodal": True, "limit": 200000},
            "claude-3-haiku": {"family": "claude", "multimodal": True, "limit": 200000},
            "claude-3": {"family": "claude", "multimodal": True, "limit": 200000},
            "anthropic.claude-4-sonnet-latest-extended-thinking": {
                "family": "claude", "multimodal": True, "limit": 200000, "special": "thinking"
            },
            
            # Doubaoç³»åˆ—
            "doubao-1.5-vision-pro": {"family": "doubao", "multimodal": True, "limit": 128000},
            "doubao-1.5-vision-lite": {"family": "doubao", "multimodal": True, "limit": 128000},
            "doubao-1.5-thinking-pro": {"family": "doubao", "multimodal": False, "limit": 128000, "special": "thinking"},
            "doubao-seed-1-6-250615": {"family": "doubao", "multimodal": True, "limit": 50000},
            "doubao-seed": {"family": "doubao", "multimodal": False, "limit": 50000},
            "doubao": {"family": "doubao", "multimodal": False, "limit": 50000},
            "doubao-1-5-pro-256k": {"family": "doubao", "multimodal": False, "limit": 200000},
            
            # Geminiç³»åˆ—
            "gemini-pro": {"family": "gemini", "multimodal": False, "limit": 128000},
            "gemini-pro-vision": {"family": "gemini", "multimodal": True, "limit": 128000},
            
            # Qwenç³»åˆ—
            "qwen-vl": {"family": "qwen", "multimodal": True, "limit": 32000},
        }
        
        # æ¨¡ç³ŠåŒ¹é…è§„åˆ™ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        self.fuzzy_patterns = [
            # Thinkingæ¨¡å‹ä¼˜å…ˆåŒ¹é…ï¼ˆé¿å…è¯¯åŒ¹é…ï¼‰
            {"pattern": r".*thinking.*", "family": "thinking", "multimodal": False, "limit": 200000, "special": "thinking"},
            
            # GPTç³»åˆ—æ¨¡ç³ŠåŒ¹é…
            {"pattern": r"gpt-4o.*", "family": "gpt", "multimodal": True, "limit": 128000},
            {"pattern": r"gpt-4.*vision.*", "family": "gpt", "multimodal": True, "limit": 128000},
            {"pattern": r"gpt-4.*turbo.*", "family": "gpt", "multimodal": True, "limit": 128000},
            {"pattern": r"gpt-4.*", "family": "gpt", "multimodal": False, "limit": 8192},
            {"pattern": r"gpt-3\.5.*", "family": "gpt", "multimodal": False, "limit": 16385},
            {"pattern": r"gpt.*", "family": "gpt", "multimodal": False, "limit": 16385},
            
            # Claudeç³»åˆ—æ¨¡ç³ŠåŒ¹é…
            {"pattern": r"claude.*3.*5.*sonnet.*", "family": "claude", "multimodal": True, "limit": 200000},
            {"pattern": r"claude.*3.*opus.*", "family": "claude", "multimodal": True, "limit": 200000},
            {"pattern": r"claude.*3.*haiku.*", "family": "claude", "multimodal": True, "limit": 200000},
            {"pattern": r"claude.*3.*", "family": "claude", "multimodal": True, "limit": 200000},
            {"pattern": r"claude.*", "family": "claude", "multimodal": True, "limit": 200000},
            {"pattern": r"anthropic.*claude.*", "family": "claude", "multimodal": True, "limit": 200000},
            
            # Doubaoç³»åˆ—æ¨¡ç³ŠåŒ¹é…
            {"pattern": r"doubao.*vision.*", "family": "doubao", "multimodal": True, "limit": 128000},
            {"pattern": r"doubao.*seed.*", "family": "doubao", "multimodal": True, "limit": 50000},
            {"pattern": r"doubao.*256k.*", "family": "doubao", "multimodal": False, "limit": 200000},
            {"pattern": r"doubao.*1\.5.*", "family": "doubao", "multimodal": False, "limit": 128000},
            {"pattern": r"doubao.*", "family": "doubao", "multimodal": False, "limit": 50000},
            
            # Geminiç³»åˆ—æ¨¡ç³ŠåŒ¹é…
            {"pattern": r"gemini.*vision.*", "family": "gemini", "multimodal": True, "limit": 128000},
            {"pattern": r"gemini.*pro.*", "family": "gemini", "multimodal": False, "limit": 128000},
            {"pattern": r"gemini.*", "family": "gemini", "multimodal": False, "limit": 128000},
            
            # Qwenç³»åˆ—æ¨¡ç³ŠåŒ¹é…
            {"pattern": r"qwen.*vl.*", "family": "qwen", "multimodal": True, "limit": 32000},
            {"pattern": r"qwen.*", "family": "qwen", "multimodal": False, "limit": 32000},
        ]

    def match_model(self, model_name: str) -> Dict[str, Any]:
        """æ™ºèƒ½åŒ¹é…æ¨¡å‹ä¿¡æ¯"""
        if not model_name:
            return {"family": "unknown", "multimodal": False, "limit": 200000}
        
        model_lower = model_name.lower().strip()
        
        # 1. ç²¾ç¡®åŒ¹é…
        for exact_name, info in self.exact_matches.items():
            if exact_name.lower() == model_lower:
                return {**info, "matched_name": exact_name, "match_type": "exact"}
        
        # 2. æ¨¡ç³ŠåŒ¹é…
        for pattern_info in self.fuzzy_patterns:
            pattern = pattern_info["pattern"]
            if re.match(pattern, model_lower):
                return {
                    "family": pattern_info["family"],
                    "multimodal": pattern_info["multimodal"],
                    "limit": pattern_info["limit"],
                    "special": pattern_info.get("special"),
                    "matched_pattern": pattern,
                    "match_type": "fuzzy"
                }
        
        # 3. é»˜è®¤åŒ¹é…
        return {"family": "unknown", "multimodal": False, "limit": 200000, "match_type": "default"}


class TokenCalculator:
    """ç®€åŒ–çš„Tokenè®¡ç®—å™¨ - åªç”¨tiktoken"""
    
    def __init__(self):
        self._encoding = None

    def get_encoding(self):
        """è·å–tiktokenç¼–ç å™¨"""
        if not TIKTOKEN_AVAILABLE:
            return None
        
        if self._encoding is None:
            try:
                self._encoding = tiktoken.get_encoding("cl100k_base")
            except Exception:
                pass
        return self._encoding

    def count_tokens(self, text: str) -> int:
        """ç®€åŒ–çš„tokenè®¡ç®— - åªç”¨tiktoken"""
        if not text:
            return 0
        
        # è·å–tiktokenç¼–ç å™¨
        encoding = self.get_encoding()
        if encoding:
            try:
                return len(encoding.encode(str(text)))
            except Exception:
                pass
        
        # ç®€å•fallback
        return len(str(text)) // 4

    def calculate_image_tokens(self, image_) -> int:
        """ç®€åŒ–çš„å›¾ç‰‡tokenè®¡ç®—"""
        if not image_data:
            return 0
        # ç®€å•ä¼°ç®—ï¼šæ¯ä¸ªå›¾ç‰‡æŒ‰1500tokensè®¡ç®—
        return 1500


class Filter:
    class Valves(BaseModel):
        # åŸºç¡€æ§åˆ¶
        enable_processing: bool = Field(default=True, description="ğŸ”„ å¯ç”¨å†…å®¹æœ€å¤§åŒ–å¤„ç†")
        excluded_models: str = Field(default="", description="ğŸš« æ’é™¤æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)")
        
        # æ ¸å¿ƒé…ç½® - å†…å®¹æœ€å¤§åŒ–ä¸“ç”¨
        max_window_utilization: float = Field(default=0.95, description="ğŸªŸ æœ€å¤§çª—å£åˆ©ç”¨ç‡(95%)")
        aggressive_content_recovery: bool = Field(default=True, description="ğŸ”„ æ¿€è¿›å†…å®¹åˆå¹¶æ¨¡å¼")
        min_preserve_ratio: float = Field(default=0.75, description="ğŸ”’ æœ€å°å†…å®¹ä¿ç•™æ¯”ä¾‹(75%)")
        
        # å°½é‡ä¿ç•™é…ç½®
        enable_try_preserve: bool = Field(default=True, description="ğŸ”’ å¯ç”¨å°½é‡ä¿ç•™æœºåˆ¶")
        try_preserve_ratio: float = Field(default=0.40, description="ğŸ”’ å°½é‡ä¿ç•™é¢„ç®—æ¯”ä¾‹(40%)")
        try_preserve_exchanges: int = Field(default=3, description="ğŸ”’ å°½é‡ä¿ç•™å¯¹è¯è½®æ¬¡æ•°")
        
        # å“åº”ç©ºé—´é…ç½®
        response_buffer_ratio: float = Field(default=0.06, description="ğŸ“ å“åº”ç©ºé—´é¢„ç•™æ¯”ä¾‹(6%)")
        response_buffer_max: int = Field(default=3000, description="ğŸ“ å“åº”ç©ºé—´æœ€å¤§å€¼(tokens)")
        response_buffer_min: int = Field(default=1000, description="ğŸ“ å“åº”ç©ºé—´æœ€å°å€¼(tokens)")
        
        # å¤šæ¨¡æ€å¤„ç†é…ç½®
        multimodal_direct_threshold: float = Field(default=0.70, description="ğŸ¯ å¤šæ¨¡æ€ç›´æ¥è¾“å…¥Tokené¢„ç®—é˜ˆå€¼(70%)")
        preserve_images_in_multimodal: bool = Field(default=True, description="ğŸ“¸ å¤šæ¨¡æ€æ¨¡å‹æ˜¯å¦ä¿ç•™åŸå§‹å›¾ç‰‡")
        always_process_images_before_summary: bool = Field(default=True, description="ğŸ“ æ‘˜è¦å‰æ€»æ˜¯å…ˆå¤„ç†å›¾ç‰‡")
        
        # ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†é…ç½®
        enable_context_maximization: bool = Field(default=True, description="ğŸ“š å¯ç”¨ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†")
        context_max_direct_preserve_ratio: float = Field(default=0.30, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ç›´æ¥ä¿ç•™æ¯”ä¾‹(30%)")
        context_max_skip_rag: bool = Field(default=True, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–è·³è¿‡RAGå¤„ç†")
        context_max_prioritize_recent: bool = Field(default=True, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¼˜å…ˆä¿ç•™æœ€è¿‘å†…å®¹")
        
        # å®¹é”™æœºåˆ¶é…ç½®
        enable_fallback_preservation: bool = Field(default=True, description="ğŸ›¡ï¸ å¯ç”¨å®¹é”™ä¿æŠ¤æœºåˆ¶")
        fallback_preserve_ratio: float = Field(default=0.20, description="ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤é¢„ç•™æ¯”ä¾‹(20%)")
        min_history_messages: int = Field(default=5, description="ğŸ›¡ï¸ æœ€å°‘å†å²æ¶ˆæ¯æ•°é‡")
        force_preserve_recent_user_exchanges: int = Field(default=2, description="ğŸ›¡ï¸ å¼ºåˆ¶ä¿ç•™æœ€è¿‘ç”¨æˆ·å¯¹è¯è½®æ¬¡")
        
        # åŠŸèƒ½å¼€å…³
        enable_multimodal: bool = Field(default=True, description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å¤„ç†")
        enable_vision_preprocessing: bool = Field(default=True, description="ğŸ‘ï¸ å¯ç”¨å›¾ç‰‡é¢„å¤„ç†")
        enable_vector_retrieval: bool = Field(default=True, description="ğŸ” å¯ç”¨å‘é‡æ£€ç´¢")
        enable_intelligent_chunking: bool = Field(default=True, description="ğŸ§© å¯ç”¨æ™ºèƒ½åˆ†ç‰‡")
        enable_recursive_summarization: bool = Field(default=True, description="ğŸ”„ å¯ç”¨é€’å½’æ‘˜è¦")
        enable_reranking: bool = Field(default=True, description="ğŸ”„ å¯ç”¨é‡æ’åº")
        
        # æ™ºèƒ½å…³é”®å­—ç”Ÿæˆå’Œä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹
        enable_keyword_generation: bool = Field(default=True, description="ğŸ”‘ å¯ç”¨æ™ºèƒ½å…³é”®å­—ç”Ÿæˆ")
        enable_ai_context_max_detection: bool = Field(default=True, description="ğŸ§  å¯ç”¨AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹")
        keyword_generation_for_context_max: bool = Field(default=True, description="ğŸ”‘ å¯¹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¯ç”¨å…³é”®å­—ç”Ÿæˆ")
        
        # ç»Ÿè®¡å’Œè°ƒè¯•
        enable_detailed_stats: bool = Field(default=True, description="ğŸ“Š å¯ç”¨è¯¦ç»†ç»Ÿè®¡")
        enable_detailed_progress: bool = Field(default=True, description="ğŸ“± å¯ç”¨è¯¦ç»†è¿›åº¦æ˜¾ç¤º")
        debug_level: int = Field(default=2, description="ğŸ› è°ƒè¯•çº§åˆ« 0-3")
        show_frontend_progress: bool = Field(default=True, description="ğŸ“± æ˜¾ç¤ºå¤„ç†è¿›åº¦")
        
        # APIé…ç½®
        api_error_retry_times: int = Field(default=2, description="ğŸ”„ APIé”™è¯¯é‡è¯•æ¬¡æ•°")
        api_error_retry_delay: float = Field(default=1.0, description="â±ï¸ APIé”™è¯¯é‡è¯•å»¶è¿Ÿ(ç§’)")
        
        # Tokenç®¡ç†
        default_token_limit: int = Field(default=200000, description="âš–ï¸ é»˜è®¤tokené™åˆ¶")
        token_safety_ratio: float = Field(default=0.92, description="ğŸ›¡ï¸ Tokenå®‰å…¨æ¯”ä¾‹(92%)")
        target_window_usage: float = Field(default=0.85, description="ğŸªŸ ç›®æ ‡çª—å£ä½¿ç”¨ç‡(85%)")
        max_processing_iterations: int = Field(default=5, description="ğŸ”„ æœ€å¤§å¤„ç†è¿­ä»£æ¬¡æ•°")
        
        # ä¿æŠ¤ç­–ç•¥
        force_preserve_current_user_message: bool = Field(default=True, description="ğŸ”’ å¼ºåˆ¶ä¿ç•™å½“å‰ç”¨æˆ·æ¶ˆæ¯(æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯)")
        preserve_recent_exchanges: int = Field(default=4, description="ğŸ’¬ ä¿æŠ¤æœ€è¿‘å®Œæ•´å¯¹è¯è½®æ¬¡")
        max_preserve_ratio: float = Field(default=0.3, description="ğŸ”’ ä¿æŠ¤æ¶ˆæ¯æœ€å¤§tokenæ¯”ä¾‹")
        max_single_message_tokens: int = Field(default=20000, description="ğŸ“ å•æ¡æ¶ˆæ¯æœ€å¤§token")
        
        # æ™ºèƒ½åˆ†ç‰‡é…ç½®
        enable_smart_chunking: bool = Field(default=True, description="ğŸ§© å¯ç”¨æ™ºèƒ½åˆ†ç‰‡")
        chunk_target_tokens: int = Field(default=4000, description="ğŸ§© åˆ†ç‰‡ç›®æ ‡tokenæ•°")
        chunk_overlap_tokens: int = Field(default=400, description="ğŸ”— åˆ†ç‰‡é‡å tokenæ•°")
        chunk_min_tokens: int = Field(default=1000, description="ğŸ“ åˆ†ç‰‡æœ€å°tokenæ•°")
        chunk_max_tokens: int = Field(default=8000, description="ğŸ“ åˆ†ç‰‡æœ€å¤§tokenæ•°")
        large_message_threshold: int = Field(default=10000, description="ğŸ“ å¤§æ¶ˆæ¯åˆ†ç‰‡é˜ˆå€¼")
        preserve_paragraph_integrity: bool = Field(default=True, description="ğŸ“ ä¿æŒæ®µè½å®Œæ•´æ€§")
        preserve_sentence_integrity: bool = Field(default=True, description="ğŸ“ ä¿æŒå¥å­å®Œæ•´æ€§")
        preserve_code_blocks: bool = Field(default=True, description="ğŸ’» ä¿æŒä»£ç å—å®Œæ•´æ€§")
        
        # å†…å®¹ä¼˜å…ˆçº§è®¾ç½®
        high_priority_content: str = Field(
            default="ä»£ç ,é…ç½®,å‚æ•°,æ•°æ®,é”™è¯¯,è§£å†³æ–¹æ¡ˆ,æ­¥éª¤,æ–¹æ³•,æŠ€æœ¯ç»†èŠ‚,API,å‡½æ•°,ç±»,å˜é‡,é—®é¢˜,bug,ä¿®å¤,å®ç°,ç®—æ³•,æ¶æ„,ç”¨æˆ·é—®é¢˜,å…³é”®å›ç­”",
            description="ğŸ¯ é«˜ä¼˜å…ˆçº§å†…å®¹å…³é”®è¯(é€—å·åˆ†éš”)"
        )
        
        # ç»Ÿä¸€çš„APIé…ç½® - ç®€åŒ–æ¨¡å‹é…ç½®
        api_base: str = Field(default="https://ark.cn-beijing.volces.com/api/v3", description="ğŸ”— APIåŸºç¡€åœ°å€")
        api_key: str = Field(default="", description="ğŸ”‘ APIå¯†é’¥")
        
        # å¤šæ¨¡æ€æ¨¡å‹é…ç½®ï¼ˆVisionå’Œå¤šæ¨¡æ€æ‘˜è¦å…±ç”¨ï¼‰
        multimodal_model: str = Field(default="doubao-1.5-vision-pro-250328", description="ğŸ–¼ï¸ å¤šæ¨¡æ€æ¨¡å‹")
        
        # æ–‡æœ¬æ¨¡å‹é…ç½®ï¼ˆæ‘˜è¦ã€å…³é”®å­—ç”Ÿæˆã€ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹å…±ç”¨ï¼‰
        text_model: str = Field(default="doubao-1-5-lite-32k-250115", description="ğŸ“ æ–‡æœ¬å¤„ç†æ¨¡å‹")
        
        # å‘é‡æ¨¡å‹é…ç½®
        text_vector_model: str = Field(default="doubao-embedding-large-text-250515", description="ğŸ§  æ–‡æœ¬å‘é‡æ¨¡å‹")
        multimodal_vector_model: str = Field(default="doubao-embedding-vision-250615", description="ğŸ§  å¤šæ¨¡æ€å‘é‡æ¨¡å‹")
        
        # Visionç›¸å…³é…ç½®
        vision_prompt_template: str = Field(
            default="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€æ–‡å­—ã€é¢œè‰²ã€å¸ƒå±€ç­‰æ‰€æœ‰å¯è§ä¿¡æ¯ã€‚ç‰¹åˆ«æ³¨æ„ä»£ç ã€é…ç½®ã€æ•°æ®ç­‰æŠ€æœ¯ä¿¡æ¯ã€‚ä¿æŒå®¢è§‚å‡†ç¡®ï¼Œé‡ç‚¹çªå‡ºå…³é”®ä¿¡æ¯ã€‚",
            description="ğŸ‘ï¸ Visionæç¤ºè¯"
        )
        vision_max_tokens: int = Field(default=2000, description="ğŸ‘ï¸ Visionæœ€å¤§è¾“å‡ºtokens")
        
        # å…³é”®å­—ç”Ÿæˆé…ç½®
        keyword_generation_prompt: str = Field(
            default="""ä½ æ˜¯ä¸“ä¸šçš„æœç´¢å…³é”®å­—ç”ŸæˆåŠ©æ‰‹ã€‚ç”¨æˆ·è¾“å…¥äº†ä¸€ä¸ªæŸ¥è¯¢ï¼Œä½ éœ€è¦ç”Ÿæˆå¤šä¸ªç›¸å…³çš„æœç´¢å…³é”®å­—æ¥å¸®åŠ©åœ¨å¯¹è¯å†å²ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚

ğŸ“‹ ä»»åŠ¡è¦æ±‚ï¼š
1. åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾å’Œä¸»é¢˜
2. ç”Ÿæˆ5-10ä¸ªç›¸å…³çš„æœç´¢å…³é”®å­—
3. åŒ…å«åŒä¹‰è¯ã€ç›¸å…³è¯ã€æŠ€æœ¯æœ¯è¯­
4. å¯¹äºå®½æ³›æŸ¥è¯¢ï¼ˆå¦‚"èŠäº†ä»€ä¹ˆ"ã€"è¯´äº†ä»€ä¹ˆ"ï¼‰ï¼Œç”Ÿæˆé€šç”¨ä½†æœ‰æ•ˆçš„å…³é”®å­—
5. å…³é”®å­—åº”è¯¥èƒ½è¦†ç›–å¯èƒ½çš„å¯¹è¯ä¸»é¢˜

ğŸ“ è¾“å‡ºæ ¼å¼ï¼š
ç›´æ¥è¾“å‡ºå…³é”®å­—ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

ç°åœ¨è¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”Ÿæˆå…³é”®å­—ï¼š""",
            description="ğŸ”‘ å…³é”®å­—ç”Ÿæˆæç¤ºè¯"
        )
        
        # ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹é…ç½®
        context_max_detection_prompt: str = Field(
            default="""ä½ æ˜¯ä¸“ä¸šçš„æŸ¥è¯¢æ„å›¾åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢æ˜¯å¦éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†ã€‚

ğŸ“‹ åˆ¤æ–­æ ‡å‡†ï¼š
éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„æŸ¥è¯¢ç‰¹å¾ï¼š
- è¯¢é—®"èŠäº†ä»€ä¹ˆ"ã€"è¯´äº†ä»€ä¹ˆ"ã€"è®¨è®ºäº†ä»€ä¹ˆ"ç­‰å®½æ³›å†…å®¹
- è¯¢é—®"ä¹‹å‰çš„å†…å®¹"ã€"å†å²è®°å½•"ã€"å¯¹è¯å†å²"ç­‰
- ç¼ºä¹å…·ä½“çš„ä¸»é¢˜ã€å…³é”®è¯æˆ–æ˜ç¡®çš„æœç´¢æ„å›¾
- æŸ¥è¯¢è¯æ±‡å°‘äº3ä¸ªæœ‰æ•ˆè¯æ±‡

ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„æŸ¥è¯¢ç‰¹å¾ï¼š
- åŒ…å«æ˜ç¡®çš„ä¸»é¢˜ã€æŠ€æœ¯æœ¯è¯­ã€äº§å“åç§°ç­‰
- æœ‰å…·ä½“çš„é—®é¢˜æŒ‡å‘
- åŒ…å«è¯¦ç»†çš„æè¿°æˆ–èƒŒæ™¯ä¿¡æ¯

ğŸ“ è¾“å‡ºæ ¼å¼ï¼š
åªè¾“å‡º "éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–" æˆ– "ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–"ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

ç°åœ¨è¯·åˆ†æä»¥ä¸‹æŸ¥è¯¢ï¼š""",
            description="ğŸ§  ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹æç¤ºè¯"
        )
        
        # å‘é‡æ£€ç´¢é…ç½®
        vector_similarity_threshold: float = Field(default=0.06, description="ğŸ¯ åŸºç¡€ç›¸ä¼¼åº¦é˜ˆå€¼")
        multimodal_similarity_threshold: float = Field(default=0.04, description="ğŸ–¼ï¸ å¤šæ¨¡æ€ç›¸ä¼¼åº¦é˜ˆå€¼")
        text_similarity_threshold: float = Field(default=0.08, description="ğŸ“ æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼")
        vector_top_k: int = Field(default=150, description="ğŸ” å‘é‡æ£€ç´¢Top-Kæ•°é‡")
        
        # é‡æ’åºAPIé…ç½®
        rerank_api_base: str = Field(default="https://api.bochaai.com", description="ğŸ”„ é‡æ’åºAPI")
        rerank_api_key: str = Field(default="", description="ğŸ”‘ é‡æ’åºå¯†é’¥")
        rerank_model: str = Field(default="gte-rerank", description="ğŸ§  é‡æ’åºæ¨¡å‹")
        rerank_top_k: int = Field(default=100, description="ğŸ” é‡æ’åºè¿”å›æ•°é‡")
        
        # æ‘˜è¦é…ç½®
        max_summary_length: int = Field(default=25000, description="ğŸ“ æ‘˜è¦æœ€å¤§é•¿åº¦")
        min_summary_ratio: float = Field(default=0.30, description="ğŸ“ æ‘˜è¦æœ€å°é•¿åº¦æ¯”ä¾‹")
        summary_compression_ratio: float = Field(default=0.40, description="ğŸ“Š æ‘˜è¦å‹ç¼©æ¯”ä¾‹")
        max_recursion_depth: int = Field(default=3, description="ğŸ”„ æœ€å¤§é€’å½’æ·±åº¦")
        
        # æ€§èƒ½é…ç½®
        max_concurrent_requests: int = Field(default=6, description="âš¡ æœ€å¤§å¹¶å‘æ•°")
        request_timeout: int = Field(default=45, description="â±ï¸ è¯·æ±‚è¶…æ—¶(ç§’)")

    def __init__(self):
        print("\n" + "=" * 60)
        print("ğŸš€ Advanced Context Manager v2.3.1 - ä¿®å¤RAGæœç´¢æ— ç»“æœé—®é¢˜")
        print("ğŸ“ æ’ä»¶æ­£åœ¨åˆå§‹åŒ–...")
        print("ğŸ”§ å¢å¼ºå®¹é”™æœºåˆ¶ï¼Œç¡®ä¿ç”¨æˆ·æ¶ˆæ¯æ­£ç¡®è¯†åˆ«...")
        
        self.valves = self.Valves()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model_matcher = ModelMatcher()
        self.token_calculator = TokenCalculator()
        
        # å¤„ç†ç»Ÿè®¡
        self.stats = ProcessingStats()
        
        # æ¶ˆæ¯é¡ºåºç®¡ç†å™¨
        self.message_order = None
        self.current_processing_id = None
        self.current_user_message = None
        self.current_model_info = None
        
        # è§£æé…ç½®
        self._parse_configurations()
        
        print(f"âœ… æ’ä»¶åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ”¥ å†…å®¹æœ€å¤§åŒ–æ¨¡å¼: å¯ç”¨")
        print(f"ğŸªŸ æœ€å¤§çª—å£åˆ©ç”¨ç‡: {self.valves.max_window_utilization:.1%}")
        print(f"ğŸ”„ æ¿€è¿›å†…å®¹åˆå¹¶: {self.valves.aggressive_content_recovery}")
        print(f"ğŸ”’ å°½é‡ä¿ç•™æœºåˆ¶: {self.valves.enable_try_preserve} (é¢„ç®—:{self.valves.try_preserve_ratio:.1%})")
        print(f"ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–: {self.valves.enable_context_maximization} (ç›´æ¥ä¿ç•™:{self.valves.context_max_direct_preserve_ratio:.1%})")
        print(f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤æœºåˆ¶: {self.valves.enable_fallback_preservation} (é¢„ç•™:{self.valves.fallback_preserve_ratio:.1%})")
        print(f"ğŸ”‘ æ™ºèƒ½å…³é”®å­—ç”Ÿæˆ: {self.valves.enable_keyword_generation}")
        print(f"ğŸ§  AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹: {self.valves.enable_ai_context_max_detection}")
        print(f"ğŸ§© æ™ºèƒ½åˆ†ç‰‡: {self.valves.enable_smart_chunking} (é˜ˆå€¼:{self.valves.large_message_threshold:,}tokens)")
        print(f"ğŸ“Š Tokenè®¡ç®—å™¨: ç®€åŒ–ç‰ˆï¼ˆä»…ç”¨tiktokenï¼‰")
        print(f"ğŸ¯ æ¨¡å‹åŒ¹é…å™¨: æ™ºèƒ½æ¨¡ç³ŠåŒ¹é…")
        print(f"ğŸ–¼ï¸ å¤šæ¨¡æ€æ¨¡å‹: {self.valves.multimodal_model}")
        print(f"ğŸ“ æ–‡æœ¬å¤„ç†æ¨¡å‹: {self.valves.text_model}")
        print(f"ğŸ“Š è¯¦ç»†ç»Ÿè®¡: {self.valves.enable_detailed_stats}")
        print(f"ğŸ› è°ƒè¯•çº§åˆ«: {self.valves.debug_level}")
        print("=" * 60 + "\n")

    def _parse_configurations(self):
        """è§£æé…ç½®é¡¹"""
        # è§£æé«˜ä¼˜å…ˆçº§å†…å®¹å…³é”®è¯
        self.high_priority_keywords = set()
        if self.valves.high_priority_content:
            self.high_priority_keywords = {
                keyword.strip().lower() 
                for keyword in self.valves.high_priority_content.split(",") 
                if keyword.strip()
            }

    def reset_processing_state(self):
        """é‡ç½®å¤„ç†çŠ¶æ€"""
        self.current_processing_id = None
        self.message_order = None
        self.current_user_message = None
        self.current_model_info = None
        self.stats = ProcessingStats()

    def debug_log(self, level: int, message: str, emoji: str = "ğŸ”§"):
        """åˆ†çº§è°ƒè¯•æ—¥å¿—"""
        if self.valves.debug_level >= level:
            prefix = ["", "ğŸ›[DEBUG]", "ğŸ”[DETAIL]", "ğŸ“‹[VERBOSE]"][min(level, 3)]
            # åŸºæœ¬æ¸…ç†ï¼Œä¿ç•™ä¸»è¦å†…å®¹
            message = message.replace("\n", " ").replace("\r", " ")
            message = re.sub(r"\s+", " ", message).strip()
            print(f"{prefix} {emoji} {message}")

    def is_model_excluded(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«æ’é™¤"""
        if not self.valves.excluded_models or not model_name:
            return False
        
        excluded_list = [
            model.strip().lower() 
            for model in self.valves.excluded_models.split(",") 
            if model.strip()
        ]
        
        if not excluded_list:
            return False
        
        model_lower = model_name.lower()
        for excluded_model in excluded_list:
            if excluded_model in model_lower:
                self.debug_log(1, f"æ¨¡å‹ {model_name} åœ¨æ’é™¤åˆ—è¡¨ä¸­", "ğŸš«")
                return True
        
        return False

    def analyze_model(self, model_name: str) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹ä¿¡æ¯ - ä½¿ç”¨æ™ºèƒ½åŒ¹é…å™¨"""
        model_info = self.model_matcher.match_model(model_name)
        
        self.debug_log(2, f"æ¨¡å‹åˆ†æ: {model_name} -> {model_info['family']} "
                          f"({'å¤šæ¨¡æ€' if model_info['multimodal'] else 'æ–‡æœ¬'}) "
                          f"{model_info['limit']:,}tokens "
                          f"[{model_info['match_type']}åŒ¹é…]", "ğŸ¯")
        
        if model_info.get("special") == "thinking":
            self.debug_log(1, f"æ£€æµ‹åˆ°Thinkingæ¨¡å‹: {model_name}", "ğŸ§ ")
        
        return model_info

    def count_tokens(self, text: str) -> int:
        """ç®€åŒ–çš„tokenè®¡ç®—"""
        if not text:
            return 0
        return self.token_calculator.count_tokens(text)

    def count_message_tokens(self, message: dict) -> int:
        """è®¡ç®—å•æ¡æ¶ˆæ¯çš„tokenæ•°é‡ - ç®€åŒ–ç‰ˆæœ¬"""
        if not message:
            return 0
        
        content = message.get("content", "")
        role = message.get("role", "")
        total_tokens = 0
        
        # å¤„ç†å¤šæ¨¡æ€å†…å®¹
        if isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    text = item.get("text", "")
                    total_tokens += self.count_tokens(text)
                elif item.get("type") == "image_url":
                    # ç®€åŒ–å›¾ç‰‡tokenè®¡ç®—
                    total_tokens += self.token_calculator.calculate_image_tokens("")
        else:
            # çº¯æ–‡æœ¬å†…å®¹
            total_tokens = self.count_tokens(content)
        
        # åŠ ä¸Šè§’è‰²å’Œæ ¼å¼å¼€é”€
        total_tokens += self.count_tokens(role) + 20  # æ ¼å¼å¼€é”€
        
        return total_tokens

    def count_messages_tokens(self, messages: List[dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€»tokenæ•°é‡"""
        if not messages:
            return 0
        
        total_tokens = sum(self.count_message_tokens(msg) for msg in messages)
        self.debug_log(2, f"æ¶ˆæ¯åˆ—è¡¨tokenè®¡ç®—: {len(messages)}æ¡æ¶ˆæ¯ -> {total_tokens:,}tokens", "ğŸ“Š")
        return total_tokens

    def get_model_token_limit(self, model_name: str) -> int:
        """è·å–æ¨¡å‹çš„tokené™åˆ¶ï¼ˆåº”ç”¨å®‰å…¨ç³»æ•°ï¼‰"""
        model_info = self.analyze_model(model_name)
        limit = model_info.get("limit", self.valves.default_token_limit)
        safe_limit = int(limit * self.valves.token_safety_ratio)
        
        self.debug_log(2, f"æ¨¡å‹tokené™åˆ¶: {model_name} -> {limit} -> {safe_limit}", "âš–ï¸")
        return safe_limit

    def is_multimodal_model(self, model_name: str) -> bool:
        """åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€è¾“å…¥"""
        model_info = self.analyze_model(model_name)
        return model_info.get("multimodal", False)

    def should_force_vision_processing(self, model_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¼ºåˆ¶è¿›è¡Œè§†è§‰å¤„ç†"""
        # å¯¹äºéå¤šæ¨¡æ€æ¨¡å‹ï¼Œå¼ºåˆ¶è¿›è¡Œè§†è§‰å¤„ç†
        return not self.is_multimodal_model(model_name)

    def find_current_user_message(self, messages: List[dict]) -> Optional[dict]:
        """æŸ¥æ‰¾å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€æ–°çš„ç”¨æˆ·è¾“å…¥ï¼‰- ä¿®å¤ç‰ˆæœ¬"""
        if not messages:
            return None
        
        # ä»æœ€åä¸€æ¡æ¶ˆæ¯å¼€å§‹æŸ¥æ‰¾ï¼Œæ‰¾åˆ°æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        for msg in reversed(messages):
            if msg.get("role") == "user":
                self.debug_log(2, f"æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯: {len(self.extract_text_from_content(msg.get('content', '')))}å­—ç¬¦", "ğŸ’¬")
                return msg
        
        return None

    def separate_current_and_history_messages(self, messages: List[dict]) -> Tuple[Optional[dict], List[dict]]:
        """åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯ - ä¿®å¤ç‰ˆæœ¬ï¼Œä½¿ç”¨æ¶ˆæ¯IDè€Œä¸æ˜¯å¯¹è±¡å¼•ç”¨"""
        if not messages:
            return None, []
        
        # æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€æ–°çš„ç”¨æˆ·è¾“å…¥ï¼‰
        current_user_message = self.find_current_user_message(messages)
        if not current_user_message:
            return None, messages
        
        # è·å–å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„å”¯ä¸€æ ‡è¯†
        current_user_id = current_user_message.get("_order_id")
        if not current_user_id:
            # å¦‚æœæ²¡æœ‰order_idï¼Œä½¿ç”¨å†…å®¹keyä½œä¸ºå¤‡é€‰
            current_user_id = current_user_message.get("_content_key")
        
        # åˆ†ç¦»å†å²æ¶ˆæ¯ï¼ˆé™¤äº†å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯ï¼‰
        history_messages = []
        for msg in messages:
            msg_id = msg.get("_order_id") or msg.get("_content_key")
            # ä½¿ç”¨IDåŒ¹é…è€Œä¸æ˜¯å¯¹è±¡å¼•ç”¨åŒ¹é…
            if msg_id != current_user_id:
                history_messages.append(msg)
        
        self.debug_log(1, f"æ¶ˆæ¯åˆ†ç¦»: å½“å‰ç”¨æˆ·æ¶ˆæ¯1æ¡({self.count_message_tokens(current_user_message)}tokens), "
                          f"å†å²æ¶ˆæ¯{len(history_messages)}æ¡({self.count_messages_tokens(history_messages):,}tokens)", "ğŸ“‹")
        
        return current_user_message, history_messages

    def calculate_target_tokens(self, model_name: str, current_user_tokens: int) -> int:
        """è®¡ç®—ç›®æ ‡tokenæ•°ï¼šæ¨¡å‹é™åˆ¶ - å½“å‰ç”¨æˆ·æ¶ˆæ¯ - å“åº”ç©ºé—´"""
        model_token_limit = self.get_model_token_limit(model_name)
        
        # è®¡ç®—å“åº”ç©ºé—´
        response_buffer = min(
            self.valves.response_buffer_max,
            max(
                self.valves.response_buffer_min,
                int(model_token_limit * self.valves.response_buffer_ratio)
            )
        )
        
        # è®¡ç®—ç›®æ ‡ï¼šæ€»é™åˆ¶ - å½“å‰ç”¨æˆ·æ¶ˆæ¯ - å“åº”ç¼“å†²åŒº
        target_tokens = model_token_limit - current_user_tokens - response_buffer
        
        # ç¡®ä¿ä¸å°äºåŸºç¡€å€¼
        min_target = max(10000, model_token_limit * 0.3)
        target_tokens = max(target_tokens, min_target)
        
        self.debug_log(1, f"ğŸ¯ ç›®æ ‡tokenè®¡ç®—: {model_token_limit} - {current_user_tokens} - {response_buffer} = {target_tokens}", "ğŸ¯")
        return int(target_tokens)

    # ========== å¤šæ¨¡æ€å¤„ç†ç›¸å…³æ–¹æ³• ==========
    
    def has_images_in_content(self, content) -> bool:
        """æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        if isinstance(content, list):
            return any(item.get("type") == "image_url" for item in content)
        return False

    def has_images_in_messages(self, messages: List[dict]) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯åˆ—è¡¨ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        return any(self.has_images_in_content(msg.get("content")) for msg in messages)

    def extract_text_from_content(self, content) -> str:
        """ä»å†…å®¹ä¸­æå–æ–‡æœ¬ - æœ€å°åŒ–æ¸…ç†"""
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if item.get("type") == "text":
                    # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼Œä¸è¿‡åº¦æ¸…ç†
                    text = item.get("text", "")
                    text_parts.append(text)
            return " ".join(text_parts)
        else:
            # ç›´æ¥è¿”å›åŸå§‹å†…å®¹ï¼ŒåªåšåŸºæœ¬çš„å­—ç¬¦ä¸²è½¬æ¢
            return str(content) if content else ""

    def extract_images_from_content(self, content) -> List[dict]:
        """ä»å†…å®¹ä¸­æå–å›¾ç‰‡ä¿¡æ¯"""
        if isinstance(content, list):
            images = []
            for item in content:
                if item.get("type") == "image_url":
                    images.append(item)
            return images
        return []

    def extract_multimodal_messages(self, messages: List[dict]) -> Tuple[List[dict], List[dict]]:
        """æå–å¤šæ¨¡æ€æ¶ˆæ¯å’Œçº¯æ–‡æœ¬æ¶ˆæ¯"""
        multimodal_messages = []
        text_messages = []
        
        for msg in messages:
            if self.has_images_in_content(msg.get("content")):
                multimodal_messages.append(msg)
            else:
                text_messages.append(msg)
        
        self.debug_log(2, f"å¤šæ¨¡æ€æ¶ˆæ¯æå–: {len(multimodal_messages)}æ¡å¤šæ¨¡æ€, {len(text_messages)}æ¡æ–‡æœ¬", "ğŸ¨")
        return multimodal_messages, text_messages

    def is_high_priority_content(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜ä¼˜å…ˆçº§å†…å®¹"""
        if not text or not self.high_priority_keywords:
            return False
        
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.high_priority_keywords)

    def calculate_multimodal_budget_sufficient(self, messages: List[dict], target_tokens: int) -> bool:
        """è®¡ç®—å¤šæ¨¡æ€æ¨¡å‹çš„Tokené¢„ç®—æ˜¯å¦å……è¶³"""
        current_tokens = self.count_messages_tokens(messages)
        usage_ratio = current_tokens / target_tokens if target_tokens > 0 else 1.0
        threshold = self.valves.multimodal_direct_threshold
        is_sufficient = usage_ratio <= threshold
        
        self.debug_log(1, f"ğŸ¯ å¤šæ¨¡æ€é¢„ç®—æ£€æŸ¥: {current_tokens:,}/{target_tokens:,} = {usage_ratio:.2%} "
                          f"{'â‰¤' if is_sufficient else '>'} {threshold:.1%}", "ğŸ’°")
        
        return is_sufficient

    # ========== ç»Ÿä¸€çš„APIå®¢æˆ·ç«¯ç®¡ç† ==========
    
    def get_api_client(self, client_type: str = "default"):
        """è·å–APIå®¢æˆ·ç«¯ - ç»Ÿä¸€ç®¡ç†"""
        if not OPENAI_AVAILABLE:
            return None
        
        if self.valves.api_key:
            return AsyncOpenAI(
                base_url=self.valves.api_base,
                api_key=self.valves.api_key,
                timeout=self.valves.request_timeout
            )
        return None

    # ========== å®‰å…¨APIè°ƒç”¨ ==========
    
    def is_json_response(self, content: str) -> bool:
        """æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºJSONæ ¼å¼"""
        if not content:
            return False
        content = content.strip()
        return content.startswith("{") or content.startswith("[")

    def extract_error_info(self, content: str) -> str:
        """ä»é”™è¯¯å“åº”ä¸­æå–å…³é”®ä¿¡æ¯"""
        if not content:
            return "ç©ºå“åº”"
        
        # åŸºæœ¬æ¸…ç†
        content = content.replace("\n", " ").replace("\r", " ")
        content = re.sub(r"\s+", " ", content).strip()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºHTMLé”™è¯¯é¡µé¢
        if content.strip().startswith("<!DOCTYPE") or "<html" in content:
            title_match = re.search(r"<title>(.*?)</title>", content, re.IGNORECASE)
            if title_match:
                return f"HTMLé”™è¯¯é¡µé¢: {title_match.group(1)}"
            return "HTMLé”™è¯¯é¡µé¢"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºJSONé”™è¯¯
        try:
            if self.is_json_response(content):
                error_data = json.loads(content)
                if isinstance(error_data, dict):
                    error_msg = error_data.get("error", error_data.get("message", ""))
                    if error_msg:
                        return f"APIé”™è¯¯: {error_msg}"
            
            return f"å“åº”å†…å®¹: {content[:200]}..."
        except Exception:
            return f"å“åº”å†…å®¹: {content[:200]}..."

    async def safe_api_call(self, call_func, call_name: str, *args, **kwargs):
        """å®‰å…¨çš„APIè°ƒç”¨åŒ…è£…å™¨"""
        for attempt in range(self.valves.api_error_retry_times + 1):
            try:
                result = await call_func(*args, **kwargs)
                return result
            except Exception as e:
                error_msg = str(e)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºJSONè§£æé”™è¯¯
                if "is not valid JSON" in error_msg or "Unexpected token" in error_msg:
                    self.debug_log(1, f"{call_name} JSONè§£æé”™è¯¯: {error_msg}", "âŒ")
                    return None
                
                if attempt < self.valves.api_error_retry_times:
                    self.debug_log(1, f"{call_name} ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{self.valves.api_error_retry_delay}ç§’åé‡è¯•: {error_msg}", "ğŸ”„")
                    await asyncio.sleep(self.valves.api_error_retry_delay)
                else:
                    self.debug_log(1, f"{call_name} æœ€ç»ˆå¤±è´¥: {error_msg}", "âŒ")
                    return None
        
        return None

    # ========== æ™ºèƒ½ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹ ==========
    
    async def _detect_context_max_need_impl(self, query_text: str, event_emitter):
        """å®é™…çš„ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹å®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        # åŸºæœ¬æ¸…ç†æŸ¥è¯¢æ–‡æœ¬
        cleaned_query = query_text.replace("\n", " ").replace("\r", " ")
        cleaned_query = re.sub(r"\s+", " ", cleaned_query).strip()
        
        # æ„å»ºæç¤º
        prompt = f"{self.valves.context_max_detection_prompt}\n\n{cleaned_query}"
        
        response = await client.chat.completions.create(
            model=self.valves.text_model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=50,
            temperature=0.1,
            timeout=self.valves.request_timeout
        )
        
        if response.choices and response.choices[0].message.content:
            result = response.choices[0].message.content.strip()
            # åŸºæœ¬æ¸…ç†ç»“æœ
            result = result.replace("\n", " ").replace("\r", " ")
            result = re.sub(r"\s+", " ", result).strip()
            
            need_context_max = "éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–" in result
            self.debug_log(2, f"AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹ç»“æœ: {result} -> {need_context_max}", "ğŸ§ ")
            return need_context_max
        
        return None

    async def detect_context_max_need(self, query_text: str, event_emitter) -> bool:
        """ä½¿ç”¨AIæ£€æµ‹æ˜¯å¦éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–"""
        if not self.valves.enable_ai_context_max_detection:
            # å›é€€åˆ°ç®€å•çš„æ¨¡å¼åŒ¹é…
            return self.is_context_max_need_simple(query_text)
        
        self.debug_log(1, f"ğŸ§  AIæ£€æµ‹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚: {query_text}", "ğŸ§ ")
        
        # è°ƒç”¨AIæ£€æµ‹
        need_context_max = await self.safe_api_call(
            self._detect_context_max_need_impl, "ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹", query_text, event_emitter
        )
        
        if need_context_max is not None:
            self.stats.context_maximization_detections += 1
            self.debug_log(1, f"ğŸ§  AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹å®Œæˆ: {'éœ€è¦' if need_context_max else 'ä¸éœ€è¦'}", "ğŸ§ ")
            return need_context_max
        else:
            # AIæ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•
            self.debug_log(1, f"ğŸ§  AIæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•", "âš ï¸")
            return self.is_context_max_need_simple(query_text)

    def is_context_max_need_simple(self, query_text: str) -> bool:
        """ç®€å•çš„ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚åˆ¤æ–­ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        if not query_text:
            return True
        
        # éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„ç‰¹å¾
        context_max_patterns = [
            r".*èŠ.*ä»€ä¹ˆ.*",
            r".*è¯´.*ä»€ä¹ˆ.*",
            r".*è®¨è®º.*ä»€ä¹ˆ.*",
            r".*è°ˆ.*ä»€ä¹ˆ.*",
            r".*å†…å®¹.*",
            r".*è¯é¢˜.*",
            r".*å†å².*",
            r".*è®°å½•.*",
            r".*ä¹‹å‰.*",
            r"what.*discuss.*",
            r"what.*talk.*",
            r"what.*chat.*",
            r".*conversation.*",
            r".*history.*",
        ]
        
        query_lower = query_text.lower()
        for pattern in context_max_patterns:
            if re.search(pattern, query_lower):
                return True
        
        return len(query_text.split()) <= 3  # 3ä¸ªè¯ä»¥å†…ä¹Ÿè®¤ä¸ºéœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–

    # ========== æ™ºèƒ½å…³é”®å­—ç”Ÿæˆ ==========
    
    async def _generate_keywords_impl(self, query_text: str, event_emitter):
        """å®é™…çš„å…³é”®å­—ç”Ÿæˆå®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        # åŸºæœ¬æ¸…ç†æŸ¥è¯¢æ–‡æœ¬
        cleaned_query = query_text.replace("\n", " ").replace("\r", " ")
        cleaned_query = re.sub(r"\s+", " ", cleaned_query).strip()
        
        # æ„å»ºæç¤º
        prompt = f"{self.valves.keyword_generation_prompt}\n\n{cleaned_query}"
        
        response = await client.chat.completions.create(
            model=self.valves.text_model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.3,
            timeout=self.valves.request_timeout
        )
        
        if response.choices and response.choices[0].message.content:
            keywords_text = response.choices[0].message.content.strip()
            # åŸºæœ¬æ¸…ç†å…³é”®å­—
            keywords_text = keywords_text.replace("\n", " ").replace("\r", " ")
            keywords_text = re.sub(r"\s+", " ", keywords_text).strip()
            
            # è§£æå…³é”®å­—
            keywords = [kw.strip() for kw in keywords_text.split(",") if kw.strip()]
            # è¿‡æ»¤å¤ªçŸ­çš„å…³é”®å­—
            keywords = [kw for kw in keywords if len(kw) >= 2]
            
            self.debug_log(2, f"ç”Ÿæˆå…³é”®å­—: {keywords[:5]}...", "ğŸ”‘")
            return keywords
        
        return None

    async def generate_search_keywords(self, query_text: str, event_emitter) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®å­—"""
        if not self.valves.enable_keyword_generation:
            return [query_text]
        
        # ä½¿ç”¨AIæ£€æµ‹æ˜¯å¦éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–
        need_context_max = await self.detect_context_max_need(query_text, event_emitter)
        
        # å¦‚æœä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¸”ä¸å¼ºåˆ¶ç”Ÿæˆå…³é”®å­—ï¼Œç›´æ¥è¿”å›åŸæ–‡
        if not need_context_max and not self.valves.keyword_generation_for_context_max:
            self.debug_log(2, f"å…·ä½“æŸ¥è¯¢ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬: {query_text}", "ğŸ”‘")
            return [query_text]
        
        self.debug_log(1, f"ğŸ”‘ ç”Ÿæˆæœç´¢å…³é”®å­—: {query_text}", "ğŸ”‘")
        
        # è°ƒç”¨å…³é”®å­—ç”ŸæˆAPI
        keywords = await self.safe_api_call(
            self._generate_keywords_impl, "å…³é”®å­—ç”Ÿæˆ", query_text, event_emitter
        )
        
        if keywords:
            # æ·»åŠ åŸå§‹æŸ¥è¯¢ä½œä¸ºå¤‡é€‰
            final_keywords = [query_text] + keywords
            # å»é‡
            final_keywords = list(dict.fromkeys(final_keywords))
            
            self.stats.keyword_generations += 1
            self.debug_log(1, f"ğŸ”‘ å…³é”®å­—ç”Ÿæˆå®Œæˆ: {len(final_keywords)}ä¸ª", "ğŸ”‘")
            return final_keywords
        else:
            self.debug_log(1, f"ğŸ”‘ å…³é”®å­—ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢", "âš ï¸")
            return [query_text]

    # ========== å…¬ç”¨çš„æ™ºèƒ½åˆ†ç‰‡åŠŸèƒ½ ==========
    
    async def smart_chunk_and_summarize_messages(
        self, messages: List[dict], target_tokens: int, progress: ProgressTracker, purpose: str = "å¤„ç†"
    ) -> List[dict]:
        """å…¬ç”¨çš„æ™ºèƒ½åˆ†ç‰‡å’Œæ‘˜è¦åŠŸèƒ½ - ä¾›RAGå’Œä¸Šä¸‹æ–‡æœ€å¤§åŒ–å…±ç”¨"""
        if not messages:
            return messages
        
        current_tokens = self.count_messages_tokens(messages)
        self.debug_log(1, f"ğŸ§© å¼€å§‹æ™ºèƒ½åˆ†ç‰‡å’Œæ‘˜è¦({purpose}): {len(messages)}æ¡æ¶ˆæ¯({current_tokens:,}tokens) -> ç›®æ ‡{target_tokens:,}tokens", "ğŸ§©")
        
        # å¦‚æœå½“å‰tokenæ•°é‡å·²ç»åˆé€‚ï¼Œç›´æ¥è¿”å›
        if current_tokens <= target_tokens:
            self.debug_log(1, f"ğŸ§© æ¶ˆæ¯å·²ç¬¦åˆç›®æ ‡å¤§å°ï¼Œæ— éœ€åˆ†ç‰‡", "ğŸ§©")
            return messages
        
        # 1. æ™ºèƒ½åˆ†ç‰‡å¤„ç†
        await progress.update_progress(0, 2, f"æ™ºèƒ½åˆ†ç‰‡å¤„ç†({purpose})")
        chunked_messages = []
        
        if self.valves.enable_smart_chunking:
            # åˆ†ç¦»å¤šæ¨¡æ€å’Œæ–‡æœ¬æ¶ˆæ¯
            multimodal_messages, text_messages = self.extract_multimodal_messages(messages)
            
            # å¤šæ¨¡æ€æ¶ˆæ¯ç›´æ¥ä¿ç•™
            chunked_messages.extend(multimodal_messages)
            
            # æ–‡æœ¬æ¶ˆæ¯è¿›è¡Œåˆ†ç‰‡
            for msg in text_messages:
                msg_tokens = self.count_message_tokens(msg)
                if msg_tokens > self.valves.large_message_threshold:
                    # å¤§æ¶ˆæ¯éœ€è¦åˆ†ç‰‡
                    text_content = self.extract_text_from_content(msg.get("content", ""))
                    
                    # æ™ºèƒ½åˆ†ç‰‡
                    chunks = self._split_text_smart(text_content, self.valves.chunk_target_tokens)
                    
                    # åˆ›å»ºåˆ†ç‰‡æ¶ˆæ¯
                    for i, chunk in enumerate(chunks):
                        chunk_msg = copy.deepcopy(msg)
                        chunk_msg["content"] = chunk
                        chunk_msg["_chunk_id"] = f"chunk_{i}"
                        chunk_msg["_is_chunk"] = True
                        chunk_msg["_original_message_id"] = msg.get("_order_id")
                        chunked_messages.append(chunk_msg)
                    
                    self.debug_log(2, f"ğŸ§© å¤§æ¶ˆæ¯åˆ†ç‰‡: {msg_tokens}tokens -> {len(chunks)}ä¸ªåˆ†ç‰‡", "ğŸ§©")
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.stats.chunk_created += len(chunks)
                    self.stats.chunked_messages += 1
                else:
                    # å°æ¶ˆæ¯ç›´æ¥ä¿ç•™
                    chunked_messages.append(msg)
        else:
            # ä¸å¯ç”¨åˆ†ç‰‡ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ¶ˆæ¯
            chunked_messages = messages
        
        # 2. é€’å½’æ‘˜è¦å¤„ç†
        await progress.update_progress(1, 2, f"é€’å½’æ‘˜è¦å¤„ç†({purpose})")
        if self.valves.enable_recursive_summarization:
            chunked_tokens = self.count_messages_tokens(chunked_messages)
            if chunked_tokens > target_tokens:
                self.debug_log(1, f"ğŸ”„ å¼€å§‹é€’å½’æ‘˜è¦: {len(chunked_messages)}æ¡æ¶ˆæ¯({chunked_tokens:,}tokens) -> ç›®æ ‡{target_tokens:,}tokens", "ğŸ”„")
                
                summarized_messages = await self.recursive_summarize_messages(
                    chunked_messages, target_tokens, progress
                )
                
                self.debug_log(1, f"ğŸ”„ é€’å½’æ‘˜è¦å®Œæˆ: {len(chunked_messages)} -> {len(summarized_messages)}æ¡", "ğŸ”„")
                return summarized_messages
        
        return chunked_messages

    def _split_text_smart(self, text: str, target_tokens: int) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†ç‰‡"""
        if not text:
            return []
        
        # è®¡ç®—æ–‡æœ¬tokens
        text_tokens = self.count_tokens(text)
        
        # å¦‚æœæ–‡æœ¬å·²ç»è¶³å¤Ÿå°ï¼Œç›´æ¥è¿”å›
        if text_tokens <= target_tokens:
            return [text]
        
        # æ™ºèƒ½åˆ†ç‰‡
        chunks = []
        
        # é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split("\n\n")
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph_tokens = self.count_tokens(paragraph)
            current_tokens = self.count_tokens(current_chunk)
            
            if current_tokens + paragraph_tokens <= target_tokens:
                # å¯ä»¥æ·»åŠ åˆ°å½“å‰åˆ†ç‰‡
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                # å½“å‰åˆ†ç‰‡å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°åˆ†ç‰‡
                if current_chunk:
                    chunks.append(current_chunk)
                
                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡ç›®æ ‡å¤§å°ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                if paragraph_tokens > target_tokens:
                    sub_chunks = self._split_paragraph(paragraph, target_tokens)
                    chunks.extend(sub_chunks[:-1])  # é™¤äº†æœ€åä¸€ä¸ª
                    current_chunk = sub_chunks[-1] if sub_chunks else ""
                else:
                    current_chunk = paragraph
        
        # æ·»åŠ æœ€åä¸€ä¸ªåˆ†ç‰‡
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks if chunks else [text]

    def _split_paragraph(self, paragraph: str, target_tokens: int) -> List[str]:
        """åˆ†å‰²é•¿æ®µè½"""
        sentences = re.split(r"[.!?ã€‚ï¼ï¼Ÿ]+\s*", paragraph)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if not sentence.strip():
                continue
            
            sentence_tokens = self.count_tokens(sentence)
            current_tokens = self.count_tokens(current_chunk)
            
            if current_tokens + sentence_tokens <= target_tokens:
                if current_chunk:
                    current_chunk += "ã€‚" + sentence
                else:
                    current_chunk = sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks if chunks else [paragraph]

    # ========== é€’å½’æ‘˜è¦å¤„ç† ==========
    
    async def _summarize_messages_impl(self, messages_text: str, summary_target: int, event_emitter, has_multimodal: bool = False):
        """å®é™…çš„æ¶ˆæ¯æ‘˜è¦å®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        # åŸºæœ¬æ¸…ç†æ–‡æœ¬
        cleaned_text = messages_text.replace("\n", " ").replace("\r", " ")
        cleaned_text = re.sub(r"\s+", " ", cleaned_text).strip()
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹å¯¹è¯å†…å®¹è¿›è¡Œæ™ºèƒ½æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€é‡è¦ç»†èŠ‚å’Œæ ¸å¿ƒå†…å®¹ã€‚æ‘˜è¦åº”è¯¥ï¼š
1. ä¿æŒé€»è¾‘è¿è´¯æ€§å’Œæ—¶é—´é¡ºåº
2. é‡ç‚¹ä¿ç•™æŠ€æœ¯ç»†èŠ‚ã€ä»£ç ã€é…ç½®ã€æ•°æ®ç­‰å…³é”®ä¿¡æ¯
3. ä¿ç•™ç”¨æˆ·é—®é¢˜å’Œé‡è¦å›ç­”
4. ä¿æŒåŸæ–‡çš„ä¸“ä¸šæœ¯è¯­å’Œå…³é”®è¯
5. æ§åˆ¶é•¿åº¦åœ¨{summary_target}å­—ç¬¦ä»¥å†…

å¯¹è¯å†…å®¹ï¼š
{cleaned_text}

è¯·ç”Ÿæˆæ‘˜è¦ï¼š"""
        
        # æ ¹æ®æ˜¯å¦æœ‰å¤šæ¨¡æ€å†…å®¹é€‰æ‹©æ¨¡å‹
        model_to_use = self.valves.multimodal_model if has_multimodal else self.valves.text_model
        
        response = await client.chat.completions.create(
            model=model_to_use,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=min(8000, summary_target // 2),
            temperature=0.3,
            timeout=self.valves.request_timeout
        )
        
        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            # åŸºæœ¬æ¸…ç†æ‘˜è¦
            summary = summary.replace("\n", " ").replace("\r", " ")
            summary = re.sub(r"\s+", " ", summary).strip()
            return summary
        
        return None

    async def recursive_summarize_messages(
        self, messages: List[dict], target_tokens: int, progress: ProgressTracker, depth: int = 0
    ) -> List[dict]:
        """é€’å½’æ‘˜è¦å¤„ç†è¶…å¤§æ¶ˆæ¯é›†åˆ"""
        if not self.valves.enable_recursive_summarization or depth >= self.valves.max_recursion_depth:
            return messages
        
        current_tokens = self.count_messages_tokens(messages)
        
        # å¦‚æœå½“å‰tokenæ•°é‡åˆé€‚ï¼Œç›´æ¥è¿”å›
        if current_tokens <= target_tokens:
            return messages
        
        self.debug_log(1, f"ğŸ”„ å¼€å§‹é€’å½’æ‘˜è¦(æ·±åº¦{depth+1}): {current_tokens:,} -> {target_tokens:,}tokens", "ğŸ”„")
        
        # åˆ†æ‰¹å¤„ç†æ¶ˆæ¯
        batch_size = max(5, len(messages) // 4)  # æ¯æ‰¹è‡³å°‘5æ¡æ¶ˆæ¯
        batches = [messages[i:i + batch_size] for i in range(0, len(messages), batch_size)]
        
        summarized_messages = []
        
        for i, batch in enumerate(batches):
            # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦åŒ…å«å¤šæ¨¡æ€å†…å®¹
            has_multimodal = any(self.has_images_in_content(msg.get("content", "")) for msg in batch)
            
            # åˆå¹¶æ‰¹æ¬¡æ¶ˆæ¯ä¸ºæ–‡æœ¬
            batch_text = ""
            for msg in batch:
                role = msg.get("role", "")
                content = self.extract_text_from_content(msg.get("content", ""))
                batch_text += f"[{role}] {content}\n\n"
            
            # è®¡ç®—æ‘˜è¦ç›®æ ‡é•¿åº¦
            batch_tokens = self.count_tokens(batch_text)
            summary_target = int(batch_tokens * self.valves.summary_compression_ratio)
            
            # è°ƒç”¨æ‘˜è¦API
            summary = await self.safe_api_call(
                self._summarize_messages_impl, "æ¶ˆæ¯æ‘˜è¦", 
                batch_text, summary_target, progress.event_emitter, has_multimodal
            )
            
            if summary:
                # åˆ›å»ºæ‘˜è¦æ¶ˆæ¯
                summary_msg = {
                    "role": "assistant",
                    "content": f"[æ‘˜è¦] {summary}",
                    "_is_summary": True,
                    "_original_count": len(batch),
                    "_summary_depth": depth + 1
                }
                summarized_messages.append(summary_msg)
                
                self.debug_log(2, f"ğŸ”„ æ‰¹æ¬¡æ‘˜è¦å®Œæˆ: {len(batch)}æ¡ -> 1æ¡æ‘˜è¦({self.count_message_tokens(summary_msg)}tokens)", "ğŸ”„")
            else:
                # æ‘˜è¦å¤±è´¥ï¼Œä¿ç•™åŸå§‹æ¶ˆæ¯ï¼ˆä½†å¯èƒ½éœ€è¦æˆªæ–­ï¼‰
                summarized_messages.extend(batch[:3])  # åªä¿ç•™å‰3æ¡
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.recursive_summaries += 1
        self.stats.summarized_messages += len(messages) - len(summarized_messages)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥é€’å½’
        final_tokens = self.count_messages_tokens(summarized_messages)
        if final_tokens > target_tokens and depth < self.valves.max_recursion_depth - 1:
            summarized_messages = await self.recursive_summarize_messages(
                summarized_messages, target_tokens, progress, depth + 1
            )
        
        self.debug_log(1, f"ğŸ”„ é€’å½’æ‘˜è¦å®Œæˆ(æ·±åº¦{depth+1}): {len(messages)}æ¡ -> {len(summarized_messages)}æ¡", "âœ…")
        return summarized_messages

    # ========== å®¹é”™ä¿æŠ¤æœºåˆ¶ ==========
    
    def apply_fallback_preservation(self, history_messages: List[dict], available_tokens: int) -> List[dict]:
        """åº”ç”¨å®¹é”™ä¿æŠ¤æœºåˆ¶ï¼Œç¡®ä¿å³ä½¿RAGæœç´¢æ— ç»“æœä¹Ÿæœ‰è¶³å¤Ÿçš„å†å²ä¸Šä¸‹æ–‡"""
        if not self.valves.enable_fallback_preservation or not history_messages:
            return history_messages
        
        # è®¡ç®—å®¹é”™ä¿æŠ¤é¢„ç®—
        fallback_budget = int(available_tokens * self.valves.fallback_preserve_ratio)
        
        self.debug_log(1, f"ğŸ›¡ï¸ åº”ç”¨å®¹é”™ä¿æŠ¤æœºåˆ¶: é¢„ç®—{fallback_budget:,}tokens", "ğŸ›¡ï¸")
        
        # ä»æœ€è¿‘çš„æ¶ˆæ¯å¼€å§‹ä¿ç•™
        fallback_messages = []
        used_tokens = 0
        
        # ç¡®ä¿è‡³å°‘ä¿ç•™å‡ æ¡æœ€è¿‘çš„ç”¨æˆ·-åŠ©æ‰‹å¯¹è¯
        user_exchange_count = 0
        target_exchanges = self.valves.force_preserve_recent_user_exchanges
        
        for msg in reversed(history_messages):
            msg_tokens = self.count_message_tokens(msg)
            
            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºé¢„ç®—
            if used_tokens + msg_tokens > fallback_budget:
                # å¦‚æœè¿˜æ²¡æœ‰è¾¾åˆ°æœ€å°è¦æ±‚ï¼Œå¼ºåˆ¶ä¿ç•™
                if (len(fallback_messages) < self.valves.min_history_messages or 
                    user_exchange_count < target_exchanges):
                    # å¼ºåˆ¶ä¿ç•™ï¼Œä½†æˆªæ–­è¿‡é•¿çš„æ¶ˆæ¯
                    if msg_tokens > fallback_budget // 4:
                        # æ¶ˆæ¯å¤ªé•¿ï¼Œæˆªæ–­
                        content = self.extract_text_from_content(msg.get("content", ""))
                        if content:
                            # ä¿ç•™å‰åŠéƒ¨åˆ†
                            truncated_content = content[:len(content)//2] + "...[æˆªæ–­]"
                            truncated_msg = copy.deepcopy(msg)
                            truncated_msg["content"] = truncated_content
                            truncated_msg["_is_truncated"] = True
                            fallback_messages.insert(0, truncated_msg)
                            used_tokens += self.count_message_tokens(truncated_msg)
                    else:
                        fallback_messages.insert(0, msg)
                        used_tokens += msg_tokens
                else:
                    break
            else:
                fallback_messages.insert(0, msg)
                used_tokens += msg_tokens
            
            # ç»Ÿè®¡ç”¨æˆ·å¯¹è¯è½®æ¬¡
            if msg.get("role") == "user":
                user_exchange_count += 1
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.fallback_preserve_applied += 1
        
        self.debug_log(1, f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤å®Œæˆ: ä¿ç•™{len(fallback_messages)}æ¡æ¶ˆæ¯({used_tokens:,}tokens), "
                          f"{user_exchange_count}ä¸ªç”¨æˆ·å¯¹è¯è½®æ¬¡", "ğŸ›¡ï¸")
        
        return fallback_messages

    def ensure_current_user_message_preserved(self, final_messages: List[dict]) -> List[dict]:
        """ç¡®ä¿å½“å‰ç”¨æˆ·æ¶ˆæ¯è¢«æ­£ç¡®ä¿ç•™åœ¨æœ€åä½ç½®"""
        if not self.current_user_message:
            return final_messages
        
        # æ£€æŸ¥å½“å‰ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦åœ¨æœ€åä½ç½®
        if final_messages and final_messages[-1].get("role") == "user":
            current_id = self.current_user_message.get("_order_id")
            last_id = final_messages[-1].get("_order_id")
            
            if current_id == last_id:
                # å½“å‰ç”¨æˆ·æ¶ˆæ¯å·²ç»åœ¨æ­£ç¡®ä½ç½®
                return final_messages
        
        # å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¸åœ¨æœ€åä½ç½®ï¼Œéœ€è¦ä¿®å¤
        self.debug_log(1, "ğŸ›¡ï¸ æ£€æµ‹åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½ç½®é”™è¯¯ï¼Œå¼€å§‹ä¿®å¤", "ğŸ›¡ï¸")
        
        # ç§»é™¤æ‰€æœ‰å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„å‰¯æœ¬
        current_id = self.current_user_message.get("_order_id")
        filtered_messages = []
        
        for msg in final_messages:
            if msg.get("_order_id") != current_id:
                filtered_messages.append(msg)
        
        # å°†å½“å‰ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°æœ€å
        filtered_messages.append(self.current_user_message)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.user_message_recovery_count += 1
        
        self.debug_log(1, "ğŸ›¡ï¸ å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½ç½®ä¿®å¤å®Œæˆ", "ğŸ›¡ï¸")
        return filtered_messages

    # ========== ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¸“ç”¨å¤„ç†ç­–ç•¥ - ä¿®å¤å¤„ç†æµç¨‹ ==========
    
    async def process_context_maximization(
        self, history_messages: List[dict], available_tokens: int, progress: ProgressTracker, need_context_max: bool = True
    ) -> List[dict]:
        """ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†ç­–ç•¥ - ä¿®å¤å¤„ç†æµç¨‹ï¼Œå¢å¼ºå®¹é”™æœºåˆ¶"""
        if not self.valves.enable_context_maximization or not need_context_max:
            return history_messages
        
        await progress.start_phase("ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†", len(history_messages))
        
        self.debug_log(1, f"ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†: {len(history_messages)}æ¡æ¶ˆæ¯, å¯ç”¨é¢„ç®—: {available_tokens:,}tokens", "ğŸ“š")
        
        # 1. å…ˆæå–å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œä¿ç•™ç»™å¤šæ¨¡æ€æ¨¡å‹å¤„ç†
        multimodal_messages, text_messages = self.extract_multimodal_messages(history_messages)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.multimodal_extracted = len(multimodal_messages)
        
        self.debug_log(2, f"ğŸ“š æ¶ˆæ¯åˆ†ç±»: {len(multimodal_messages)}æ¡å¤šæ¨¡æ€, {len(text_messages)}æ¡æ–‡æœ¬", "ğŸ“š")
        
        # 2. è®¡ç®—é¢„ç®—åˆ†é…
        direct_preserve_budget = int(available_tokens * self.valves.context_max_direct_preserve_ratio)
        # ä¸ºå®¹é”™ä¿æŠ¤é¢„ç•™é¢„ç®—
        fallback_budget = int(available_tokens * self.valves.fallback_preserve_ratio)
        processing_budget = available_tokens - direct_preserve_budget - fallback_budget
        
        self.debug_log(1, f"ğŸ’° é¢„ç®—åˆ†é…: ç›´æ¥ä¿ç•™ {direct_preserve_budget:,}tokens, "
                          f"å¤„ç†å‰©ä½™ {processing_budget:,}tokens, å®¹é”™ä¿æŠ¤ {fallback_budget:,}tokens", "ğŸ’°")
        
        # 3. ä¼˜å…ˆä¿ç•™å¤šæ¨¡æ€æ¶ˆæ¯å’Œæœ€è¿‘çš„æ–‡æœ¬æ¶ˆæ¯
        preserved_messages = []
        used_tokens = 0
        
        # å…ˆä¿ç•™å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆä¼˜å…ˆæœ€æ–°çš„ï¼‰
        for msg in reversed(multimodal_messages):
            msg_tokens = self.count_message_tokens(msg)
            if used_tokens + msg_tokens <= direct_preserve_budget:
                preserved_messages.insert(0, msg)
                used_tokens += msg_tokens
                self.debug_log(3, f"ğŸ“š ä¿ç•™å¤šæ¨¡æ€æ¶ˆæ¯: {msg_tokens}tokens, ID: {msg.get('_order_id', 'None')[:8]}", "ğŸ“š")
            else:
                break
        
        # å†ä¿ç•™æœ€è¿‘çš„æ–‡æœ¬æ¶ˆæ¯
        for msg in reversed(text_messages):
            msg_tokens = self.count_message_tokens(msg)
            if used_tokens + msg_tokens <= direct_preserve_budget:
                preserved_messages.insert(0, msg)
                used_tokens += msg_tokens
                self.debug_log(3, f"ğŸ“š ä¿ç•™æ–‡æœ¬æ¶ˆæ¯: {msg_tokens}tokens, ID: {msg.get('_order_id', 'None')[:8]}", "ğŸ“š")
            else:
                break
        
        self.stats.context_max_direct_preserve = len(preserved_messages)
        self.debug_log(1, f"ğŸ“š ç›´æ¥ä¿ç•™å®Œæˆ: {len(preserved_messages)}æ¡æ¶ˆæ¯({used_tokens:,}tokens)", "ğŸ“š")
        
        # 4. å¤„ç†å‰©ä½™æ¶ˆæ¯ - ä¿®å¤ï¼šç¡®ä¿å¼ºåˆ¶å¤„ç†æ‰€æœ‰å‰©ä½™æ¶ˆæ¯
        preserved_ids = {msg.get("_order_id") for msg in preserved_messages if msg.get("_order_id")}
        self.debug_log(2, f"ğŸ“š å·²ä¿ç•™æ¶ˆæ¯IDs: {[msg_id[:8] for msg_id in preserved_ids]}", "ğŸ“š")
        
        remaining_messages = []
        for msg in history_messages:
            msg_id = msg.get("_order_id")
            if msg_id not in preserved_ids:
                remaining_messages.append(msg)
            else:
                self.debug_log(3, f"ğŸ“š è·³è¿‡å·²ä¿ç•™æ¶ˆæ¯: ID {msg_id[:8] if msg_id else 'None'}", "ğŸ“š")
        
        self.debug_log(1, f"ğŸ“š å‰©ä½™æ¶ˆæ¯: {len(remaining_messages)}æ¡ï¼Œå¤„ç†é¢„ç®—: {processing_budget:,}tokens", "ğŸ“š")
        
        processed_remaining = []
        
        # å¼ºåˆ¶å¤„ç†å‰©ä½™æ¶ˆæ¯ï¼Œç¡®ä¿æœ€å¤§åŒ–ä¿ç•™ä¸Šä¸‹æ–‡
        if remaining_messages and processing_budget > 5000:
            self.debug_log(1, f"ğŸ“š å¼€å§‹å¤„ç†å‰©ä½™æ¶ˆæ¯: {len(remaining_messages)}æ¡ ({self.count_messages_tokens(remaining_messages):,}tokens)", "ğŸ“š")
            
            # ä½¿ç”¨å…¬ç”¨çš„åˆ†ç‰‡æ‘˜è¦åŠŸèƒ½
            processed_remaining = await self.smart_chunk_and_summarize_messages(
                remaining_messages, processing_budget, progress, "ä¸Šä¸‹æ–‡æœ€å¤§åŒ–"
            )
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats.context_max_chunked = self.stats.chunk_created
            self.stats.context_max_summarized = self.stats.summarized_messages
            
            self.debug_log(1, f"ğŸ“š å‰©ä½™æ¶ˆæ¯å¤„ç†å®Œæˆ: {len(remaining_messages)} -> {len(processed_remaining)}æ¡ "
                              f"({self.count_messages_tokens(processed_remaining):,}tokens)", "ğŸ“š")
        else:
            self.debug_log(1, f"ğŸ“š å¤„ç†é¢„ç®—ä¸è¶³æˆ–æ— å‰©ä½™æ¶ˆæ¯ï¼Œè·³è¿‡å¤„ç†", "ğŸ“š")
        
        # 5. åˆå¹¶ç»“æœ
        preliminary_messages = preserved_messages + processed_remaining
        
        # 6. åº”ç”¨å®¹é”™ä¿æŠ¤æœºåˆ¶ - å¦‚æœå¤„ç†ç»“æœå¤ªå°‘ï¼Œä½¿ç”¨å®¹é”™ä¿æŠ¤
        if len(preliminary_messages) < self.valves.min_history_messages:
            self.debug_log(1, f"ğŸ›¡ï¸ å¤„ç†ç»“æœè¿‡å°‘({len(preliminary_messages)}æ¡)ï¼Œåº”ç”¨å®¹é”™ä¿æŠ¤", "ğŸ›¡ï¸")
            
            # ä½¿ç”¨å®¹é”™ä¿æŠ¤æœºåˆ¶è¡¥å……æ¶ˆæ¯
            fallback_messages = self.apply_fallback_preservation(
                remaining_messages, fallback_budget
            )
            
            # åˆå¹¶å»é‡
            all_message_ids = {msg.get("_order_id") for msg in preliminary_messages if msg.get("_order_id")}
            for msg in fallback_messages:
                if msg.get("_order_id") not in all_message_ids:
                    preliminary_messages.append(msg)
                    all_message_ids.add(msg.get("_order_id"))
        
        # 7. ç¡®ä¿æ¶ˆæ¯é¡ºåºæ­£ç¡®
        final_messages = preliminary_messages
        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )
        
        final_tokens = self.count_messages_tokens(final_messages)
        
        self.debug_log(1, f"ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†å®Œæˆ: {len(history_messages)} -> {len(final_messages)}æ¡æ¶ˆæ¯({final_tokens:,}tokens)", "âœ…")
        
        await progress.complete_phase(f"å¤„ç†å®Œæˆ {len(final_messages)}æ¡æ¶ˆæ¯({final_tokens:,}tokens)")
        
        return final_messages

    # ========== å°½é‡ä¿ç•™æœºåˆ¶ ==========
    
    async def try_preserve_recent_messages(
        self, history_messages: List[dict], available_tokens: int, progress: ProgressTracker, need_context_max: bool = False
    ) -> Tuple[List[dict], int]:
        """å°½é‡ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯"""
        if not self.valves.enable_try_preserve or not history_messages:
            return [], available_tokens
        
        # å¯¹äºä¸Šä¸‹æ–‡æœ€å¤§åŒ–ï¼Œè°ƒæ•´ä¿ç•™æ¯”ä¾‹
        if need_context_max and self.valves.enable_context_maximization:
            # ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ—¶ï¼Œå‡å°‘å°½é‡ä¿ç•™çš„æ¯”ä¾‹ï¼Œä¸ºåç»­ç›´æ¥ä¿ç•™ç•™å‡ºç©ºé—´
            try_preserve_ratio = min(self.valves.try_preserve_ratio, 0.25)
        else:
            try_preserve_ratio = self.valves.try_preserve_ratio
        
        # è®¡ç®—å°½é‡ä¿ç•™çš„é¢„ç®—
        try_preserve_budget = int(available_tokens * try_preserve_ratio)
        
        await progress.start_phase("å°½é‡ä¿ç•™æœºåˆ¶", len(history_messages))
        
        self.debug_log(1, f"ğŸ”’ å°½é‡ä¿ç•™é¢„ç®—: {try_preserve_budget:,}tokens ({try_preserve_ratio:.1%}) "
                          f"{'[ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¼˜åŒ–]' if need_context_max else ''}", "ğŸ”’")
        
        preserved_messages = []
        used_tokens = 0
        
        # ä»åå¾€å‰éå†å†å²æ¶ˆæ¯ï¼Œå¯»æ‰¾å®Œæ•´çš„å¯¹è¯è½®æ¬¡
        i = len(history_messages) - 1
        preserved_exchanges = 0
        
        while i >= 0 and preserved_exchanges < self.valves.try_preserve_exchanges:
            msg = history_messages[i]
            msg_tokens = self.count_message_tokens(msg)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¼€å§‹ä¸€ä¸ªæ–°çš„äº¤æ¢ï¼‰
            if msg.get("role") == "user":
                # æŸ¥æ‰¾è¿™ä¸ªç”¨æˆ·æ¶ˆæ¯å¯¹åº”çš„åŠ©æ‰‹å›å¤
                user_msg = msg
                assistant_msg = None
                
                # æŸ¥æ‰¾ä¸‹ä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯
                if i + 1 < len(history_messages) and history_messages[i + 1].get("role") == "assistant":
                    assistant_msg = history_messages[i + 1]
                
                # è®¡ç®—è¿™ä¸ªå¯¹è¯è½®æ¬¡çš„æ€»tokenæ•°
                exchange_tokens = msg_tokens
                if assistant_msg:
                    exchange_tokens += self.count_message_tokens(assistant_msg)
                
                # æ£€æŸ¥æ˜¯å¦èƒ½å®Œæ•´ä¿ç•™è¿™ä¸ªå¯¹è¯è½®æ¬¡
                if used_tokens + exchange_tokens <= try_preserve_budget:
                    # å®Œæ•´ä¿ç•™
                    preserved_messages.insert(0, user_msg)
                    used_tokens += msg_tokens
                    
                    if assistant_msg:
                        preserved_messages.insert(1, assistant_msg)
                        used_tokens += self.count_message_tokens(assistant_msg)
                    
                    preserved_exchanges += 1
                    
                    self.debug_log(2, f"ğŸ”’ å®Œæ•´ä¿ç•™å¯¹è¯è½®æ¬¡{preserved_exchanges}: {exchange_tokens}tokens", "ğŸ”’")
                    
                    # è·³è¿‡å·²å¤„ç†çš„åŠ©æ‰‹æ¶ˆæ¯
                    if assistant_msg:
                        i -= 1
                else:
                    # æ— æ³•å®Œæ•´ä¿ç•™ï¼Œè·³å‡ºå¾ªç¯
                    break
            else:
                # ä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼Œå•ç‹¬å¤„ç†
                if used_tokens + msg_tokens <= try_preserve_budget:
                    preserved_messages.insert(0, msg)
                    used_tokens += msg_tokens
                    self.debug_log(3, f"ğŸ”’ å•ç‹¬ä¿ç•™æ¶ˆæ¯: {msg_tokens}tokens", "ğŸ”’")
                else:
                    # æ— æ³•ä¿ç•™ï¼Œè·³å‡ºå¾ªç¯
                    break
            
            i -= 1
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.try_preserve_messages = len(preserved_messages)
        self.stats.try_preserve_tokens = used_tokens
        
        # è®¡ç®—å‰©ä½™é¢„ç®—
        remaining_budget = available_tokens - used_tokens
        
        self.debug_log(1, f"ğŸ”’ å°½é‡ä¿ç•™å®Œæˆ: {len(preserved_messages)}æ¡æ¶ˆæ¯({used_tokens:,}tokens), "
                          f"å‰©ä½™é¢„ç®—: {remaining_budget:,}tokens", "ğŸ”’")
        
        await progress.complete_phase(f"ä¿ç•™{len(preserved_messages)}æ¡æ¶ˆæ¯({used_tokens:,}tokens)")
        
        return preserved_messages, remaining_budget

    # ========== è§†è§‰å¤„ç† ==========
    
    async def _describe_image_impl(self, image_, event_emitter):
        """å®é™…çš„å›¾ç‰‡æè¿°å®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        # ç¡®ä¿å›¾ç‰‡æ•°æ®æ˜¯base64æ ¼å¼
        if not image_data.startswith("data:"):
            self.debug_log(1, "å›¾ç‰‡æ•°æ®ä¸æ˜¯base64æ ¼å¼", "âš ï¸")
            return None
        
        response = await client.chat.completions.create(
            model=self.valves.multimodal_model,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": self.valves.vision_prompt_template},
                    {"type": "image_url", "image_url": {"url": image_data}}
                ]
            }],
            max_tokens=self.valves.vision_max_tokens,
            temperature=0.2
        )
        
        if response.choices:
            description = response.choices[0].message.content.strip()
            # åŸºæœ¬æ¸…ç†æè¿°
            description = description.replace("\n", " ").replace("\r", " ")
            description = re.sub(r"\s+", " ", description).strip()
            return description
        
        return None

    async def describe_image(self, image_data: str, event_emitter) -> str:
        """æè¿°å•å¼ å›¾ç‰‡"""
        image_hash = hashlib.md5(image_data.encode()).hexdigest()
        self.debug_log(2, f"å¼€å§‹è¯†åˆ«å›¾ç‰‡: {image_hash[:8]}", "ğŸ‘ï¸")
        
        description = await self.safe_api_call(
            self._describe_image_impl, "å›¾ç‰‡è¯†åˆ«", image_data, event_emitter
        )
        
        if description:
            # æé«˜æè¿°é•¿åº¦é™åˆ¶
            if len(description) > 2500:
                description = description[:2500] + "..."
            self.debug_log(2, f"å›¾ç‰‡è¯†åˆ«å®Œæˆ: {len(description)}å­—ç¬¦", "âœ…")
            return description
        
        return "å›¾ç‰‡å¤„ç†å¤±è´¥ï¼šæ— æ³•è·å–æè¿°"

    async def process_message_images(self, message: dict, progress: ProgressTracker) -> dict:
        """å¤„ç†å•æ¡æ¶ˆæ¯ä¸­çš„å›¾ç‰‡"""
        content = message.get("content", "")
        if not isinstance(content, list):
            return message
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        images = [item for item in content if item.get("type") == "image_url"]
        if not images:
            return message
        
        # å¤„ç†å›¾ç‰‡
        processed_content = []
        image_count = 0
        
        for item in content:
            if item.get("type") == "text":
                # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬
                text = item.get("text", "")
                processed_content.append(text)
            elif item.get("type") == "image_url":
                image_count += 1
                image_data = item.get("image_url", {}).get("url", "")
                if image_data:
                    if progress:
                        await progress.update_progress(
                            image_count, len(images), f"å¤„ç†å›¾ç‰‡ {image_count}/{len(images)}"
                        )
                    description = await self.describe_image(
                        image_data, progress.event_emitter if progress else None
                    )
                    processed_content.append(f"[å›¾ç‰‡{image_count}æè¿°] {description}")
        
        # åˆ›å»ºæ–°æ¶ˆæ¯ - ä¿æŒåŸå§‹é¡ºåºä¿¡æ¯
        processed_message = copy.deepcopy(message)
        processed_message["content"] = "\n".join(processed_content) if processed_content else ""
        
        self.debug_log(2, f"æ¶ˆæ¯å›¾ç‰‡å¤„ç†å®Œæˆ: {image_count}å¼ å›¾ç‰‡", "ğŸ–¼ï¸")
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.multimodal_processed += image_count
        
        return processed_message

    # ========== å‘é‡åŒ–å¤„ç† ==========
    
    async def _get_text_embedding_impl(self, text: str, event_emitter):
        """å®é™…çš„æ–‡æœ¬å‘é‡è·å–å®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        # åŸºæœ¬æ¸…ç†æ–‡æœ¬
        cleaned_text = text.replace("\n", " ").replace("\r", " ")
        cleaned_text = re.sub(r"\s+", " ", cleaned_text).strip()
        
        response = await client.embeddings.create(
            model=self.valves.text_vector_model,
            input=[cleaned_text[:8000]],
            encoding_format="float"
        )
        
        if response and response.data and len(response.data) > 0 and response.data[0].embedding:
            return response.data[0].embedding
        
        return None

    async def get_text_embedding(self, text: str, event_emitter) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬å‘é‡"""
        if not text:
            return None
        
        # åŸºæœ¬æ¸…ç†æ–‡æœ¬
        cleaned_text = text.replace("\n", " ").replace("\r", " ")
        cleaned_text = re.sub(r"\s+", " ", cleaned_text).strip()
        
        embedding = await self.safe_api_call(
            self._get_text_embedding_impl, "æ–‡æœ¬å‘é‡", cleaned_text, event_emitter
        )
        
        if embedding:
            self.debug_log(3, f"æ–‡æœ¬å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ“")
        
        return embedding

    async def _get_multimodal_embedding_impl(self, content, event_emitter):
        """å®é™…çš„å¤šæ¨¡æ€å‘é‡è·å–å®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        # å¤„ç†è¾“å…¥æ ¼å¼ï¼Œç¡®ä¿ç¬¦åˆAPIè¦æ±‚
        if isinstance(content, list):
            # å·²ç»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œæ¸…ç†æ–‡æœ¬å†…å®¹
            cleaned_content = []
            for item in content:
                if item.get("type") == "text":
                    cleaned_item = item.copy()
                    # åŸºæœ¬æ¸…ç†æ–‡æœ¬
                    text = item.get("text", "")
                    cleaned_text = text.replace("\n", " ").replace("\r", " ")
                    cleaned_text = re.sub(r"\s+", " ", cleaned_text).strip()
                    cleaned_item["text"] = cleaned_text
                    cleaned_content.append(cleaned_item)
                elif item.get("type") == "image_url":
                    # å›¾ç‰‡å†…å®¹ä¿æŒä¸å˜ï¼Œbase64ç¼–ç 
                    cleaned_content.append(item)
                else:
                    # å…¶ä»–ç±»å‹ä¹Ÿä¿æŒä¸å˜
                    cleaned_content.append(item)
            input_data = cleaned_content
        else:
            # çº¯æ–‡æœ¬å†…å®¹ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            text = str(content)
            cleaned_text = text.replace("\n", " ").replace("\r", " ")
            cleaned_text = re.sub(r"\s+", " ", cleaned_text).strip()
            input_data = [{"type": "text", "text": cleaned_text[:8000]}]
        
        self.debug_log(3, f"å¤šæ¨¡æ€å‘é‡è¾“å…¥: {len(input_data)}ä¸ªå…ƒç´ ", "ğŸ–¼ï¸")
        
        # ä½¿ç”¨æ­£ç¡®çš„APIè°ƒç”¨æ–¹å¼
        try:
            response = await client.embeddings.create(
                model=self.valves.multimodal_vector_model,
                input=input_data,
                encoding_format="float"
            )
            
            # å¤„ç†å“åº”
            if hasattr(response, "data") and hasattr(response.data, "embedding"):
                return response.data.embedding
            elif hasattr(response, "data") and isinstance(response.data, list) and len(response.data) > 0:
                return response.data[0].embedding
            else:
                self.debug_log(1, f"å¤šæ¨¡æ€å‘é‡å“åº”æ ¼å¼å¼‚å¸¸: {type(response.data)}", "âš ï¸")
                return None
        except Exception as e:
            self.debug_log(1, f"å¤šæ¨¡æ€å‘é‡è°ƒç”¨å¤±è´¥: {e}", "âŒ")
            raise

    async def get_multimodal_embedding(self, content, event_emitter) -> Optional[List[float]]:
        """è·å–å¤šæ¨¡æ€å‘é‡"""
        if not content:
            return None
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ¨¡æ€å†…å®¹
        has_multimodal_content = False
        if isinstance(content, list):
            has_multimodal_content = any(item.get("type") in ["image_url", "video_url"] for item in content)
        
        if not has_multimodal_content:
            self.debug_log(3, "å†…å®¹ä¸åŒ…å«å¤šæ¨¡æ€å…ƒç´ ï¼Œä¸ä½¿ç”¨å¤šæ¨¡æ€å‘é‡", "ğŸ“")
            return None
        
        embedding = await self.safe_api_call(
            self._get_multimodal_embedding_impl, "å¤šæ¨¡æ€å‘é‡", content, event_emitter
        )
        
        if embedding:
            self.debug_log(3, f"å¤šæ¨¡æ€å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ–¼ï¸")
        
        return embedding

    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)

    # ========== å‘é‡æ£€ç´¢ ==========
    
    async def vector_retrieve_relevant_messages(
        self, query_message: dict, candidate_messages: List[dict], progress: ProgressTracker
    ) -> List[dict]:
        """åŸºäºå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³æ¶ˆæ¯ - å¢å¼ºå®¹é”™æœºåˆ¶"""
        if not candidate_messages or not self.valves.enable_vector_retrieval:
            return candidate_messages
        
        await progress.start_phase("æ™ºèƒ½å‘é‡æ£€ç´¢", len(candidate_messages))
        
        self.debug_log(1, f"å¼€å§‹å‘é‡æ£€ç´¢: æŸ¥è¯¢1æ¡ï¼Œå€™é€‰{len(candidate_messages)}æ¡", "ğŸ”")
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.vector_retrievals += 1
        
        # è·å–æŸ¥è¯¢å†…å®¹
        query_content = query_message.get("content", "")
        query_text = self.extract_text_from_content(query_content)
        
        # ç”Ÿæˆæœç´¢å…³é”®å­—
        await progress.update_progress(0, len(candidate_messages), "ç”Ÿæˆæœç´¢å…³é”®å­—")
        search_keywords = await self.generate_search_keywords(query_text, progress.event_emitter)
        
        self.debug_log(1, f"ğŸ”‘ æœç´¢å…³é”®å­—({len(search_keywords)}ä¸ª): {search_keywords[:3]}...", "ğŸ”‘")
        
        # å¯¹æ¯ä¸ªå…³é”®å­—è¿›è¡Œå‘é‡æ£€ç´¢
        all_similarities = []
        
        for keyword_idx, keyword in enumerate(search_keywords):
            await progress.update_progress(
                keyword_idx * len(candidate_messages) // len(search_keywords),
                len(candidate_messages),
                f"å…³é”®å­—{keyword_idx+1}/{len(search_keywords)}: {keyword[:20]}..."
            )
            
            # è·å–å…³é”®å­—å‘é‡
            keyword_vector = None
            
            # æ™ºèƒ½å‘é‡åŒ–ç­–ç•¥
            if self.has_images_in_content(query_content):
                # å¤šæ¨¡æ€å†…å®¹ï¼šä¼˜å…ˆä½¿ç”¨å¤šæ¨¡æ€å‘é‡
                keyword_vector = await self.get_multimodal_embedding(query_content, progress.event_emitter)
                self.debug_log(3, f"å…³é”®å­—ä½¿ç”¨å¤šæ¨¡æ€å‘é‡: {'æˆåŠŸ' if keyword_vector else 'å¤±è´¥'}", "ğŸ–¼ï¸")
                
                # å¦‚æœå¤šæ¨¡æ€å‘é‡å¤±è´¥ï¼Œè½¬æ¢ä¸ºæ–‡æœ¬å‘é‡
                if not keyword_vector:
                    keyword_vector = await self.get_text_embedding(keyword, progress.event_emitter)
                    self.debug_log(3, f"å…³é”®å­—ä½¿ç”¨æ–‡æœ¬å‘é‡: {'æˆåŠŸ' if keyword_vector else 'å¤±è´¥'}", "ğŸ“")
            else:
                # çº¯æ–‡æœ¬å†…å®¹ï¼šä½¿ç”¨æ–‡æœ¬å‘é‡
                keyword_vector = await self.get_text_embedding(keyword, progress.event_emitter)
                self.debug_log(3, f"å…³é”®å­—ä½¿ç”¨æ–‡æœ¬å‘é‡: {'æˆåŠŸ' if keyword_vector else 'å¤±è´¥'}", "ğŸ“")
            
            if not keyword_vector:
                continue
            
            # è®¡ç®—ä¸å€™é€‰æ¶ˆæ¯çš„ç›¸ä¼¼åº¦
            for msg_idx, msg in enumerate(candidate_messages):
                msg_content = msg.get("content", "")
                msg_vector = None
                
                # ä¸ºå€™é€‰æ¶ˆæ¯è·å–å‘é‡
                if self.has_images_in_content(msg_content):
                    # å¤šæ¨¡æ€å†…å®¹ï¼šä¼˜å…ˆä½¿ç”¨å¤šæ¨¡æ€å‘é‡
                    msg_vector = await self.get_multimodal_embedding(msg_content, progress.event_emitter)
                    if not msg_vector:
                        # å¤šæ¨¡æ€å‘é‡å¤±è´¥ï¼Œè½¬æ¢ä¸ºæ–‡æœ¬å‘é‡
                        text_content = self.extract_text_from_content(msg_content)
                        if text_content:
                            msg_vector = await self.get_text_embedding(text_content, progress.event_emitter)
                else:
                    # çº¯æ–‡æœ¬å†…å®¹ï¼šä½¿ç”¨æ–‡æœ¬å‘é‡
                    text_content = self.extract_text_from_content(msg_content)
                    if text_content:
                        msg_vector = await self.get_text_embedding(text_content, progress.event_emitter)
                
                if msg_vector:
                    similarity = self.cosine_similarity(keyword_vector, msg_vector)
                    
                    # æ ¹æ®å…³é”®å­—æƒé‡è°ƒæ•´ç›¸ä¼¼åº¦
                    if keyword_idx == 0:  # åŸå§‹æŸ¥è¯¢æƒé‡æ›´é«˜
                        similarity *= 1.2
                    
                    # é«˜ä¼˜å…ˆçº§å†…å®¹ç»™äºˆåŠ æƒ
                    if self.is_high_priority_content(self.extract_text_from_content(msg_content)):
                        similarity = min(1.0, similarity * 1.25)
                    
                    # ç»™æœ€è¿‘çš„æ¶ˆæ¯é¢å¤–åŠ æƒ
                    if msg_idx >= len(candidate_messages) - self.valves.preserve_recent_exchanges * 2:
                        similarity = min(1.0, similarity * 1.15)
                    
                    all_similarities.append((msg_idx, similarity, msg, keyword_idx))
        
        if not all_similarities:
            self.debug_log(1, "å‘é‡æ£€ç´¢å¤±è´¥ï¼Œè¿”å›åŸå§‹æ¶ˆæ¯", "âš ï¸")
            # æ›´æ–°RAGæ— ç»“æœç»Ÿè®¡
            self.stats.rag_no_results_count += 1
            await progress.complete_phase("å‘é‡æ£€ç´¢å¤±è´¥")
            return candidate_messages
        
        # æŒ‰æ¶ˆæ¯åˆ†ç»„ï¼Œå–æœ€é«˜ç›¸ä¼¼åº¦
        msg_best_similarity = {}
        for msg_idx, similarity, msg, keyword_idx in all_similarities:
            if msg_idx not in msg_best_similarity or similarity > msg_best_similarity[msg_idx][1]:
                msg_best_similarity[msg_idx] = (msg_idx, similarity, msg)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities = list(msg_best_similarity.values())
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼è¿‡æ»¤
        threshold = self.valves.vector_similarity_threshold
        filtered_similarities = [item for item in similarities if item[1] >= threshold]
        
        # å¦‚æœè¿‡æ»¤åå¤ªå°‘ï¼Œé™ä½é˜ˆå€¼ç¡®ä¿ä¿ç•™è¶³å¤Ÿæ¶ˆæ¯ - å¢å¼ºå®¹é”™æœºåˆ¶
        min_keep_ratio = 0.4  # è‡³å°‘ä¿ç•™40%çš„æ¶ˆæ¯
        min_keep = max(20, int(len(candidate_messages) * min_keep_ratio))
        
        if len(filtered_similarities) < min_keep:
            lower_threshold = max(0.02, threshold - 0.04)
            filtered_similarities = [item for item in similarities if item[1] >= lower_threshold]
            self.debug_log(2, f"é™ä½é˜ˆå€¼åˆ°{lower_threshold:.3f}ï¼Œä¿ç•™æ›´å¤šæ¶ˆæ¯", "ğŸ”")
        
        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€å®šæ•°é‡çš„æ¶ˆæ¯
        if len(filtered_similarities) < min_keep:
            filtered_similarities = similarities[:min_keep]
            self.debug_log(2, f"å¼ºåˆ¶ä¿ç•™{min_keep}æ¡æ¶ˆæ¯ï¼Œç¡®ä¿ä¸ä¸¢å¤±æ•°æ®", "ğŸ”")
        
        # å¦‚æœä»ç„¶æ²¡æœ‰è¶³å¤Ÿçš„ç»“æœï¼Œè®°å½•ç»Ÿè®¡
        if len(filtered_similarities) < self.valves.min_history_messages:
            self.stats.rag_no_results_count += 1
            self.debug_log(1, f"ğŸ” RAGæ£€ç´¢ç»“æœè¿‡å°‘: {len(filtered_similarities)}æ¡", "âš ï¸")
            
            # åº”ç”¨å®¹é”™ä¿æŠ¤æœºåˆ¶
            if self.valves.enable_fallback_preservation:
                fallback_messages = self.apply_fallback_preservation(
                    candidate_messages, self.count_messages_tokens(candidate_messages)
                )
                self.debug_log(1, f"ğŸ›¡ï¸ åº”ç”¨å®¹é”™ä¿æŠ¤ï¼Œè¡¥å……åˆ°{len(fallback_messages)}æ¡æ¶ˆæ¯", "ğŸ›¡ï¸")
                
                # åˆå¹¶æ£€ç´¢ç»“æœå’Œå®¹é”™ä¿æŠ¤ç»“æœ
                result_message_ids = {similarities[i][0] for i in range(len(filtered_similarities))}
                for i, msg in enumerate(fallback_messages):
                    if i not in result_message_ids:
                        filtered_similarities.append((i, 0.5, msg))  # ç»™å®¹é”™ä¿æŠ¤çš„æ¶ˆæ¯ä¸€ä¸ªä¸­ç­‰ç›¸ä¼¼åº¦
        
        # é™åˆ¶æ•°é‡
        top_similarities = filtered_similarities[:self.valves.vector_top_k]
        
        # æå–æ¶ˆæ¯å¹¶ä¿æŒåŸå§‹é¡ºåº
        relevant_messages = []
        selected_indices = sorted([item[0] for item in top_similarities])
        
        for idx in selected_indices:
            if idx < len(candidate_messages):  # é˜²æ­¢ç´¢å¼•è¶Šç•Œ
                relevant_messages.append(candidate_messages[idx])
        
        # ä½¿ç”¨æ¶ˆæ¯é¡ºåºç®¡ç†å™¨ç¡®ä¿é¡ºåºæ­£ç¡®ï¼Œä½†ä¸åŒ…æ‹¬å½“å‰ç”¨æˆ·æ¶ˆæ¯
        if self.message_order:
            relevant_messages = self.message_order.sort_messages_preserve_user(
                relevant_messages, self.current_user_message
            )
        
        self.debug_log(1, f"ğŸ” æ™ºèƒ½å‘é‡æ£€ç´¢å®Œæˆ: {len(candidate_messages)} -> {len(relevant_messages)}æ¡", "âœ…")
        
        await progress.complete_phase(f"æ£€ç´¢åˆ°{len(relevant_messages)}æ¡ç›¸å…³æ¶ˆæ¯")
        
        return relevant_messages

    # ========== é‡æ’åº ==========
    
    async def _rerank_messages_impl(self, query_text: str, documents: List[str], event_emitter):
        """å®é™…çš„é‡æ’åºå®ç°"""
        if not HTTPX_AVAILABLE:
            return None
        
        # åŸºæœ¬æ¸…ç†æŸ¥è¯¢æ–‡æœ¬å’Œæ–‡æ¡£
        cleaned_query_text = query_text.replace("\n", " ").replace("\r", " ")
        cleaned_query_text = re.sub(r"\s+", " ", cleaned_query_text).strip()
        
        cleaned_documents = []
        for doc in documents:
            cleaned_doc = doc.replace("\n", " ").replace("\r", " ")
            cleaned_doc = re.sub(r"\s+", " ", cleaned_doc).strip()
            cleaned_documents.append(cleaned_doc)
        
        async with httpx.AsyncClient(timeout=self.valves.request_timeout) as client:
            headers = {
                "Authorization": f"Bearer {self.valves.rerank_api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.valves.rerank_model,
                "query": cleaned_query_text,
                "documents": cleaned_documents,
                "top_n": min(self.valves.rerank_top_k, len(cleaned_documents)),
                "return_documents": True
            }
            
            response = await client.post(
                f"{self.valves.rerank_api_base}/v1/rerank",
                headers=headers,
                json=data
            )
            
            if response.status_code != 200:
                raise Exception(f"HTTP {response.status_code}: {response.text}")
            
            # æ£€æŸ¥å“åº”æ ¼å¼
            response_text = response.text
            if not self.is_json_response(response_text):
                error_info = self.extract_error_info(response_text)
                raise Exception(f"éJSONå“åº”: {error_info}")
            
            result = response.json()
            
            if result.get("code") != 200:
                error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                raise Exception(f"APIé”™è¯¯: {error_msg}")
            
            return result.get("data", {}).get("results", [])

    async def rerank_messages(
        self, query_message: dict, candidate_messages: List[dict], progress: ProgressTracker
    ) -> List[dict]:
        """é‡æ’åºæ¶ˆæ¯"""
        if not candidate_messages or not self.valves.enable_reranking:
            return candidate_messages
        
        await progress.start_phase("é‡æ’åº", len(candidate_messages))
        
        self.debug_log(1, f"å¼€å§‹é‡æ’åº: æŸ¥è¯¢1æ¡ï¼Œå€™é€‰{len(candidate_messages)}æ¡", "ğŸ”„")
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.rerank_operations += 1
        
        # å‡†å¤‡æŸ¥è¯¢æ–‡æœ¬
        query_text = self.extract_text_from_content(query_message.get("content", ""))
        if not query_text:
            await progress.complete_phase("æŸ¥è¯¢æ–‡æœ¬ä¸ºç©º")
            return candidate_messages
        
        await progress.update_progress(0, 1, "å‡†å¤‡æ–‡æ¡£åˆ—è¡¨")
        
        # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
        documents = []
        for msg in candidate_messages:
            text = self.extract_text_from_content(msg.get("content", ""))
            if text:
                # æé«˜æ–‡æ¡£é•¿åº¦é™åˆ¶
                if len(text) > 12000:
                    text = text[:12000] + "..."
                documents.append(text)
            else:
                documents.append("ç©ºæ¶ˆæ¯")
        
        if not documents:
            await progress.complete_phase("æ— æœ‰æ•ˆæ–‡æ¡£")
            return candidate_messages
        
        await progress.update_progress(1, 1, "è°ƒç”¨é‡æ’åºAPI")
        
        # è°ƒç”¨é‡æ’åºAPI
        rerank_results = await self.safe_api_call(
            self._rerank_messages_impl, "é‡æ’åº", query_text, documents, progress.event_emitter
        )
        
        if rerank_results:
            # æŒ‰é‡æ’åºç»“æœé‡æ–°æ’åˆ—æ¶ˆæ¯
            reranked_messages = []
            for item in rerank_results:
                original_index = item.get("index", 0)
                if 0 <= original_index < len(candidate_messages):
                    reranked_messages.append(candidate_messages[original_index])
                    score = item.get("relevance_score", 0)
                    self.debug_log(3, f"é‡æ’åºç»“æœ: index={original_index}, score={score:.3f}", "ğŸ“Š")
            
            # ä½¿ç”¨æ¶ˆæ¯é¡ºåºç®¡ç†å™¨ç¡®ä¿é¡ºåºæ­£ç¡®ï¼Œä½†ä¸åŒ…æ‹¬å½“å‰ç”¨æˆ·æ¶ˆæ¯
            if self.message_order:
                reranked_messages = self.message_order.sort_messages_preserve_user(
                    reranked_messages, self.current_user_message
                )
            
            self.debug_log(1, f"ğŸ”„ é‡æ’åºå®Œæˆ: {len(candidate_messages)} -> {len(reranked_messages)}æ¡", "âœ…")
            
            await progress.complete_phase(f"é‡æ’åºåˆ°{len(reranked_messages)}æ¡æ¶ˆæ¯")
            return reranked_messages
        
        await progress.complete_phase("é‡æ’åºå¤±è´¥")
        return candidate_messages

    # ========== å†…å®¹æœ€å¤§åŒ–æ ¸å¿ƒå¤„ç†é€»è¾‘ ==========
    
    def should_force_maximize_content(self, messages: List[dict], target_tokens: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼ºåˆ¶è¿›è¡Œå†…å®¹æœ€å¤§åŒ–å¤„ç†"""
        current_tokens = self.count_messages_tokens(messages)
        utilization = current_tokens / target_tokens if target_tokens > 0 else 0
        
        # å¦‚æœå½“å‰åˆ©ç”¨ç‡ä½äºæœ€å¤§åˆ©ç”¨ç‡ï¼Œå¼ºåˆ¶æœ€å¤§åŒ–
        # æˆ–è€…å¦‚æœè¶…è¿‡äº†ç›®æ ‡é™åˆ¶ï¼Œä¹Ÿéœ€è¦å¤„ç†
        should_maximize = (
            utilization < self.valves.max_window_utilization 
            or current_tokens > target_tokens
        )
        
        self.debug_log(1, f"ğŸ”¥ å†…å®¹æœ€å¤§åŒ–åˆ¤æ–­: {current_tokens:,}tokens / {target_tokens:,}tokens = {utilization:.1%}", "ğŸ”¥")
        self.debug_log(1, f"ğŸ”¥ éœ€è¦æœ€å¤§åŒ–: {should_maximize} "
                          f"(åˆ©ç”¨ç‡{'<' if utilization < self.valves.max_window_utilization else '>='}"
                          f"{self.valves.max_window_utilization:.1%} æˆ– è¶…é™åˆ¶)", "ğŸ”¥")
        
        return should_maximize

    async def maximize_content_comprehensive_processing(
        self, messages: List[dict], target_tokens: int, progress: ProgressTracker
    ) -> List[dict]:
        """å†…å®¹æœ€å¤§åŒ–ç»¼åˆå¤„ç† - ä¿®å¤RAGæœç´¢æ— ç»“æœé—®é¢˜ï¼Œå¢å¼ºå®¹é”™æœºåˆ¶"""
        start_time = time.time()
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        self.stats.original_tokens = self.count_messages_tokens(messages)
        self.stats.original_messages = len(messages)
        self.stats.token_limit = self.get_model_token_limit("unknown")
        self.stats.target_tokens = target_tokens
        current_tokens = self.stats.original_tokens
        
        # è®°å½•çœŸå®çš„åŸå§‹æ•°æ®é‡
        self.debug_log(1, f"ğŸ”¥ çœŸå®åŸå§‹æ•°æ®é‡: {current_tokens:,} tokens, {len(messages)} æ¡æ¶ˆæ¯", "ğŸ”¥")
        
        await progress.start_phase("å†…å®¹æœ€å¤§åŒ–å¤„ç†", 10)
        
        self.debug_log(1, f"ğŸ”¥ å¼€å§‹å†…å®¹æœ€å¤§åŒ–å¤„ç† [ID:{self.current_processing_id}]: "
                          f"{current_tokens:,} -> {target_tokens:,} tokens", "ğŸ”¥")
        
        # 1. ä½¿ç”¨ä¿®å¤åçš„æ¶ˆæ¯åˆ†ç¦»é€»è¾‘
        await progress.update_progress(1, 10, "åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯")
        current_user_message, history_messages = self.separate_current_and_history_messages(messages)
        
        # ä¿å­˜å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„å¼•ç”¨
        self.current_user_message = current_user_message
        
        # ç³»ç»Ÿæ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        
        # 2. æ£€æµ‹æ˜¯å¦éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–
        need_context_max = False
        if current_user_message and self.valves.enable_context_maximization:
            query_text = self.extract_text_from_content(current_user_message.get("content", ""))
            need_context_max = await self.detect_context_max_need(query_text, progress.event_emitter)
            if need_context_max:
                self.debug_log(1, f"ğŸ“š æ£€æµ‹åˆ°éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ï¼Œå¯ç”¨ç‰¹æ®Šå¤„ç†ç­–ç•¥", "ğŸ“š")
        
        # 3. è®¡ç®—ä¿æŠ¤æ¶ˆæ¯çš„token
        protected_messages = system_messages
        if current_user_message:
            protected_messages.append(current_user_message)
            self.stats.current_user_tokens = self.count_message_tokens(current_user_message)
        
        protected_tokens = self.count_messages_tokens(protected_messages)
        available_for_processing = target_tokens - protected_tokens
        
        self.debug_log(1, f"ğŸ”’ ä¿æŠ¤æ¶ˆæ¯: {len(protected_messages)}æ¡ ({protected_tokens:,}tokens)", "ğŸ”’")
        self.debug_log(1, f"ğŸ’° å¯ç”¨å¤„ç†ç©ºé—´: {available_for_processing:,}tokens", "ğŸ’°")
        
        # 4. å¦‚æœå†å²æ¶ˆæ¯ä¸ºç©ºï¼Œç›´æ¥è¿”å›
        if not history_messages:
            self.debug_log(1, f"ğŸ“‹ æ— å†å²æ¶ˆæ¯ï¼Œç›´æ¥è¿”å›ä¿æŠ¤æ¶ˆæ¯", "ğŸ“‹")
            await progress.complete_phase("æ— å†å²æ¶ˆæ¯éœ€è¦å¤„ç†")
            return protected_messages
        
        # 5. æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©å¤„ç†ç­–ç•¥
        if need_context_max and self.valves.enable_context_maximization:
            # ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ï¼šä½¿ç”¨ä¸“é—¨çš„å¤„ç†ç­–ç•¥
            await progress.update_progress(2, 10, "ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¸“ç”¨å¤„ç†")
            processed_history = await self.process_context_maximization(
                history_messages, available_for_processing, progress, need_context_max
            )
            self.debug_log(1, f"ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†å®Œæˆ: {len(history_messages)} -> {len(processed_history)}æ¡", "ğŸ“š")
        else:
            # å…·ä½“æŸ¥è¯¢ï¼šä½¿ç”¨æ ‡å‡†çš„RAGå¤„ç†ç­–ç•¥
            await progress.update_progress(2, 10, "æ ‡å‡†RAGå¤„ç†")
            
            # å°½é‡ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
            try_preserved_messages, remaining_budget = await self.try_preserve_recent_messages(
                history_messages, available_for_processing, progress, need_context_max
            )
            
            # ä»å†å²æ¶ˆæ¯ä¸­ç§»é™¤å·²å°½é‡ä¿ç•™çš„æ¶ˆæ¯
            if try_preserved_messages:
                preserved_ids = {msg.get("_order_id") for msg in try_preserved_messages}
                remaining_history = [
                    msg for msg in history_messages 
                    if msg.get("_order_id") not in preserved_ids
                ]
            else:
                remaining_history = history_messages
            
            # æ™ºèƒ½åˆ†ç‰‡å¤„ç†å¤§æ¶ˆæ¯ - ä½¿ç”¨å…¬ç”¨åŠŸèƒ½
            await progress.update_progress(3, 10, "æ™ºèƒ½åˆ†ç‰‡å¤„ç†")
            if remaining_budget > 5000 and remaining_history:
                processed_remaining = await self.smart_chunk_and_summarize_messages(
                    remaining_history, remaining_budget, progress, "RAG"
                )
            else:
                processed_remaining = remaining_history
            
            # RAGå¤„ç†
            await progress.update_progress(4, 10, "RAGå¤„ç†")
            if remaining_budget > 2000 and processed_remaining and current_user_message:
                # å‘é‡æ£€ç´¢
                if self.valves.enable_vector_retrieval:
                    self.debug_log(1, f"ğŸ” å¯åŠ¨å‘é‡æ£€ç´¢: {len(processed_remaining)}æ¡å€™é€‰æ¶ˆæ¯", "ğŸ”")
                    
                    relevant_history = await self.vector_retrieve_relevant_messages(
                        current_user_message, processed_remaining, progress
                    )
                    
                    # é‡æ’åº
                    if self.valves.enable_reranking and len(relevant_history) > 5:
                        relevant_history = await self.rerank_messages(
                            current_user_message, relevant_history, progress
                        )
                    
                    processed_remaining = relevant_history
                    self.debug_log(1, f"ğŸ” RAGå¤„ç†å®Œæˆ: {len(processed_remaining)}æ¡ç›¸å…³æ¶ˆæ¯", "ğŸ”")
                    
                    # æ£€æŸ¥RAGç»“æœæ˜¯å¦è¿‡å°‘ï¼Œåº”ç”¨å®¹é”™ä¿æŠ¤
                    if len(processed_remaining) < self.valves.min_history_messages:
                        self.debug_log(1, f"ğŸ›¡ï¸ RAGç»“æœè¿‡å°‘({len(processed_remaining)}æ¡)ï¼Œåº”ç”¨å®¹é”™ä¿æŠ¤", "ğŸ›¡ï¸")
                        
                        # ä½¿ç”¨å®¹é”™ä¿æŠ¤æœºåˆ¶è¡¥å……æ¶ˆæ¯
                        fallback_messages = self.apply_fallback_preservation(
                            remaining_history, remaining_budget
                        )
                        
                        # åˆå¹¶å»é‡
                        result_message_ids = {msg.get("_order_id") for msg in processed_remaining if msg.get("_order_id")}
                        for msg in fallback_messages:
                            if msg.get("_order_id") not in result_message_ids:
                                processed_remaining.append(msg)
                                result_message_ids.add(msg.get("_order_id"))
                        
                        self.debug_log(1, f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤åè¡¥å……åˆ°{len(processed_remaining)}æ¡æ¶ˆæ¯", "ğŸ›¡ï¸")
                else:
                    # å¦‚æœä¸å¯ç”¨å‘é‡æ£€ç´¢ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
                    processed_remaining = self.sort_messages_by_priority(processed_remaining)
            
            # ç»„åˆç»“æœ
            processed_history = try_preserved_messages + processed_remaining
        
        # 6. æœ€ç»ˆé¢„ç®—æ§åˆ¶
        await progress.update_progress(6, 10, "æœ€ç»ˆé¢„ç®—æ§åˆ¶")
        final_history = processed_history
        final_tokens = self.count_messages_tokens(final_history)
        
        # å¦‚æœä»ç„¶è¶…å‡ºé¢„ç®—ï¼Œè¿›è¡Œç´§æ€¥æˆªæ–­
        if final_tokens > available_for_processing:
            truncated_history = []
            used_tokens = 0
            
            for msg in final_history:
                msg_tokens = self.count_message_tokens(msg)
                if used_tokens + msg_tokens <= available_for_processing:
                    truncated_history.append(msg)
                    used_tokens += msg_tokens
                else:
                    break
            
            final_history = truncated_history
            self.stats.emergency_truncations += 1
            
            self.debug_log(1, f"ğŸ†˜ ç´§æ€¥æˆªæ–­: ä¿ç•™{len(final_history)}æ¡å†å²æ¶ˆæ¯({used_tokens:,}tokens)", "ğŸ†˜")
        
        # 7. ç»„åˆæœ€ç»ˆç»“æœ
        await progress.update_progress(8, 10, "ç»„åˆæœ€ç»ˆç»“æœ")
        
        # æŒ‰é¡ºåºç»„åˆï¼šç³»ç»Ÿæ¶ˆæ¯ + å¤„ç†åçš„å†å²æ¶ˆæ¯
        current_result = system_messages + final_history
        
        # ç¡®ä¿æ¶ˆæ¯é¡ºåºæ­£ç¡®ï¼Œä½†ä¸åŒ…æ‹¬å½“å‰ç”¨æˆ·æ¶ˆæ¯
        if self.message_order:
            current_result = self.message_order.sort_messages_preserve_user(
                current_result, self.current_user_message
            )
        
        # æœ€ç»ˆç»„åˆ
        final_messages = []
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯å’Œå†å²æ¶ˆæ¯
        for msg in current_result:
            final_messages.append(msg)
        
        # æœ€åæ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        if current_user_message:
            final_messages.append(current_user_message)
        
        # 8. åº”ç”¨æœ€ç»ˆçš„ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤æœºåˆ¶
        await progress.update_progress(9, 10, "ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤éªŒè¯")
        final_messages = self.ensure_current_user_message_preserved(final_messages)
        
        # 9. æ›´æ–°æœ€ç»ˆç»Ÿè®¡
        await progress.update_progress(10, 10, "æ›´æ–°ç»Ÿè®¡")
        self.stats.final_tokens = self.count_messages_tokens(final_messages)
        self.stats.final_messages = len(final_messages)
        self.stats.processing_time = time.time() - start_time
        self.stats.iterations = 1
        
        # ä¿®å¤ç»Ÿè®¡è®¡ç®—
        if self.stats.original_tokens > 0:
            self.stats.content_loss_ratio = max(
                0, (self.stats.original_tokens - self.stats.final_tokens) / self.stats.original_tokens
            )
        
        if target_tokens > 0:
            self.stats.window_utilization = self.stats.final_tokens / target_tokens
        
        # æ£€æŸ¥å½“å‰ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦ä¿ç•™
        if current_user_message:
            self.stats.current_user_preserved = any(
                msg.get("_order_id") == current_user_message.get("_order_id")
                for msg in final_messages
            )
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        retention_ratio = self.stats.calculate_retention_ratio()
        window_usage = self.stats.calculate_window_usage_ratio()
        
        self.debug_log(1, f"ğŸ”¥ å†…å®¹æœ€å¤§åŒ–å¤„ç†å®Œæˆ [ID:{self.current_processing_id}]: "
                          f"ä¿ç•™{retention_ratio:.1%} çª—å£ä½¿ç”¨{window_usage:.1%}", "ğŸ”¥")
        
        # éªŒè¯å½“å‰ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦åœ¨æœ€å
        if current_user_message and final_messages:
            last_msg = final_messages[-1]
            if last_msg.get("_order_id") == current_user_message.get("_order_id"):
                self.debug_log(1, f"âœ… å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤æˆåŠŸï¼šåœ¨æœ€åä½ç½®", "âœ…")
            else:
                self.debug_log(1, f"âŒ å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤å¤±è´¥ï¼šä¸åœ¨æœ€åä½ç½®", "âŒ")
        
        await progress.update_progress(10, 10, "å¤„ç†å®Œæˆ")
        await progress.complete_phase(
            f"æœ€å¤§åŒ–å®Œæˆ ä¿ç•™{retention_ratio:.1%} çª—å£ä½¿ç”¨{window_usage:.1%} "
            f"{'[ä¸Šä¸‹æ–‡æœ€å¤§åŒ–]' if need_context_max else '[å…·ä½“æŸ¥è¯¢]'}"
        )
        
        return final_messages

    def sort_messages_by_priority(self, messages: List[dict]) -> List[dict]:
        """æŒ‰ä¼˜å…ˆçº§æ’åºæ¶ˆæ¯"""
        def get_priority_score(msg):
            content = self.extract_text_from_content(msg.get("content", ""))
            score = 0
            
            # é«˜ä¼˜å…ˆçº§å…³é”®è¯åŠ åˆ†
            if self.is_high_priority_content(content):
                score += 100
            
            # é•¿åº¦åŠ åˆ†ï¼ˆæ›´é•¿çš„æ¶ˆæ¯å¯èƒ½åŒ…å«æ›´å¤šä¿¡æ¯ï¼‰
            score += min(len(content) // 100, 50)
            
            # è§’è‰²åŠ åˆ†
            if msg.get("role") == "user":
                score += 20
            elif msg.get("role") == "assistant":
                score += 10
            
            # åŸå§‹ç´¢å¼•åŠ åˆ†ï¼ˆè¶Šæ–°çš„æ¶ˆæ¯åˆ†æ•°è¶Šé«˜ï¼‰
            original_index = msg.get("_original_index", 0)
            score += original_index * 0.1
            
            return score
        
        return sorted(messages, key=get_priority_score, reverse=True)

    def print_detailed_stats(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.valves.enable_detailed_stats:
            return
        
        print("\n" + "=" * 70)
        print(self.stats.get_summary())
        print("=" * 70)

    # ========== å¤šæ¨¡æ€å¤„ç†ç­–ç•¥ ==========
    
    async def determine_multimodal_processing_strategy(
        self, messages: List[dict], model_name: str, target_tokens: int
    ) -> Tuple[str, str]:
        """ç¡®å®šå¤šæ¨¡æ€å¤„ç†ç­–ç•¥"""
        # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return "text_only", "æ— å›¾ç‰‡å†…å®¹ï¼ŒæŒ‰æ–‡æœ¬å¤„ç†"
        
        # 2. åˆ¤æ–­æ¨¡å‹ç±»å‹
        is_multimodal = self.is_multimodal_model(model_name)
        should_force_vision = self.should_force_vision_processing(model_name)
        
        self.debug_log(1, f"ğŸ¤– æ¨¡å‹åˆ†æ: {model_name} | å¤šæ¨¡æ€æ”¯æŒ: {is_multimodal} | å¼ºåˆ¶è§†è§‰å¤„ç†: {should_force_vision}", "ğŸ¤–")
        
        # 3. æ™ºèƒ½è‡ªé€‚åº”ç­–ç•¥
        if is_multimodal:
            # å¤šæ¨¡æ€æ¨¡å‹ï¼šæ£€æŸ¥Tokené¢„ç®—
            budget_sufficient = self.calculate_multimodal_budget_sufficient(messages, target_tokens)
            if budget_sufficient:
                return "direct_multimodal", "å¤šæ¨¡æ€æ¨¡å‹ï¼ŒTokené¢„ç®—å……è¶³ï¼Œç›´æ¥è¾“å…¥"
            else:
                return "multimodal_rag", "å¤šæ¨¡æ€æ¨¡å‹ï¼ŒTokené¢„ç®—ä¸è¶³ï¼Œä½¿ç”¨å¤šæ¨¡æ€å‘é‡RAG"
        else:
            # çº¯æ–‡æœ¬æ¨¡å‹ï¼šéœ€è¦å…ˆè¯†åˆ«å›¾ç‰‡
            return "vision_to_text", "çº¯æ–‡æœ¬æ¨¡å‹ï¼Œå…ˆè¯†åˆ«å›¾ç‰‡å†å¤„ç†"

    async def process_multimodal_content(
        self, messages: List[dict], model_name: str, target_tokens: int, progress: ProgressTracker
    ) -> List[dict]:
        """å¤šæ¨¡æ€å†…å®¹å¤„ç†"""
        if not self.valves.enable_multimodal:
            return messages
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        has_images = self.has_images_in_messages(messages)
        if not has_images:
            self.debug_log(2, "æ¶ˆæ¯ä¸­æ— å›¾ç‰‡å†…å®¹ï¼Œè·³è¿‡å¤šæ¨¡æ€å¤„ç†", "ğŸ“")
            return messages
        
        # ç¡®å®šå¤šæ¨¡æ€å¤„ç†ç­–ç•¥
        strategy, strategy_desc = await self.determine_multimodal_processing_strategy(
            messages, model_name, target_tokens
        )
        
        self.debug_log(1, f"ğŸ¯ å¤šæ¨¡æ€å¤„ç†ç­–ç•¥: {strategy} - {strategy_desc}", "ğŸ¯")
        
        # æ ¹æ®ç­–ç•¥å¤„ç†
        if strategy == "text_only":
            # æ— å›¾ç‰‡ï¼Œç›´æ¥è¿”å›
            return messages
        elif strategy == "direct_multimodal":
            # å¤šæ¨¡æ€æ¨¡å‹ï¼ŒTokené¢„ç®—å……è¶³ï¼Œç›´æ¥è¾“å…¥åŸå§‹å†…å®¹
            self.debug_log(1, f"âœ… å¤šæ¨¡æ€æ¨¡å‹ç›´æ¥è¾“å…¥åŸå§‹å†…å®¹", "ğŸ–¼ï¸")
            return messages
        elif strategy == "vision_to_text":
            # çº¯æ–‡æœ¬æ¨¡å‹æˆ–å¼ºåˆ¶å¤„ç†ï¼Œå…ˆè¯†åˆ«å›¾ç‰‡å†å¤„ç†
            await progress.start_phase("è§†è§‰è¯†åˆ«è½¬æ–‡æœ¬", len(messages))
            
            # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
            total_images = 0
            for msg in messages:
                if isinstance(msg.get("content"), list):
                    total_images += len([
                        item for item in msg.get("content", [])
                        if item.get("type") == "image_url"
                    ])
            
            self.debug_log(1, f"ğŸ” å¼€å§‹è§†è§‰è¯†åˆ«è½¬æ–‡æœ¬ï¼š{total_images} å¼ å›¾ç‰‡", "ğŸ”")
            
            # å¹¶å‘å¤„ç†æ‰€æœ‰æ¶ˆæ¯
            semaphore = asyncio.Semaphore(self.valves.max_concurrent_requests)
            
            async def process_single_message(i, message):
                if self.has_images_in_content(message.get("content")):
                    async with semaphore:
                        # åˆ›å»ºå­è¿›åº¦è¿½è¸ª
                        class SubProgress:
                            def __init__(self, parent, base_count):
                                self.parent = parent
                                self.base_count = base_count
                                self.event_emitter = parent.event_emitter
                            
                            async def update_progress(self, current, total, detail):
                                await self.parent.update_progress(
                                    self.base_count + current, self.parent.phase_total, detail
                                )
                        
                        sub_progress = SubProgress(progress, i)
                        processed_message = await self.process_message_images(message, sub_progress)
                        return processed_message
                else:
                    return message
            
            # å¹¶å‘å¤„ç†æ‰€æœ‰æ¶ˆæ¯
            process_tasks = []
            for i, message in enumerate(messages):
                task = process_single_message(i, message)
                process_tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            processed_messages = await asyncio.gather(*process_tasks)
            
            # ç¡®ä¿æ¶ˆæ¯é¡ºåºæ­£ç¡®
            if self.message_order:
                processed_messages = self.message_order.sort_messages_preserve_user(
                    processed_messages, self.current_user_message
                )
            
            self.debug_log(1, f"âœ… è§†è§‰è¯†åˆ«è½¬æ–‡æœ¬å®Œæˆï¼š{total_images} å¼ å›¾ç‰‡", "âœ…")
            await progress.complete_phase(f"å¤„ç†å®Œæˆ {total_images} å¼ å›¾ç‰‡")
            
            return processed_messages
        elif strategy == "multimodal_rag":
            # å¤šæ¨¡æ€æ¨¡å‹ï¼ŒTokené¢„ç®—ä¸è¶³ï¼Œä¿ç•™åŸå§‹å†…å®¹ç”¨äºåç»­RAGå¤„ç†
            self.debug_log(1, f"ğŸ” å¤šæ¨¡æ€RAGç­–ç•¥ï¼šä¿ç•™åŸå§‹å†…å®¹ç”¨äºå‘é‡å¤„ç†", "ğŸ”")
            # åœ¨è¿™ä¸ªé˜¶æ®µä¸å¤„ç†å›¾ç‰‡ï¼Œè®©åç»­çš„RAGæµç¨‹å¤„ç†
            return messages
        else:
            # é»˜è®¤ç­–ç•¥
            self.debug_log(1, f"âš ï¸ æœªçŸ¥å¤„ç†ç­–ç•¥ {strategy}ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†", "âš ï¸")
            return messages

    # ========== ä¸»è¦å…¥å£å‡½æ•° ==========
    
    async def inlet(
        self, body: dict, user: Optional[dict] = None, __event_emitter__: Optional[Callable] = None
    ) -> dict:
        """å…¥å£å‡½æ•° - å¤„ç†è¯·æ±‚ - ä¿®å¤RAGæœç´¢æ— ç»“æœé—®é¢˜"""
        print("\nğŸš€ ===== INLET CALLED (Content Maximization Only v2.3.1) =====")
        print(f"ğŸ“¨ æ”¶åˆ°è¯·æ±‚: {list(body.keys())}")
        
        if not self.valves.enable_processing:
            print("âŒ å¤„ç†åŠŸèƒ½å·²ç¦ç”¨")
            return body
        
        messages = body.get("messages", [])
        if not messages:
            print("âŒ æ— æ¶ˆæ¯å†…å®¹")
            return body
        
        model_name = body.get("model", "æœªçŸ¥")
        print(f"ğŸ“‹ æ¨¡å‹: {model_name}, æ¶ˆæ¯æ•°: {len(messages)}")
        
        if self.is_model_excluded(model_name):
            print(f"ğŸš« æ¨¡å‹å·²æ’é™¤")
            return body
        
        # é‡ç½®å¤„ç†çŠ¶æ€
        self.reset_processing_state()
        
        # åˆ†ææ¨¡å‹ä¿¡æ¯ - ä½¿ç”¨æ™ºèƒ½åŒ¹é…å™¨
        self.current_model_info = self.analyze_model(model_name)
        
        # åˆ›å»ºè¿›åº¦è¿½è¸ªå™¨
        progress = ProgressTracker(__event_emitter__)
        
        # åˆå§‹åŒ–æ¶ˆæ¯é¡ºåºç®¡ç†å™¨
        self.message_order = MessageOrder(messages)
        
        # ä½¿ç”¨ä¿®å¤åçš„æ¶ˆæ¯åˆ†ç¦»é€»è¾‘
        current_user_message, history_messages = self.separate_current_and_history_messages(messages)
        
        # ä¿å­˜å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„å¼•ç”¨
        self.current_user_message = current_user_message
        
        # ç®€åŒ–Tokenåˆ†æ - ä½¿ç”¨ç®€åŒ–è®¡ç®—å™¨
        original_tokens = self.count_messages_tokens(messages)
        model_token_limit = self.get_model_token_limit(model_name)
        
        # ä½¿ç”¨å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„tokenæ•°è®¡ç®—ç›®æ ‡
        current_user_tokens = self.count_message_tokens(current_user_message) if current_user_message else 0
        target_tokens = self.calculate_target_tokens(model_name, current_user_tokens)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.token_limit = model_token_limit
        self.stats.target_tokens = target_tokens
        self.stats.current_user_tokens = current_user_tokens
        
        print(f"ğŸ”¥ ç®€åŒ–Tokenç»Ÿè®¡: {original_tokens:,}/{model_token_limit:,} (ç›®æ ‡:{target_tokens:,})")
        print(f"ğŸ¯ æ¨¡å‹ä¿¡æ¯: {self.current_model_info['family']}å®¶æ— | "
              f"{'å¤šæ¨¡æ€' if self.current_model_info['multimodal'] else 'æ–‡æœ¬'} | "
              f"{self.current_model_info['match_type']}åŒ¹é…")
        print(f"ğŸ”¥ å†…å®¹æœ€å¤§åŒ–æ¨¡å¼: å¯ç”¨")
        print(f"ğŸªŸ æœ€å¤§çª—å£åˆ©ç”¨ç‡: {self.valves.max_window_utilization:.1%}")
        print(f"ğŸ”„ æ¿€è¿›å†…å®¹åˆå¹¶: {self.valves.aggressive_content_recovery}")
        print(f"ğŸ”’ å°½é‡ä¿ç•™æœºåˆ¶: {self.valves.enable_try_preserve} (é¢„ç®—:{self.valves.try_preserve_ratio:.1%})")
        print(f"ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–: {self.valves.enable_context_maximization} "
              f"(ç›´æ¥ä¿ç•™:{self.valves.context_max_direct_preserve_ratio:.1%})")
        print(f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤æœºåˆ¶: {self.valves.enable_fallback_preservation} "
              f"(é¢„ç•™:{self.valves.fallback_preserve_ratio:.1%})")
        print(f"ğŸ”‘ æ™ºèƒ½å…³é”®å­—ç”Ÿæˆ: {self.valves.enable_keyword_generation}")
        print(f"ğŸ§  AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹: {self.valves.enable_ai_context_max_detection}")
        print(f"ğŸ§© æ™ºèƒ½åˆ†ç‰‡: {self.valves.enable_smart_chunking} "
              f"(é˜ˆå€¼:{self.valves.large_message_threshold:,}tokens)")
        print(f"ğŸ“Š Tokenè®¡ç®—å™¨: ç®€åŒ–ç‰ˆï¼ˆä»…ç”¨tiktokenï¼‰")
        print(f"ğŸ–¼ï¸ å¤šæ¨¡æ€æ¨¡å‹: {self.valves.multimodal_model}")
        print(f"ğŸ“ æ–‡æœ¬å¤„ç†æ¨¡å‹: {self.valves.text_model}")
        
        # åˆ†æå½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯
        if current_user_message:
            content_preview = self.message_order.get_message_preview(current_user_message)
            
            # ç”Ÿæˆå¤„ç†ID
            processing_id = hashlib.md5(f"{current_user_message.get('_order_id', '')}{content_preview}".encode()).hexdigest()[:8]
            self.current_processing_id = processing_id
            
            if len(content_preview) > 50:
                content_preview = content_preview[:50] + "..."
            
            print(f"ğŸ’¬ å½“å‰ç”¨æˆ·æ¶ˆæ¯ [ID:{processing_id}]: {current_user_tokens}tokens")
            print(f'ğŸ’¬ å½“å‰ç”¨æˆ·è¾“å…¥: "{content_preview}"')
            print(f"ğŸ“œ å†å²æ¶ˆæ¯: {len(history_messages)}æ¡ ({self.count_messages_tokens(history_messages):,}tokens)")
            
            # ä½¿ç”¨AIæ£€æµ‹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚
            query_text = self.extract_text_from_content(current_user_message.get("content", ""))
            if self.valves.enable_ai_context_max_detection:
                try:
                    need_context_max = await self.detect_context_max_need(query_text, __event_emitter__)
                    print(f"ğŸ§  AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹ç»“æœ: {'éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–' if need_context_max else 'ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–'}")
                    if need_context_max:
                        print(f"ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å°†å¼ºåˆ¶å¤„ç†æ‰€æœ‰å‰©ä½™æ¶ˆæ¯")
                        print(f"ğŸ“š ç›´æ¥ä¿ç•™æ¯”ä¾‹: {self.valves.context_max_direct_preserve_ratio:.1%}")
                        print(f"ğŸ§© å¤§æ¶ˆæ¯åˆ†ç‰‡é˜ˆå€¼: {self.valves.large_message_threshold:,}tokens")
                        print(f"ğŸ”„ é€’å½’æ‘˜è¦å‹ç¼©æ¯”ä¾‹: {self.valves.summary_compression_ratio:.1%}")
                        print(f"ğŸ”„ å¼ºåˆ¶è¿›å…¥åˆ†ç‰‡+æ‘˜è¦æµç¨‹å¤„ç†å‰©ä½™æ¶ˆæ¯")
                        print(f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤ç¡®ä¿æœ€å°‘ä¿ç•™{self.valves.min_history_messages}æ¡å†å²æ¶ˆæ¯")
                except Exception as e:
                    print(f"ğŸ§  AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹å¤±è´¥: {e}")
                    need_context_max = self.is_context_max_need_simple(query_text)
                    print(f"ğŸ§  ä½¿ç”¨ç®€å•æ–¹æ³•æ£€æµ‹: {'éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–' if need_context_max else 'ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–'}")
            else:
                need_context_max = self.is_context_max_need_simple(query_text)
                print(f"ğŸ§  ç®€å•æ–¹æ³•æ£€æµ‹: {'éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–' if need_context_max else 'ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–'}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯")
        
        # ä¿®å¤åˆ¤æ–­é€»è¾‘
        should_maximize = self.should_force_maximize_content(messages, target_tokens)
        print(f"ğŸ”¥ æ˜¯å¦éœ€è¦æœ€å¤§åŒ–: {should_maximize}")
        print(f"ğŸ“Š å½“å‰çª—å£åˆ©ç”¨ç‡: {original_tokens/target_tokens:.1%}")
        print(f"ğŸ¯ ç›®æ ‡çª—å£ä½¿ç”¨ç‡: {self.valves.target_window_usage:.1%}")
        print(f"ğŸ”’ æœ€å°å†…å®¹ä¿ç•™æ¯”ä¾‹: {self.valves.min_preserve_ratio:.1%}")
        
        try:
            # 1. å¤šæ¨¡æ€å¤„ç†
            if self.valves.enable_detailed_progress:
                await progress.start_phase("å¤šæ¨¡æ€å¤„ç†", 1)
            
            processed_messages = await self.process_multimodal_content(
                messages, model_name, target_tokens, progress
            )
            
            processed_tokens = self.count_messages_tokens(processed_messages)
            print(f"ğŸ“Š å¤šæ¨¡æ€å¤„ç†å: {processed_tokens:,} tokens")
            
            # 2. å†…å®¹æœ€å¤§åŒ–å¤„ç†
            if should_maximize:
                print(f"ğŸ”¥ å¯åŠ¨å†…å®¹æœ€å¤§åŒ–å¤„ç†...")
                
                if current_user_message:
                    query_text = self.extract_text_from_content(current_user_message.get("content", ""))
                    if self.valves.enable_context_maximization:
                        need_context_max = await self.detect_context_max_need(query_text, __event_emitter__)
                        if need_context_max:
                            print(f"ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¸“ç”¨å¤„ç†ï¼šå¼ºåˆ¶å¤„ç†æ‰€æœ‰å‰©ä½™æ¶ˆæ¯")
                            print(f"ğŸ“š ä¿®å¤å¤„ç†æµç¨‹ï¼Œç¡®ä¿å¤„ç†æ‰€æœ‰å‰©ä½™æ¶ˆæ¯")
                            print(f"ğŸ§© å…¬ç”¨åˆ†ç‰‡åŠŸèƒ½ï¼Œç¡®ä¿åˆ†ç‰‡+æ‘˜è¦æµç¨‹æ­£å¸¸å·¥ä½œ")
                            print(f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤æœºåˆ¶é˜²æ­¢RAGæœç´¢æ— ç»“æœå¯¼è‡´çš„ä¸Šä¸‹æ–‡ä¸¢å¤±")
                        else:
                            print(f"ğŸ” å…·ä½“æŸ¥è¯¢æ ‡å‡†å¤„ç†ï¼šRAGæµç¨‹åŒ…å«å‘é‡æ£€ç´¢å’Œé‡æ’åº")
                            print(f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤ç¡®ä¿å³ä½¿RAGæ— ç»“æœä¹Ÿæœ‰è¶³å¤Ÿå†å²ä¸Šä¸‹æ–‡")
                    else:
                        print(f"ğŸ” æ ‡å‡†RAGæµç¨‹å°†å¯åŠ¨ï¼ŒåŒ…å«å‘é‡æ£€ç´¢å’Œé‡æ’åº")
                        print(f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤æœºåˆ¶é˜²æ­¢æœç´¢æ— ç»“æœ")
                
                print(f"ğŸ§© å…¬ç”¨æ™ºèƒ½åˆ†ç‰‡å°†å¤„ç†å¤§æ¶ˆæ¯ï¼ˆé˜ˆå€¼:{self.valves.large_message_threshold:,}tokensï¼‰")
                print(f"ğŸ”„ é€’å½’æ‘˜è¦å°†å¤„ç†è¶…å¤§å†…å®¹é›†åˆ")
                print(f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤é¢„ç•™{self.valves.fallback_preserve_ratio:.1%}é¢„ç®—ç¡®ä¿æœ€å°‘{self.valves.min_history_messages}æ¡å†å²æ¶ˆæ¯")
                
                # ä½¿ç”¨ä¿®å¤åçš„å†…å®¹æœ€å¤§åŒ–ç»¼åˆå¤„ç†ç­–ç•¥
                final_messages = await self.maximize_content_comprehensive_processing(
                    processed_messages, target_tokens, progress
                )
                
                # æ‰“å°è¯¦ç»†ç»Ÿè®¡
                self.print_detailed_stats()
                
                # ç¡®ä¿è¿”å›çš„æ¶ˆæ¯æ˜¯æ·±æ‹·è´ï¼Œä¸è¿‡åº¦æ¸…ç†
                body["messages"] = copy.deepcopy(final_messages)
                
                # æœ€ç»ˆç»Ÿè®¡
                final_tokens = self.count_messages_tokens(final_messages)
                window_utilization = final_tokens / target_tokens if target_tokens > 0 else 0
                
                print(f"ğŸ”¥ å†…å®¹æœ€å¤§åŒ–å¤„ç†å®Œæˆ [ID:{self.current_processing_id}]")
                print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {len(final_messages)}æ¡æ¶ˆæ¯, {final_tokens:,}tokens")
                print(f"ğŸªŸ çª—å£åˆ©ç”¨ç‡: {window_utilization:.1%}")
                print(f"ğŸ“ˆ å†…å®¹ä¿ç•™ç‡: {self.stats.calculate_retention_ratio():.1%}")
                print(f"ğŸ”‘ å…³é”®å­—ç”Ÿæˆ: {self.stats.keyword_generations}æ¬¡")
                print(f"ğŸ§  ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹: {self.stats.context_maximization_detections}æ¬¡")
                print(f"ğŸ”’ å°½é‡ä¿ç•™: {self.stats.try_preserve_messages}æ¡({self.stats.try_preserve_tokens:,}tokens)")
                print(f"ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†: ç›´æ¥ä¿ç•™{self.stats.context_max_direct_preserve}æ¡, "
                      f"åˆ†ç‰‡{self.stats.context_max_chunked}æ¡, æ‘˜è¦{self.stats.context_max_summarized}æ¡")
                print(f"ğŸ¨ å¤šæ¨¡æ€æå–: {self.stats.multimodal_extracted}ä¸ªå¤šæ¨¡æ€æ¶ˆæ¯")
                print(f"ğŸ§© æ™ºèƒ½åˆ†ç‰‡: åˆ›å»º{self.stats.chunk_created}ä¸ªåˆ†ç‰‡ï¼Œå¤„ç†{self.stats.chunked_messages}æ¡å¤§æ¶ˆæ¯")
                print(f"ğŸ”„ é€’å½’æ‘˜è¦: {self.stats.recursive_summaries}æ¬¡ï¼Œæ‘˜è¦{self.stats.summarized_messages}æ¡æ¶ˆæ¯")
                print(f"ğŸ” å‘é‡æ£€ç´¢: {self.stats.vector_retrievals}æ¬¡")
                print(f"ğŸ”„ é‡æ’åº: {self.stats.rerank_operations}æ¬¡")
                print(f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤: åå¤‡ä¿ç•™{self.stats.fallback_preserve_applied}æ¬¡, "
                      f"ç”¨æˆ·æ¶ˆæ¯æ¢å¤{self.stats.user_message_recovery_count}æ¬¡, RAGæ— ç»“æœ{self.stats.rag_no_results_count}æ¬¡")
                print(f"ğŸ¯ Tokenè®¡ç®—å™¨: ç®€åŒ–ç‰ˆï¼ˆä»…ç”¨tiktokenï¼‰")
                print(f"ğŸ–¼ï¸ å¤šæ¨¡æ€æ¨¡å‹: {self.valves.multimodal_model}")
                print(f"ğŸ“ æ–‡æœ¬å¤„ç†æ¨¡å‹: {self.valves.text_model}")
                
                # éªŒè¯å½“å‰ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦ä¿æŠ¤æˆåŠŸ
                if current_user_message and final_messages:
                    last_msg = final_messages[-1]
                    if last_msg.get("role") == "user":
                        print(f"âœ… å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤æˆåŠŸï¼")
                    else:
                        print(f"âŒ æœ€åä¸€æ¡æ¶ˆæ¯ä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼")
                        print(f"âŒ æœ€åä¸€æ¡æ¶ˆæ¯è§’è‰²: {last_msg.get('role', 'unknown')}")
            else:
                # ç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯
                self.stats.original_tokens = self.count_messages_tokens(messages)
                self.stats.original_messages = len(messages)
                self.stats.final_tokens = processed_tokens
                self.stats.final_messages = len(processed_messages)
                
                # æ£€æŸ¥å½“å‰ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦ä¿ç•™
                final_current_message = self.find_current_user_message(processed_messages)
                self.stats.current_user_preserved = final_current_message is not None
                
                # è®¡ç®—çª—å£ä½¿ç”¨ç‡
                window_usage = self.stats.calculate_window_usage_ratio()
                print(f"ğŸªŸ çª—å£ä½¿ç”¨ç‡: {window_usage:.1%}")
                
                if self.valves.enable_detailed_progress:
                    await progress.complete_phase("æ— éœ€æœ€å¤§åŒ–å¤„ç†")
                
                # ç¡®ä¿è¿”å›çš„æ¶ˆæ¯æ˜¯æ·±æ‹·è´ï¼Œä¸è¿‡åº¦æ¸…ç†
                body["messages"] = copy.deepcopy(processed_messages)
                
                print(f"âœ… ç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯ [ID:{self.current_processing_id}]")
        
        except Exception as e:
            print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            
            if self.valves.enable_detailed_progress:
                await progress.update_status(f"å¤„ç†å¤±è´¥: {str(e)[:50]}", True)
        
        print(f"ğŸ ===== INLET DONE (Content Maximization Only v2.3.1) [ID:{self.current_processing_id}] =====\n")
        return body

    async def outlet(self, body: dict, user: Optional[dict] = None, __event_emitter__: Optional[Callable] = None) -> dict:
        """å‡ºå£å‡½æ•° - è¿”å›å“åº”"""
        return body

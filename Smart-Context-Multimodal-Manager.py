"""
title: ğŸš€ Advanced Context Manager - Zero-Loss Coverage-First v2.4.4
author: JiangNanGenius
version: 2.4.4
license: MIT
required_open_webui_version: 0.5.17
description: å®Œæ•´ä¿®å¤ç‰ˆæœ¬ - æ‰€æœ‰è¯­æ³•é”™è¯¯å·²è§£å†³
"""
import json
import hashlib
import asyncio
import re
import base64
import math
import time
import copy
import html
from typing import Optional, List, Dict, Callable, Any, Tuple, Union
from pydantic import BaseModel, Field
from enum import Enum
from collections import defaultdict

# å¯¼å…¥ä¾èµ–åº“
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    tiktoken = None

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None

class MessageOrder:
    """æ¶ˆæ¯é¡ºåºç®¡ç†å™¨ - IDç¨³å®šåŒ–æ”¹è¿›"""
    def __init__(self, original_messages: List[dict]):
        # ä¸è¦deepcopyï¼Œç›´æ¥åœ¨åŸæ¶ˆæ¯ä¸Šæ‰“æ ‡ç­¾
        self.original_messages = original_messages
        self.order_map = {}  # æ¶ˆæ¯IDåˆ°åŸå§‹ç´¢å¼•çš„æ˜ å°„
        self.message_ids = {}  # åŸå§‹ç´¢å¼•åˆ°æ¶ˆæ¯IDçš„æ˜ å°„
        self.content_map = {}  # å†…å®¹æ ‡è¯†åˆ°åŸå§‹ç´¢å¼•çš„æ˜ å°„
        
        # IDç¨³å®šåŒ–ï¼šä½¿ç”¨å¯é‡ç°çš„hashï¼Œä¸æ··å…¥time.time()
        for i, msg in enumerate(self.original_messages):
            content_key = self._generate_stable_content_key(msg)
            # ä½¿ç”¨ç´¢å¼•+å†…å®¹ç”Ÿæˆç¨³å®šIDï¼Œä¸åŒ…å«æ—¶é—´æˆ³
            msg_id = hashlib.md5(f"{i}_{content_key}".encode()).hexdigest()
            self.order_map[msg_id] = i
            self.message_ids[i] = msg_id
            self.content_map[content_key] = i
            
            # åœ¨æ¶ˆæ¯ä¸­æ·»åŠ é¡ºåºæ ‡è®°ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰
            msg["_order_id"] = msg_id
            msg["_original_index"] = i
            msg["_content_key"] = content_key

    def _generate_stable_content_key(self, msg: dict) -> str:
        """ç”Ÿæˆç¨³å®šçš„æ¶ˆæ¯å†…å®¹æ ‡è¯†ï¼ˆä¸å«æ—¶é—´æˆ³ï¼‰"""
        role = msg.get("role", "")
        content = msg.get("content", "")
        
        # å¤„ç†å¤šæ¨¡æ€å†…å®¹
        if isinstance(content, list):
            content_parts = []
            for item in content:
                if item.get("type") == "text":
                    content_parts.append(item.get("text", "")[:100])  # å–å‰100å­—ç¬¦
                elif item.get("type") == "image_url":
                    image_data = item.get("image_url", {}).get("url", "")
                    if image_data.startswith("data:"):
                        # å¯¹base64å›¾ç‰‡ï¼Œå–header+å‰50å­—ç¬¦ä½œä¸ºç¨³å®šæ ‡è¯†
                        try:
                            header, data = image_data.split("base64,", 1)
                            content_parts.append(f"[IMAGE:{header}:{data[:50]}]")
                        except:
                            content_parts.append("[IMAGE:invalid]")
                    else:
                        content_parts.append(f"[IMAGE:url:{image_data[:50]}]")
            content_str = " ".join(content_parts)
        else:
            content_str = str(content)[:200]  # å–å‰200å­—ç¬¦
        
        return f"{role}:{content_str}"

    def generate_chunk_id(self, msg_id: str, chunk_index: int) -> str:
        """ç”Ÿæˆchunk IDï¼š{msg_id}#k æ ¼å¼ä¿æŒä¸€è‡´"""
        return f"{msg_id}#{chunk_index}"

    def find_current_user_message_index(self, messages: List[dict]) -> int:
        """æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•ï¼ˆæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰"""
        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if msg.get("role") == "user":
                return i
        return -1

    def sort_messages_preserve_user(
        self, messages: List[dict], current_user_message: dict = None
    ) -> List[dict]:
        """
        æ ¹æ®åŸå§‹é¡ºåºæ’åºæ¶ˆæ¯ï¼Œä½†ä¿æŠ¤å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„ä½ç½®
        ç¡®ä¿å½“å‰ç”¨æˆ·æ¶ˆæ¯å§‹ç»ˆåœ¨æœ€å
        """
        if not messages:
            return messages

        # åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå…¶ä»–æ¶ˆæ¯
        other_messages = []
        current_user_in_list = None
        
        for msg in messages:
            if current_user_message and msg.get("_order_id") == current_user_message.get("_order_id"):
                current_user_in_list = msg
            else:
                other_messages.append(msg)

        # æŒ‰åŸå§‹é¡ºåºæ’åºå…¶ä»–æ¶ˆæ¯
        def get_order(msg):
            return msg.get("_original_index", 999999)
        
        other_messages.sort(key=get_order)

        # å¦‚æœæ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼Œå°†å…¶æ”¾åœ¨æœ€å
        if current_user_in_list:
            return other_messages + [current_user_in_list]
        else:
            return other_messages

    def get_message_preview(self, msg: dict) -> str:
        """è·å–æ¶ˆæ¯é¢„è§ˆç”¨äºè°ƒè¯•"""
        if isinstance(msg.get("content"), list):
            text_parts = []
            for item in msg.get("content", []):
                if item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
                elif item.get("type") == "image_url":
                    text_parts.append("[å›¾ç‰‡]")
            content = " ".join(text_parts)
        else:
            content = str(msg.get("content", ""))
        
        # æœ€å°æ¸…ç†
        content = content.replace("\n", " ").replace("\r", " ")
        content = re.sub(r"\s+", " ", content).strip()
        return content[:100] + "..." if len(content) > 100 else content

class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯è®°å½•å™¨ - å¢å¼ºç‰ˆ"""
    def __init__(self):
        # åŸºç¡€ç»Ÿè®¡
        self.original_tokens = 0
        self.original_messages = 0
        self.final_tokens = 0
        self.final_messages = 0
        self.token_limit = 0
        self.target_tokens = 0
        self.current_user_tokens = 0
        
        # å¤„ç†ç»Ÿè®¡
        self.iterations = 0
        self.chunked_messages = 0
        self.summarized_messages = 0
        self.vector_retrievals = 0
        self.rerank_operations = 0
        self.multimodal_processed = 0
        self.processing_time = 0.0
        
        # Coverage-Firstç­–ç•¥ç»Ÿè®¡
        self.coverage_rate = 0.0
        self.coverage_total_messages = 0
        self.coverage_preserved_count = 0
        self.coverage_preserved_tokens = 0
        self.coverage_summary_count = 0
        self.coverage_summary_tokens = 0
        self.coverage_micro_summaries = 0
        self.coverage_block_summaries = 0
        self.coverage_upgrade_count = 0
        self.coverage_upgrade_tokens_saved = 0
        self.coverage_budget_usage = 0.0
        
        # æ–°å¢ï¼šåˆ†å—ä¸é¢„ç®—ç»Ÿè®¡
        self.chunked_messages_count = 0  # è¢«åˆ†ç‰‡çš„æ¶ˆæ¯æ•°
        self.total_chunks_created = 0    # åˆ›å»ºçš„æ€»ç‰‡æ•°
        self.adaptive_blocks_created = 0 # è‡ªé€‚åº”å—æ•°
        self.block_merge_operations = 0  # å—åˆå¹¶æ“ä½œæ•°
        self.budget_scaling_applied = 0  # é¢„ç®—ç¼©æ”¾åº”ç”¨æ¬¡æ•°
        self.scaling_factor = 1.0       # å®é™…ç¼©æ”¾å› å­
        
        # æŠ¤æ ç»Ÿè®¡
        self.guard_a_warnings = 0       # æŠ¤æ Aè­¦å‘Šæ¬¡æ•°
        self.guard_b_fallbacks = 0      # æŠ¤æ Bå›é€€æ¬¡æ•°
        self.id_mapping_errors = 0      # IDæ˜ å°„é”™è¯¯æ¬¡æ•°
        
        # é›¶ä¸¢å¤±ä¿éšœç»Ÿè®¡
        self.zero_loss_guarantee = True
        self.budget_adjustments = 0
        self.min_budget_applied = 0
        self.insurance_truncation_avoided = 0
        
        # Top-upç»Ÿè®¡
        self.topup_applied = 0
        self.topup_micro_upgraded = 0
        self.topup_raw_added = 0
        self.topup_tokens_added = 0
        
        # å…¶ä»–ç»Ÿè®¡ä¿æŒä¸å˜...
        self.preserved_messages = 0
        self.processed_messages = 0
        self.summary_messages = 0
        self.emergency_truncations = 0
        self.content_loss_ratio = 0.0
        self.discarded_messages = 0
        self.recovered_messages = 0
        self.window_utilization = 0.0
        self.try_preserve_tokens = 0
        self.try_preserve_messages = 0
        self.try_preserve_summary_messages = 0
        self.keyword_generations = 0
        self.context_maximization_detections = 0
        self.chunk_created = 0
        self.chunk_processed = 0
        self.recursive_summaries = 0
        self.context_max_direct_preserve = 0
        self.context_max_chunked = 0
        self.context_max_summarized = 0
        self.multimodal_extracted = 0
        self.fallback_preserve_applied = 0
        self.user_message_recovery_count = 0
        self.rag_no_results_count = 0
        self.history_message_separation_count = 0
        self.image_processing_errors = 0
        self.syntax_errors_fixed = 0
        self.truncation_skip_count = 0
        self.truncation_recovered_messages = 0
        self.smart_truncation_applied = 0

    def calculate_retention_ratio(self) -> float:
        """è®¡ç®—å†…å®¹ä¿ç•™æ¯”ä¾‹"""
        if self.original_tokens == 0:
            return 0.0
        return self.final_tokens / self.original_tokens

    def calculate_window_usage_ratio(self) -> float:
        """è®¡ç®—å¯¹è¯çª—å£ä½¿ç”¨ç‡"""
        if self.target_tokens == 0:
            return 0.0
        return self.final_tokens / self.target_tokens

    def get_summary(self) -> str:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        retention = self.calculate_retention_ratio()
        window_usage = self.calculate_window_usage_ratio()
        compression = 1 - retention if retention > 0 else 0
        efficiency = self.final_tokens / self.processing_time if self.processing_time > 0 else 0

        return f"""
ğŸ“Š é›¶ä¸¢å¤±Coverage-First v2.4.4å¤„ç†ç»Ÿè®¡æŠ¥å‘Š:
â”œâ”€ ğŸ“¥ è¾“å…¥: {self.original_messages}æ¡æ¶ˆæ¯, {self.original_tokens:,}tokens
â”œâ”€ ğŸ“¤ è¾“å‡º: {self.final_messages}æ¡æ¶ˆæ¯, {self.final_tokens:,}tokens
â”œâ”€ ğŸ¯ æ¨¡å‹é™åˆ¶: {self.token_limit:,}tokens
â”œâ”€ ğŸªŸ ç›®æ ‡çª—å£: {self.target_tokens:,}tokens
â”œâ”€ ğŸ‘¤ å½“å‰ç”¨æˆ·: {self.current_user_tokens:,}tokens
â”œâ”€ ğŸ“ˆ å†…å®¹ä¿ç•™ç‡: {retention:.2%}
â”œâ”€ ğŸªŸ çª—å£ä½¿ç”¨ç‡: {window_usage:.2%}
â”œâ”€ ğŸ“‰ å‹ç¼©æ¯”ä¾‹: {compression:.2%}
â”œâ”€ âš¡ å¤„ç†æ•ˆç‡: {efficiency:.0f}tokens/s
â”œâ”€ ğŸ§© åˆ†ç‰‡ç»Ÿè®¡:
â”‚   â”œâ”€ è¢«åˆ†ç‰‡æ¶ˆæ¯: {self.chunked_messages_count}æ¡
â”‚   â”œâ”€ åˆ›å»ºæ€»ç‰‡æ•°: {self.total_chunks_created}ä¸ª
â”‚   â”œâ”€ è‡ªé€‚åº”å—æ•°: {self.adaptive_blocks_created}å—
â”‚   â””â”€ å—åˆå¹¶æ“ä½œ: {self.block_merge_operations}æ¬¡
â”œâ”€ ğŸ’° é¢„ç®—ç¼©æ”¾:
â”‚   â”œâ”€ ç¼©æ”¾åº”ç”¨: {self.budget_scaling_applied}æ¬¡
â”‚   â”œâ”€ ç¼©æ”¾å› å­: {self.scaling_factor:.3f}
â”‚   â””â”€ é¢„ç®—è°ƒæ•´: {self.budget_adjustments}è½®
â”œâ”€ ğŸ¯ Coverageç­–ç•¥ç»Ÿè®¡:
â”‚   â”œâ”€ ğŸ“Š è¦†ç›–ç‡: {self.coverage_rate:.1%} ({self.coverage_total_messages}æ¡å†å²æ¶ˆæ¯)
â”‚   â”œâ”€ ğŸ“ åŸæ–‡ä¿ç•™: {self.coverage_preserved_count}æ¡ ({self.coverage_preserved_tokens:,}tokens)
â”‚   â”œâ”€ ğŸ“„ æ‘˜è¦æ›¿èº«: {self.coverage_summary_count}æ¡ ({self.coverage_summary_tokens:,}tokens)
â”‚   â”œâ”€ ğŸ” å¾®æ‘˜è¦: {self.coverage_micro_summaries}æ¡
â”‚   â”œâ”€ ğŸ“š å—æ‘˜è¦: {self.coverage_block_summaries}å—
â”‚   â”œâ”€ â¬†ï¸ å‡çº§æˆåŠŸ: {self.coverage_upgrade_count}æ¡ (èŠ‚çº¦{self.coverage_upgrade_tokens_saved:,}tokens)
â”‚   â””â”€ ğŸ’° é¢„ç®—ä½¿ç”¨: {self.coverage_budget_usage:.1%}
â”œâ”€ ğŸ”¥ Top-upå¡«å……:
â”‚   â”œâ”€ å¡«å……åº”ç”¨: {self.topup_applied}æ¬¡
â”‚   â”œâ”€ å¾®æ‘˜è¦å‡çº§: {self.topup_micro_upgraded}æ¡
â”‚   â”œâ”€ åŸæ–‡æ·»åŠ : {self.topup_raw_added}æ¡
â”‚   â””â”€ æ–°å¢tokens: {self.topup_tokens_added:,}
â”œâ”€ ğŸ›¡ï¸ åŒé‡æŠ¤æ :
â”‚   â”œâ”€ æŠ¤æ Aè­¦å‘Š: {self.guard_a_warnings}æ¬¡
â”‚   â”œâ”€ æŠ¤æ Bå›é€€: {self.guard_b_fallbacks}æ¬¡
â”‚   â””â”€ IDæ˜ å°„é”™è¯¯: {self.id_mapping_errors}æ¬¡
â”œâ”€ ğŸ›¡ï¸ é›¶ä¸¢å¤±ä¿éšœ:
â”‚   â”œâ”€ âœ… é›¶ä¸¢å¤±å®ç°: {'æ˜¯' if self.zero_loss_guarantee else 'å¦'}
â”‚   â”œâ”€ ğŸ”§ é¢„ç®—è°ƒæ•´: {self.budget_adjustments}æ¬¡
â”‚   â”œâ”€ ğŸ“ æœ€å°é¢„ç®—åº”ç”¨: {self.min_budget_applied}æ¬¡
â”‚   â””â”€ ğŸš« é¿å…ä¿é™©æˆªæ–­: {self.insurance_truncation_avoided}æ¬¡
â”œâ”€ ğŸ–¼ï¸ å¤šæ¨¡æ€å¤„ç†: {self.multimodal_processed}å¼ å›¾ç‰‡
â”œâ”€ ğŸ”‘ å…³é”®å­—ç”Ÿæˆ: {self.keyword_generations}æ¬¡
â”œâ”€ ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹: {self.context_maximization_detections}æ¬¡
â”œâ”€ ğŸ” å‘é‡æ£€ç´¢: {self.vector_retrievals}æ¬¡
â”œâ”€ ğŸ”„ é‡æ’åº: {self.rerank_operations}æ¬¡
â”œâ”€ âœ‚ï¸ æ™ºèƒ½æˆªæ–­: åº”ç”¨{self.smart_truncation_applied}æ¬¡, è·³è¿‡{self.truncation_skip_count}æ¡, æ¢å¤{self.truncation_recovered_messages}æ¡
â”œâ”€ ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤: åå¤‡ä¿ç•™{self.fallback_preserve_applied}æ¬¡, ç”¨æˆ·æ¶ˆæ¯æ¢å¤{self.user_message_recovery_count}æ¬¡
â”œâ”€ ğŸ” RAGæ— ç»“æœ: {self.rag_no_results_count}æ¬¡
â”œâ”€ ğŸ“‹ å†å²æ¶ˆæ¯åˆ†ç¦»: {self.history_message_separation_count}æ¬¡
â”œâ”€ ğŸ–¼ï¸ å›¾ç‰‡å¤„ç†é”™è¯¯: {self.image_processing_errors}æ¬¡
â”œâ”€ ğŸ”§ è¯­æ³•é”™è¯¯ä¿®å¤: {self.syntax_errors_fixed}æ¬¡
â””â”€ â±ï¸ å¤„ç†æ—¶é—´: {self.processing_time:.2f}ç§’"""

class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨"""
    def __init__(self, event_emitter):
        self.event_emitter = event_emitter
        self.current_step = 0
        self.total_steps = 0
        self.current_phase = ""
        self.phase_progress = 0
        self.phase_total = 0
        self.logged_phases = set()

    def create_progress_bar(self, percentage: float, width: int = 15) -> str:
        """åˆ›å»ºç¾è§‚çš„è¿›åº¦æ¡"""
        filled = int(percentage * width / 100)
        if percentage >= 100:
            bar = "â–ˆ" * width
        else:
            bar = "â–ˆ" * filled + "â–“" * max(0, 1) + "â–‘" * max(0, width - filled - 1)
        return f"[{bar}] {percentage:.1f}%"

    async def start_phase(self, phase_name: str, total_items: int = 0):
        """å¼€å§‹æ–°é˜¶æ®µ"""
        self.current_phase = phase_name
        self.phase_progress = 0
        self.phase_total = total_items
        self.logged_phases.add(phase_name)
        await self.update_status(f"ğŸš€ å¼€å§‹ {phase_name}")

    async def update_progress(self, completed: int, total: int = None, detail: str = ""):
        """æ›´æ–°è¿›åº¦"""
        if total is None:
            total = self.phase_total
        self.phase_progress = completed
        
        if total > 0:
            percentage = (completed / total) * 100
            progress_bar = self.create_progress_bar(percentage)
            status = f"ğŸ”„ {self.current_phase} {progress_bar} ({completed}/{total})"
            if detail:
                status += f" - {detail}"
        else:
            status = f"ğŸ”„ {self.current_phase}"
            if detail:
                status += f" - {detail}"
        
        await self.update_status(status, False)

    async def complete_phase(self, message: str = ""):
        """å®Œæˆå½“å‰é˜¶æ®µ"""
        final_message = f"âœ… {self.current_phase} å®Œæˆ"
        if message:
            final_message += f" - {message}"
        await self.update_status(final_message, True)

    async def update_status(self, message: str, done: bool = False):
        """æ›´æ–°çŠ¶æ€"""
        if self.event_emitter:
            try:
                # åŸºæœ¬æ¸…ç†
                message = message.replace("\n", " ").replace("\r", " ")
                message = re.sub(r"\s+", " ", message).strip()
                await self.event_emitter({
                    "type": "status",
                    "data": {"description": message, "done": done},
                })
            except Exception as e:
                if str(e) not in self.logged_phases:
                    print(f"âš ï¸ è¿›åº¦æ›´æ–°å¤±è´¥: {e}")
                    self.logged_phases.add(str(e))

class ModelMatcher:
    """æ™ºèƒ½æ¨¡å‹åŒ¹é…å™¨ - æ–°å¢GPT-5ç³»åˆ—æ”¯æŒ"""
    def __init__(self):
        # ç²¾ç¡®åŒ¹é…è§„åˆ™ï¼ˆæ–°å¢GPT-5ç³»åˆ—ï¼‰
        self.exact_matches = {
            # GPT-5ç³»åˆ—ï¼ˆå…¨æ–°æ”¯æŒï¼‰
            "gpt-5": {"family": "gpt", "multimodal": True, "limit": 200000},
            "gpt-5-mini": {"family": "gpt", "multimodal": True, "limit": 200000},
            "gpt-5-nano": {"family": "gpt", "multimodal": True, "limit": 200000},
            # GPT-4ç³»åˆ—ï¼ˆä¿ç•™åŸæœ‰ï¼‰
            "gpt-4o": {"family": "gpt", "multimodal": True, "limit": 128000},
            "gpt-4o-mini": {"family": "gpt", "multimodal": True, "limit": 128000},
            "gpt-4": {"family": "gpt", "multimodal": False, "limit": 8192},
            "gpt-4-turbo": {"family": "gpt", "multimodal": True, "limit": 128000},
            "gpt-4-vision-preview": {"family": "gpt", "multimodal": True, "limit": 128000},
            "gpt-3.5-turbo": {"family": "gpt", "multimodal": False, "limit": 16385},
            # Claudeç³»åˆ—
            "claude-3-5-sonnet": {"family": "claude", "multimodal": True, "limit": 200000},
            "claude-3-opus": {"family": "claude", "multimodal": True, "limit": 200000},
            "claude-3-haiku": {"family": "claude", "multimodal": True, "limit": 200000},
            "claude-3": {"family": "claude", "multimodal": True, "limit": 200000},
            "anthropic.claude-4-sonnet-latest-extended-thinking": {
                "family": "claude", "multimodal": True, "limit": 200000, "special": "thinking"
            },
            # Doubaoç³»åˆ—
            "doubao-1.5-vision-pro": {"family": "doubao", "multimodal": True, "limit": 128000},
            "doubao-1.5-vision-lite": {"family": "doubao", "multimodal": True, "limit": 128000},
            "doubao-1.5-thinking-pro": {"family": "doubao", "multimodal": False, "limit": 128000, "special": "thinking"},
            "doubao-seed-1-6-250615": {"family": "doubao", "multimodal": True, "limit": 50000},
            "doubao-seed": {"family": "doubao", "multimodal": False, "limit": 50000},
            "doubao": {"family": "doubao", "multimodal": False, "limit": 50000},
            "doubao-1-5-pro-256k": {"family": "doubao", "multimodal": False, "limit": 200000},
            # Geminiç³»åˆ—
            "gemini-pro": {"family": "gemini", "multimodal": False, "limit": 128000},
            "gemini-pro-vision": {"family": "gemini", "multimodal": True, "limit": 128000},
            # Qwenç³»åˆ—
            "qwen-vl": {"family": "qwen", "multimodal": True, "limit": 32000},
        }

        # æ¨¡ç³ŠåŒ¹é…è§„åˆ™ï¼ˆæ–°å¢GPT-5æ¨¡å¼ï¼‰
        self.fuzzy_patterns = [
            # Thinkingæ¨¡å‹ä¼˜å…ˆåŒ¹é…ï¼ˆé¿å…è¯¯åŒ¹é…ï¼‰
            {"pattern": r".*thinking.*", "family": "thinking", "multimodal": False, "limit": 200000, "special": "thinking"},
            # GPT-5ç³»åˆ—æ¨¡ç³ŠåŒ¹é…ï¼ˆå…¨æ–°ï¼‰
            {"pattern": r"gpt-5.*nano.*", "family": "gpt", "multimodal": True, "limit": 200000},
            {"pattern": r"gpt-5.*mini.*", "family": "gpt", "multimodal": True, "limit": 200000},
            {"pattern": r"gpt-5.*", "family": "gpt", "multimodal": True, "limit": 200000},
            # GPT-4ç³»åˆ—æ¨¡ç³ŠåŒ¹é…ï¼ˆä¿ç•™ï¼‰
            {"pattern": r"gpt-4o.*", "family": "gpt", "multimodal": True, "limit": 128000},
            {"pattern": r"gpt-4.*vision.*", "family": "gpt", "multimodal": True, "limit": 128000},
            {"pattern": r"gpt-4.*turbo.*", "family": "gpt", "multimodal": True, "limit": 128000},
            {"pattern": r"gpt-4.*", "family": "gpt", "multimodal": False, "limit": 8192},
            {"pattern": r"gpt-3\.5.*", "family": "gpt", "multimodal": False, "limit": 16385},
            {"pattern": r"gpt.*", "family": "gpt", "multimodal": False, "limit": 16385},
            # Claudeç³»åˆ—æ¨¡ç³ŠåŒ¹é…
            {"pattern": r"claude.*3.*5.*sonnet.*", "family": "claude", "multimodal": True, "limit": 200000},
            {"pattern": r"claude.*3.*opus.*", "family": "claude", "multimodal": True, "limit": 200000},
            {"pattern": r"claude.*3.*haiku.*", "family": "claude", "multimodal": True, "limit": 200000},
            {"pattern": r"claude.*3.*", "family": "claude", "multimodal": True, "limit": 200000},
            {"pattern": r"claude.*", "family": "claude", "multimodal": True, "limit": 200000},
            {"pattern": r"anthropic.*claude.*", "family": "claude", "multimodal": True, "limit": 200000},
            # Doubaoç³»åˆ—æ¨¡ç³ŠåŒ¹é…
            {"pattern": r"doubao.*vision.*", "family": "doubao", "multimodal": True, "limit": 128000},
            {"pattern": r"doubao.*seed.*", "family": "doubao", "multimodal": True, "limit": 50000},
            {"pattern": r"doubao.*256k.*", "family": "doubao", "multimodal": False, "limit": 200000},
            {"pattern": r"doubao.*1\.5.*", "family": "doubao", "multimodal": False, "limit": 128000},
            {"pattern": r"doubao.*", "family": "doubao", "multimodal": False, "limit": 50000},
            # Geminiç³»åˆ—æ¨¡ç³ŠåŒ¹é…
            {"pattern": r"gemini.*vision.*", "family": "gemini", "multimodal": True, "limit": 128000},
            {"pattern": r"gemini.*pro.*", "family": "gemini", "multimodal": False, "limit": 128000},
            {"pattern": r"gemini.*", "family": "gemini", "multimodal": False, "limit": 128000},
            # Qwenç³»åˆ—æ¨¡ç³ŠåŒ¹é…
            {"pattern": r"qwen.*vl.*", "family": "qwen", "multimodal": True, "limit": 32000},
            {"pattern": r"qwen.*", "family": "qwen", "multimodal": False, "limit": 32000},
        ]

    def match_model(self, model_name: str) -> Dict[str, Any]:
        """æ™ºèƒ½åŒ¹é…æ¨¡å‹ä¿¡æ¯"""
        if not model_name:
            return {"family": "unknown", "multimodal": False, "limit": 200000}
        
        model_lower = model_name.lower().strip()
        
        # ç²¾ç¡®åŒ¹é…
        for exact_name, info in self.exact_matches.items():
            if exact_name.lower() == model_lower:
                return {**info, "matched_name": exact_name, "match_type": "exact"}
        
        # æ¨¡ç³ŠåŒ¹é…
        for pattern_info in self.fuzzy_patterns:
            pattern = pattern_info["pattern"]
            if re.match(pattern, model_lower):
                return {
                    "family": pattern_info["family"],
                    "multimodal": pattern_info["multimodal"],
                    "limit": pattern_info["limit"],
                    "special": pattern_info.get("special"),
                    "matched_pattern": pattern,
                    "match_type": "fuzzy"
                }
        
        # é»˜è®¤åŒ¹é…
        return {"family": "unknown", "multimodal": False, "limit": 200000, "match_type": "default"}

class TokenCalculator:
    """Tokenè®¡ç®—å™¨"""
    def __init__(self):
        self._encoding = None

    def get_encoding(self):
        """è·å–tiktokenç¼–ç å™¨"""
        if not TIKTOKEN_AVAILABLE:
            return None
        if self._encoding is None:
            try:
                self._encoding = tiktoken.get_encoding("cl100k_base")
            except Exception:
                pass
        return self._encoding

    def count_tokens(self, text: str) -> int:
        """ç®€åŒ–çš„tokenè®¡ç®—"""
        if not text:
            return 0
        
        encoding = self.get_encoding()
        if encoding:
            try:
                return len(encoding.encode(str(text)))
            except Exception:
                pass
        
        # ç®€å•fallback
        return len(str(text)) // 4

    def calculate_image_tokens(self, image_data: str) -> int:
        """å›¾ç‰‡tokenè®¡ç®—"""
        if not image_data:
            return 0
        return 1500  # æ¯ä¸ªå›¾ç‰‡æŒ‰1500tokensè®¡ç®—

class InputCleaner:
    """è¾“å…¥æ¸…æ´—ä¸ä¸¥æ ¼å…œåº• - ä¿®å¤è¯­æ³•é”™è¯¯ç‰ˆæœ¬"""
    @staticmethod
    def clean_text_for_regex(text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬ç”¨äºæ­£åˆ™è¡¨è¾¾å¼ï¼Œé˜²æ­¢è¯­æ³•é”™è¯¯"""
        if not text:
            return ""
        try:
            # ç§»é™¤ä¸å¯è§åˆ†éš”ç¬¦
            text = text.replace('\u2028', ' ').replace('\u2029', ' ')
            # ç§»é™¤å…¶ä»–æ§åˆ¶å­—ç¬¦
            text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
            # åŸºæœ¬æ¸…ç†
            text = text.replace("\n", " ").replace("\r", " ")
            text = re.sub(r"\s+", " ", text).strip()
            return text
        except Exception as e:
            print(f"âš ï¸ æ–‡æœ¬æ¸…ç†å¼‚å¸¸: {str(e)[:100]}")
            # æç«¯å…œåº•ï¼šåªä¿ç•™åŸºæœ¬å­—ç¬¦
            return ''.join(c for c in str(text) if c.isprintable() or c.isspace())[:1000]

    @staticmethod
    def validate_and_clean_data_uri(data_uri: str) -> Tuple[bool, str]:
        """éªŒè¯å¹¶æ¸…æ´— data URIï¼Œè¿”å› (æ˜¯å¦æœ‰æ•ˆ, æ¸…æ´—åçš„URI) - ä¿®å¤å¼•å·è¯­æ³•é”™è¯¯"""
        if not data_uri or not isinstance(data_uri, str):
            return False, ""
        
        try:
            # å¿…é¡»ä»¥ data: å¼€å¤´ï¼Œä¸”åŒ…å« base64,
            if not data_uri.startswith("data:"):
                return False, ""
            if "base64," not in data_uri:
                return False, ""
            
            header, b64 = data_uri.split("base64,", 1)
            
            # åªæ¥å—å›¾ç‰‡ MIME
            if not header.lower().startswith("data:image/"):
                return False, ""
            
            # å»ç©ºç™½
            b64_str = re.sub(r"\s+", "", b64)
            
            # å¤ªçŸ­åŸºæœ¬ä¸å¯èƒ½æ˜¯æœ‰æ•ˆå›¾ç‰‡
            if len(b64_str) < 100:
                return False, ""
            
            # æ ¡éªŒå‰100å­—ç¬¦ï¼ˆè¡¥é½ = é¿å… padding é”™ï¼‰
            head = b64_str[:100]
            pad_len = (-len(head)) % 4
            try:
                base64.b64decode(head + ("=" * pad_len), validate=True)
            except Exception:
                return False, ""
            
            # è¿”å›æ¸…æ´—åçš„ data uri
            return True, f"{header}base64,{b64_str}"
            
        except Exception as e:
            print(f"âš ï¸ Data URIéªŒè¯å¼‚å¸¸: {str(e)[:100]}")
            return False, ""

    @staticmethod
    def safe_regex_match(pattern: str, text: str) -> bool:
        """å®‰å…¨çš„æ­£åˆ™åŒ¹é…ï¼Œé˜²æ­¢è¯­æ³•é”™è¯¯"""
        try:
            cleaned_text = InputCleaner.clean_text_for_regex(text)
            return bool(re.match(pattern, cleaned_text))
        except Exception as e:
            print(f"âš ï¸ æ­£åˆ™åŒ¹é…å¼‚å¸¸: {str(e)[:100]}")
            return False

class MessageChunker:
    """å•æ¡æ¶ˆæ¯å†…åˆ†ç‰‡å¤„ç†å™¨"""
    def __init__(self, token_calculator: TokenCalculator, valves):
        self.token_calculator = token_calculator
        self.valves = valves

    def should_chunk_message(self, message: dict) -> bool:
        """åˆ¤æ–­æ¶ˆæ¯æ˜¯å¦éœ€è¦åˆ†ç‰‡"""
        tokens = self.token_calculator.count_tokens(
            self.extract_text_content(message)
        )
        return tokens > self.valves.large_message_threshold

    def extract_text_content(self, message: dict) -> str:
        """ä»æ¶ˆæ¯ä¸­æå–æ–‡æœ¬å†…å®¹"""
        content = message.get("content", "")
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
                elif item.get("type") == "image_url":
                    text_parts.append("[å›¾ç‰‡]")  # å ä½ç¬¦
            return " ".join(text_parts)
        else:
            return str(content)

    def chunk_single_message(self, message: dict, message_order: MessageOrder) -> List[dict]:
        """å¯¹å•æ¡æ¶ˆæ¯è¿›è¡Œåˆ†ç‰‡å¤„ç†"""
        content_text = self.extract_text_content(message)
        if not self.should_chunk_message(message):
            return [message]  # ä¸éœ€è¦åˆ†ç‰‡

        print(f"ğŸ§© å¼€å§‹åˆ†ç‰‡å¤„ç†: æ¶ˆæ¯é•¿åº¦ {len(content_text)} å­—ç¬¦")

        # åˆ†ç‰‡ç­–ç•¥ï¼šä¿æŒä»£ç å—/æ®µè½/å¥å­å®Œæ•´
        chunks = self._intelligent_chunk_text(content_text)
        if len(chunks) <= 1:
            return [message]  # åˆ†ç‰‡ååªæœ‰ä¸€ä¸ªï¼Œç›´æ¥è¿”å›åŸæ¶ˆæ¯

        # åˆ›å»ºåˆ†ç‰‡æ¶ˆæ¯
        chunked_messages = []
        msg_id = message.get("_order_id", "unknown")
        
        for i, chunk_text in enumerate(chunks):
            chunk_id = message_order.generate_chunk_id(msg_id, i)
            chunk_message = copy.deepcopy(message)
            chunk_message["content"] = chunk_text
            chunk_message["_order_id"] = chunk_id
            chunk_message["_is_chunk"] = True
            chunk_message["_parent_msg_id"] = msg_id
            chunk_message["_chunk_index"] = i
            chunk_message["_total_chunks"] = len(chunks)
            chunked_messages.append(chunk_message)

        print(f"ğŸ§© åˆ†ç‰‡å®Œæˆ: 1æ¡æ¶ˆæ¯ -> {len(chunked_messages)}ç‰‡")
        return chunked_messages

    def _intelligent_chunk_text(self, text: str) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†ç‰‡ï¼šä¿æŒå®Œæ•´æ€§ - ä¿®å¤æ¢è¡ŒæŠ¹æ‰é—®é¢˜"""
        if not text:
            return [text]

        # ä¿ç•™æ¢è¡Œç”¨äºæ®µè½åˆ‡åˆ†ï¼šä»…å»æ§åˆ¶å­—ç¬¦ï¼Œå¹¶æŠŠåˆ†éš”ç¬¦æ­£è§„åŒ–æˆæ¢è¡Œ
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        text = text.replace('\u2028', '\n').replace('\u2029', '\n')

        # åŸºæœ¬å‚æ•°
        target_size = self.valves.chunk_target_tokens * 4  # ç²—ç•¥å­—ç¬¦æ•°
        min_size = self.valves.chunk_min_tokens * 4
        max_size = self.valves.chunk_max_tokens * 4
        overlap_size = self.valves.chunk_overlap_tokens * 4

        chunks = []
        current_chunk = ""

        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text)
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            # å¦‚æœå½“å‰æ®µè½åŠ å…¥åè¶…è¿‡ç›®æ ‡å¤§å°
            if len(current_chunk) + len(paragraph) > target_size and current_chunk:
                # å®Œæˆå½“å‰chunk
                if len(current_chunk) >= min_size:
                    chunks.append(current_chunk.strip())
                    # æ·»åŠ é‡å å†…å®¹
                    if self.valves.chunk_overlap_tokens > 0:
                        overlap_text = current_chunk[-overlap_size:] if len(current_chunk) > overlap_size else current_chunk
                        current_chunk = overlap_text + "\n\n" + paragraph
                    else:
                        current_chunk = paragraph
                else:
                    current_chunk += "\n\n" + paragraph
            else:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph

            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œéœ€è¦å¥å­çº§åˆ‡åˆ†
            if len(current_chunk) > max_size:
                # ä¿å­˜å½“å‰chunk
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""

        # å¤„ç†æœ€åä¸€ä¸ªchunk
        if current_chunk and len(current_chunk.strip()) >= min_size // 2:  # æœ€åä¸€å—å¯ä»¥ç¨çŸ­
            chunks.append(current_chunk.strip())
        elif current_chunk and chunks:
            # å¤ªçŸ­äº†ï¼Œåˆå¹¶åˆ°æœ€åä¸€ä¸ªchunk
            chunks[-1] += "\n\n" + current_chunk.strip()
        elif current_chunk:
            chunks.append(current_chunk.strip())

        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªchunk
        if not chunks and text:
            chunks = [text]

        return chunks

    def preprocess_messages_with_chunking(
        self, messages: List[dict], message_order: MessageOrder
    ) -> List[dict]:
        """é¢„å¤„ç†æ¶ˆæ¯ï¼šå¯¹å¤§æ¶ˆæ¯è¿›è¡Œåˆ†ç‰‡"""
        processed_messages = []
        chunked_count = 0
        total_chunks = 0

        for message in messages:
            if self.should_chunk_message(message):
                chunked_messages = self.chunk_single_message(message, message_order)
                processed_messages.extend(chunked_messages)
                if len(chunked_messages) > 1:
                    chunked_count += 1
                    total_chunks += len(chunked_messages)
            else:
                processed_messages.append(message)

        if chunked_count > 0:
            print(f"ğŸ§© æ¶ˆæ¯åˆ†ç‰‡é¢„å¤„ç†å®Œæˆ: {chunked_count}æ¡æ¶ˆæ¯ -> {total_chunks}ç‰‡")

        return processed_messages

class CoveragePlanner:
    """Coverageè®¡åˆ’å™¨ - é‡æ„ç‰ˆï¼Œå®ç°è‡ªé€‚åº”åˆ†å—å’Œä¸€æ¬¡æ€§ç¼©æ”¾"""
    def __init__(self, token_calculator: TokenCalculator, valves):
        self.token_calculator = token_calculator
        self.valves = valves

    def plan_adaptive_coverage_summaries(
        self, scored_msgs: List[dict], total_budget: int
    ) -> Tuple[List[dict], int]:
        """è§„åˆ’è‡ªé€‚åº”è¦†ç›–æ‘˜è¦ï¼šæŒ‰åŸæ–‡tokené‡è‡ªé€‚åº”åˆ†å— + ä¸€æ¬¡æ€§æ¯”ä¾‹ç¼©æ”¾"""
        if not scored_msgs:
            return [], 0

        print(f"ğŸ“„ å¼€å§‹è‡ªé€‚åº”Coverageè§„åˆ’: {len(scored_msgs)}æ¡æ¶ˆæ¯ï¼Œé¢„ç®—{total_budget:,}tokens")

        # 1. æŒ‰åˆ†æ•°åˆ†æ¡£
        HIGH, MID, LOW = self._classify_messages_by_score(scored_msgs)
        print(f"ğŸ“„ åˆ†æ¡£ç»“æœ: é«˜æƒé‡{len(HIGH)}æ¡, ä¸­æƒé‡{len(MID)}æ¡, ä½æƒé‡{len(LOW)}æ¡")

        # 2. å¯¹ä½æƒé‡æ¶ˆæ¯è¿›è¡Œè‡ªé€‚åº”åˆ†å—
        adaptive_blocks = self._create_adaptive_blocks(LOW)
        print(f"ğŸ“„ è‡ªé€‚åº”åˆ†å—: {len(LOW)}æ¡ä½æƒé‡æ¶ˆæ¯ -> {len(adaptive_blocks)}ä¸ªè‡ªé€‚åº”å—")

        # 3. è®¡ç®—ç†æƒ³é¢„ç®—éœ€æ±‚
        entries, ideal_total_cost = self._calculate_ideal_budgets(HIGH, MID, adaptive_blocks)

        # 4. ä¸€æ¬¡æ€§æ¯”ä¾‹ç¼©æ”¾ï¼ˆå¦‚æœé¢„ç®—ä¸è¶³ï¼‰æˆ–å‘ä¸Šæ‰©å¼ ï¼ˆå¦‚æœé¢„ç®—å……è¶³ï¼‰
        if ideal_total_cost > total_budget:
            entries, actual_cost = self._apply_proportional_scaling(entries, total_budget)
            print(f"ğŸ“„ ä¸€æ¬¡æ€§æ¯”ä¾‹ç¼©æ”¾: {ideal_total_cost:,} -> {actual_cost:,} tokens")
        else:
            # å‘ä¸Šæ‰©å¼ æ¨¡å¼
            entries, actual_cost = self._apply_upward_expansion(entries, total_budget, ideal_total_cost)
            print(f"ğŸ“„ å‘ä¸Šæ‰©å¼ æ¨¡å¼: {ideal_total_cost:,} -> {actual_cost:,} tokens")

        # 5. æç«¯é€€åŒ–å¤„ç†
        if actual_cost > total_budget * 1.1:  # å…è®¸10%çš„å®¹å·®
            entries, actual_cost = self._apply_extreme_fallback(scored_msgs, total_budget)
            print(f"ğŸ“„ æç«¯é€€åŒ–å¤„ç†: ä½¿ç”¨å•æ¡å…¨å±€å—æ‘˜è¦")

        print(f"ğŸ“„ è‡ªé€‚åº”è§„åˆ’å®Œæˆ: {len(entries)}ä¸ªæ¡ç›®ï¼Œæˆæœ¬{actual_cost:,}tokens")
        return entries, actual_cost

    def _classify_messages_by_score(self, scored_msgs: List[dict]) -> Tuple[List[dict], List[dict], List[dict]]:
        """æŒ‰åˆ†æ•°åˆ†æ¡£æ¶ˆæ¯"""
        HIGH, MID, LOW = [], [], []
        for item in scored_msgs:
            if item["score"] >= self.valves.coverage_high_score_threshold:
                HIGH.append(item)
            elif item["score"] >= self.valves.coverage_mid_score_threshold:
                MID.append(item)
            else:
                LOW.append(item)
        return HIGH, MID, LOW

    def _create_adaptive_blocks(self, low_messages: List[dict]) -> List[dict]:
        """æŒ‰åŸæ–‡tokené‡è‡ªé€‚åº”åˆ†å—"""
        if not low_messages:
            return []

        # æŒ‰åŸå§‹ç´¢å¼•æ’åº
        low_sorted = sorted(low_messages, key=lambda x: x["idx"])

        blocks = []
        current_block = []
        current_tokens = 0
        raw_block_target = self.valves.raw_block_target  # 15k tokensç›®æ ‡

        for item in low_sorted:
            msg_tokens = item["tokens"]

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ‡å—
            should_cut_block = False

            # æ¡ä»¶1ï¼šç´¯è®¡tokensè¾¾åˆ°ç›®æ ‡
            if current_tokens + msg_tokens > raw_block_target and current_block:
                should_cut_block = True

            # æ¡ä»¶2ï¼šæ—¶é—´å¤§è·³ï¼ˆç›¸é‚»æ¶ˆæ¯ç´¢å¼•å·®è·å¤§ï¼‰
            if current_block and abs(item["idx"] - current_block[-1]["idx"]) > 5:
                should_cut_block = True

            # æ¡ä»¶3ï¼šè§’è‰²åˆ‡æ¢ï¼ˆç”¨æˆ·->åŠ©æ‰‹æˆ–åŠ©æ‰‹->ç”¨æˆ·ï¼‰
            if current_block:
                prev_role = current_block[-1]["msg"].get("role", "")
                curr_role = item["msg"].get("role", "")
                if prev_role != curr_role and prev_role in ["user", "assistant"] and curr_role in ["user", "assistant"]:
                    should_cut_block = True

            # æ¡ä»¶4ï¼šç›¸ä¼¼åº¦çªå˜ï¼ˆåˆ†æ•°å·®å¼‚è¿‡å¤§ï¼‰
            if current_block:
                score_diff = abs(item["score"] - current_block[-1]["score"])
                if score_diff > 0.3:  # åˆ†æ•°å·®å¼‚è¶…è¿‡0.3å°±åˆ‡å—
                    should_cut_block = True

            # æ‰§è¡Œåˆ‡å—
            if should_cut_block:
                # å®Œæˆå½“å‰å—
                if current_block:
                    blocks.append({
                        "type": "adaptive_block",
                        "idx_range": (current_block[0]["idx"], current_block[-1]["idx"]),
                        "msgs": [item["msg"] for item in current_block],
                        "raw_tokens": current_tokens,
                        "avg_score": sum(item["score"] for item in current_block) / len(current_block),
                        "msg_count": len(current_block)
                    })
                # å¼€å§‹æ–°å—
                current_block = [item]
                current_tokens = msg_tokens
            else:
                # åŠ å…¥å½“å‰å—
                current_block.append(item)
                current_tokens += msg_tokens

        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_block:
            blocks.append({
                "type": "adaptive_block",
                "idx_range": (current_block[0]["idx"], current_block[-1]["idx"]),
                "msgs": [item["msg"] for item in current_block],
                "raw_tokens": current_tokens,
                "avg_score": sum(item["score"] for item in current_block) / len(current_block),
                "msg_count": len(current_block)
            })

        # å—åˆå¹¶ï¼šå¦‚æœå—æ•°å¤ªå¤šï¼Œåˆå¹¶å°å—
        if len(blocks) > self.valves.max_blocks:
            blocks = self._merge_small_blocks(blocks)

        return blocks

    def _merge_small_blocks(self, blocks: List[dict]) -> List[dict]:
        """åˆå¹¶å°å—ï¼Œæ§åˆ¶æ€»å—æ•°"""
        if len(blocks) <= self.valves.max_blocks:
            return blocks

        # æŒ‰raw_tokensæ’åºï¼Œä¼˜å…ˆåˆå¹¶å°å—
        blocks.sort(key=lambda x: x["raw_tokens"])

        merged_blocks = []
        i = 0
        while i < len(blocks):
            current_block = blocks[i]
            # å°è¯•ä¸ä¸‹ä¸€ä¸ªå—åˆå¹¶
            if i + 1 < len(blocks) and len(merged_blocks) + (len(blocks) - i) > self.valves.max_blocks:
                next_block = blocks[i + 1]
                # åˆå¹¶æ¡ä»¶ï¼šæ€»tokenæ•°ä¸è¶…è¿‡2å€ç›®æ ‡
                if current_block["raw_tokens"] + next_block["raw_tokens"] <= self.valves.raw_block_target * 2:
                    # æ‰§è¡Œåˆå¹¶
                    merged_block = {
                        "type": "adaptive_block",
                        "idx_range": (current_block["idx_range"][0], next_block["idx_range"][1]),
                        "msgs": current_block["msgs"] + next_block["msgs"],
                        "raw_tokens": current_block["raw_tokens"] + next_block["raw_tokens"],
                        "avg_score": (current_block["avg_score"] * current_block["msg_count"] +
                                    next_block["avg_score"] * next_block["msg_count"]) /
                                   (current_block["msg_count"] + next_block["msg_count"]),
                        "msg_count": current_block["msg_count"] + next_block["msg_count"]
                    }
                    merged_blocks.append(merged_block)
                    i += 2  # è·³è¿‡ä¸¤ä¸ªå—
                    continue

            merged_blocks.append(current_block)
            i += 1

        print(f"ğŸ“„ å—åˆå¹¶å®Œæˆ: {len(blocks)} -> {len(merged_blocks)}å—")
        return merged_blocks

    def _calculate_ideal_budgets(
        self, high_msgs: List[dict], mid_msgs: List[dict], adaptive_blocks: List[dict]
    ) -> Tuple[List[dict], int]:
        """è®¡ç®—ç†æƒ³é¢„ç®—éœ€æ±‚"""
        entries = []
        total_cost = 0

        # é«˜æƒé‡å’Œä¸­æƒé‡ï¼šå¾®æ‘˜è¦
        for grp, per_token in [(high_msgs, self.valves.coverage_high_summary_tokens),
                               (mid_msgs, self.valves.coverage_mid_summary_tokens)]:
            for item in grp:
                msg_id = item["msg"].get("_order_id", f"msg_{item['idx']}")
                entry = {
                    "type": "micro",
                    "msg_id": msg_id,
                    "ideal_budget": per_token,
                    "floor_budget": max(self.valves.min_summary_tokens, per_token // 3),
                    "msg": item["msg"],
                    "score": item["score"]
                }
                entries.append(entry)
                total_cost += per_token

        # è‡ªé€‚åº”å—ï¼šå—é¢„ç®—æŒ‰å¤§å°åˆ†é…
        for block in adaptive_blocks:
            # åŸºç¡€é¢„ç®—
            floor_budget = max(self.valves.min_block_summary_tokens,
                              self.valves.floor_block)
            # ç†æƒ³é¢„ç®—ï¼šåŸºç¡€ + æŒ‰åŸæ–‡tokené‡çš„æ¯”ä¾‹åˆ†é…
            size_factor = min(3.0, block["raw_tokens"] / self.valves.raw_block_target)
            ideal_budget = int(floor_budget +
                             (self.valves.coverage_block_summary_tokens - floor_budget) * size_factor)

            block_key = f"block_{block['idx_range'][0]}_{block['idx_range'][1]}"
            entry = {
                "type": "adaptive_block",
                "block_key": block_key,
                "idx_range": block["idx_range"],
                "ideal_budget": ideal_budget,
                "floor_budget": floor_budget,
                "msgs": block["msgs"],
                "raw_tokens": block["raw_tokens"],
                "avg_score": block["avg_score"]
            }
            entries.append(entry)
            total_cost += ideal_budget

        return entries, total_cost

    def _apply_proportional_scaling(
        self, entries: List[dict], available_budget: int
    ) -> Tuple[List[dict], int]:
        """ä¸€æ¬¡æ€§æ¯”ä¾‹ç¼©æ”¾"""
        # è®¡ç®—æ€»çš„floorå’Œideal
        total_floors = sum(entry["floor_budget"] for entry in entries)
        total_ideals = sum(entry["ideal_budget"] for entry in entries)

        if total_floors > available_budget:
            # è¿flooréƒ½è¶…äº†ï¼Œæ‰§è¡Œæç«¯é€€åŒ–
            return self._apply_extreme_fallback_from_entries(entries, available_budget)

        # è®¡ç®—ç¼©æ”¾å› å­ Î± = (B - Î£floors) / Î£(ideal_i - floor_i)
        available_for_scaling = available_budget - total_floors
        scalable_amount = total_ideals - total_floors

        if scalable_amount <= 0:
            # æ‰€æœ‰æ¡ç›®éƒ½æ˜¯floorï¼Œæ— æ³•ç¼©æ”¾
            alpha = 0
        else:
            alpha = available_for_scaling / scalable_amount
            alpha = min(1.0, alpha)  # é™åˆ¶æœ€å¤§ä¸º1.0

        # åº”ç”¨ç¼©æ”¾ï¼šbudget_i' = floor_i + Î± * (ideal_i - floor_i)
        total_assigned = 0
        for entry in entries:
            floor_budget = entry["floor_budget"]
            ideal_budget = entry["ideal_budget"]
            scaled_budget = floor_budget + alpha * (ideal_budget - floor_budget)
            entry["budget"] = int(round(scaled_budget))
            total_assigned += entry["budget"]

        # è¯¯å·®æŠ¹å¹³ï¼šé«˜åˆ†å…ˆè¡¥ï¼Œä½åˆ†å…ˆæ‰£
        error = available_budget - total_assigned
        if error != 0:
            # æŒ‰åˆ†æ•°æ’åºï¼ˆé«˜åˆ†åœ¨å‰ï¼‰
            scored_entries = [(entry.get("score", entry.get("avg_score", 0)), entry) for entry in entries]
            scored_entries.sort(key=lambda x: x[0], reverse=True)

            if error > 0:
                # æœ‰ä½™é¢ï¼Œé«˜åˆ†å…ˆè¡¥
                for _, entry in scored_entries:
                    if error <= 0:
                        break
                    entry["budget"] += 1
                    error -= 1
            else:
                # è¶…é¢„ç®—ï¼Œä½åˆ†å…ˆæ‰£
                for _, entry in reversed(scored_entries):
                    if error >= 0:
                        break
                    if entry["budget"] > entry["floor_budget"]:
                        entry["budget"] -= 1
                        error += 1

        final_cost = sum(entry["budget"] for entry in entries)
        return entries, final_cost

    def _apply_upward_expansion(
        self, entries: List[dict], available_budget: int, ideal_total_cost: int
    ) -> Tuple[List[dict], int]:
        """å‘ä¸Šæ‰©å¼ æ¨¡å¼ï¼šå½“é¢„ç®—å……è¶³æ—¶å¢åŠ é¢„ç®—åˆ†é…"""
        expansion_cap = 3.0  # æœ€å¤§æ‰©å¼ å€æ•°
        target_usage = 0.6   # ç›®æ ‡é¢„ç®—ä½¿ç”¨ç‡

        # è®¡ç®—æ‰©å¼ å› å­
        target_cost = int(available_budget * target_usage)
        if ideal_total_cost >= target_cost:
            # ç†æƒ³æˆæœ¬å·²ç»å¤Ÿé«˜ï¼Œä¸éœ€è¦æ‰©å¼ 
            for entry in entries:
                entry["budget"] = entry["ideal_budget"]
            return entries, ideal_total_cost

        # è®¡ç®—æ‰©å¼ å€æ•°
        expansion_factor = min(expansion_cap, target_cost / ideal_total_cost)

        # ä¼˜å…ˆæ‰©å¼ å—æ‘˜è¦å’Œé«˜æƒé‡micro
        total_assigned = 0
        for entry in entries:
            base_budget = entry["ideal_budget"]
            if entry["type"] == "adaptive_block":
                # å—æ‘˜è¦ä¼˜å…ˆæ‰©å¼ 
                expanded_budget = int(base_budget * expansion_factor)
            elif entry["type"] == "micro" and entry.get("score", 0) >= self.valves.coverage_high_score_threshold:
                # é«˜æƒé‡microé€‚åº¦æ‰©å¼ 
                expanded_budget = int(base_budget * min(2.0, expansion_factor))
            else:
                # å…¶ä»–ä¿æŒåŸæ ·
                expanded_budget = base_budget

            entry["budget"] = expanded_budget
            total_assigned += expanded_budget

        # ç¡®ä¿ä¸è¶…é¢„ç®—
        if total_assigned > available_budget:
            # æŒ‰æ¯”ä¾‹ç¼©å°
            scale_down = available_budget / total_assigned
            for entry in entries:
                entry["budget"] = int(entry["budget"] * scale_down)
            total_assigned = sum(entry["budget"] for entry in entries)

        return entries, total_assigned

    def _apply_extreme_fallback(self, scored_msgs: List[dict], available_budget: int) -> Tuple[List[dict], int]:
        """æç«¯é€€åŒ–ï¼šå•æ¡å…¨å±€å—æ‘˜è¦"""
        print(f"ğŸ“„ åº”ç”¨æç«¯é€€åŒ–ï¼šå•æ¡å…¨å±€å—æ‘˜è¦ï¼Œé¢„ç®—{available_budget}tokens")

        # ä½¿ç”¨90%çš„é¢„ç®—ä½œä¸ºå…¨å±€æ‘˜è¦é¢„ç®—ï¼Œç•™10%ä½œä¸ºç¼“å†²
        global_budget = max(self.valves.min_block_summary_tokens, int(available_budget * 0.9))

        # æŒ‰ç´¢å¼•æ’åºæ‰€æœ‰æ¶ˆæ¯
        sorted_msgs = sorted(scored_msgs, key=lambda x: x["idx"])
        all_msgs = [item["msg"] for item in sorted_msgs]

        entry = {
            "type": "global_block",
            "block_key": f"global_0_{len(sorted_msgs)-1}",
            "idx_range": (0, len(sorted_msgs)-1),
            "budget": global_budget,
            "msgs": all_msgs,
            "avg_score": sum(item["score"] for item in sorted_msgs) / len(sorted_msgs)
        }

        return [entry], global_budget

    def _apply_extreme_fallback_from_entries(self, entries: List[dict], available_budget: int) -> Tuple[List[dict], int]:
        """ä»ç°æœ‰æ¡ç›®æ‰§è¡Œæç«¯é€€åŒ–"""
        # æ”¶é›†æ‰€æœ‰æ¶ˆæ¯
        all_msgs = []
        for entry in entries:
            if entry["type"] == "micro":
                all_msgs.append(entry["msg"])
            elif entry["type"] == "adaptive_block":
                all_msgs.extend(entry["msgs"])

        # æŒ‰åŸå§‹ç´¢å¼•æ’åº
        all_msgs.sort(key=lambda x: x.get("_original_index", 0))

        global_budget = max(self.valves.min_block_summary_tokens, int(available_budget * 0.9))

        entry = {
            "type": "global_block",
            "block_key": f"global_0_{len(all_msgs)-1}",
            "idx_range": (0, len(all_msgs)-1),
            "budget": global_budget,
            "msgs": all_msgs,
            "avg_score": 0.5  # é»˜è®¤åˆ†æ•°
        }

        return [entry], global_budget

class Filter:
    class Valves(BaseModel):
        # åŸºç¡€æ§åˆ¶
        enable_processing: bool = Field(default=True, description="ğŸ”„ å¯ç”¨å†…å®¹æœ€å¤§åŒ–å¤„ç†")
        excluded_models: str = Field(default="", description="ğŸš« æ’é™¤æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)")

        # æ ¸å¿ƒé…ç½®
        max_window_utilization: float = Field(default=0.95, description="ğŸªŸ æœ€å¤§çª—å£åˆ©ç”¨ç‡(95%)")
        aggressive_content_recovery: bool = Field(default=True, description="ğŸ”„ æ¿€è¿›å†…å®¹åˆå¹¶æ¨¡å¼")
        min_preserve_ratio: float = Field(default=0.75, description="ğŸ”’ æœ€å°å†…å®¹ä¿ç•™æ¯”ä¾‹(75%)")

        # Coverage-Firstç­–ç•¥é…ç½®
        enable_coverage_first: bool = Field(default=True, description="ğŸ¯ å¯ç”¨Coverage-Firstç­–ç•¥")
        coverage_high_score_threshold: float = Field(default=0.7, description="ğŸ¯ é«˜æƒé‡é˜ˆå€¼(70%)")
        coverage_mid_score_threshold: float = Field(default=0.4, description="ğŸ¯ ä¸­æƒé‡é˜ˆå€¼(40%)")
        coverage_high_summary_tokens: int = Field(default=100, description="ğŸ“„ é«˜æƒé‡æ¶ˆæ¯å¾®æ‘˜è¦ç›®æ ‡tokens")
        coverage_mid_summary_tokens: int = Field(default=50, description="ğŸ“„ ä¸­æƒé‡æ¶ˆæ¯å¾®æ‘˜è¦ç›®æ ‡tokens")
        coverage_low_summary_tokens: int = Field(default=20, description="ğŸ“„ ä½æƒé‡æ¶ˆæ¯å¾®æ‘˜è¦ç›®æ ‡tokens")
        coverage_block_summary_tokens: int = Field(default=350, description="ğŸ“š å—æ‘˜è¦ç›®æ ‡tokens")
        coverage_upgrade_ratio: float = Field(default=0.3, description="â¬†ï¸ å‡çº§é¢„ç®—æ¯”ä¾‹(30%)")

        # æ–°å¢ï¼šè‡ªé€‚åº”åˆ†å—é…ç½®
        raw_block_target: int = Field(default=15000, description="ğŸ§© è‡ªé€‚åº”å—ç›®æ ‡åŸæ–‡tokens")
        floor_block: int = Field(default=300, description="ğŸ“ å—æ‘˜è¦æœ€å°é¢„ç®—tokens")
        max_blocks: int = Field(default=8, description="ğŸ“š æœ€å¤§å—æ•°é‡")
        upgrade_min_pct: float = Field(default=0.2, description="â¬†ï¸ å‡çº§æ± æœ€å°é¢„ç•™æ¯”ä¾‹(20%)")

        # é›¶ä¸¢å¤±ä¿éšœé…ç½®
        enable_zero_loss_guarantee: bool = Field(default=True, description="ğŸ›¡ï¸ å¯ç”¨é›¶ä¸¢å¤±ä¿éšœ")
        min_summary_tokens: int = Field(default=30, description="ğŸ“ æœ€å°å¾®æ‘˜è¦tokens(ä¿åº•)")
        min_block_summary_tokens: int = Field(default=200, description="ğŸ“ æœ€å°å—æ‘˜è¦tokens(ä¿åº•)")
        max_budget_adjustment_rounds: int = Field(default=5, description="ğŸ”§ æœ€å¤§é¢„ç®—è°ƒæ•´è½®æ¬¡")
        disable_insurance_truncation: bool = Field(default=True, description="ğŸš« ç¦ç”¨ä¿é™©æˆªæ–­(å¼ºåˆ¶é›¶ä¸¢å¤±)")

        # å°½é‡ä¿ç•™é…ç½®
        enable_try_preserve: bool = Field(default=True, description="ğŸ”’ å¯ç”¨å°½é‡ä¿ç•™æœºåˆ¶")
        try_preserve_ratio: float = Field(default=0.40, description="ğŸ”’ å°½é‡ä¿ç•™é¢„ç®—æ¯”ä¾‹(40%)")
        try_preserve_exchanges: int = Field(default=3, description="ğŸ”’ å°½é‡ä¿ç•™å¯¹è¯è½®æ¬¡æ•°")

        # å“åº”ç©ºé—´é…ç½®
        response_buffer_ratio: float = Field(default=0.06, description="ğŸ“ å“åº”ç©ºé—´é¢„ç•™æ¯”ä¾‹(6%)")
        response_buffer_max: int = Field(default=3000, description="ğŸ“ å“åº”ç©ºé—´æœ€å¤§å€¼(tokens)")
        response_buffer_min: int = Field(default=1000, description="ğŸ“ å“åº”ç©ºé—´æœ€å°å€¼(tokens)")

        # å¤šæ¨¡æ€å¤„ç†é…ç½®
        multimodal_direct_threshold: float = Field(default=0.70, description="ğŸ¯ å¤šæ¨¡æ€ç›´æ¥è¾“å…¥Tokené¢„ç®—é˜ˆå€¼(70%)")
        preserve_images_in_multimodal: bool = Field(default=True, description="ğŸ“¸ å¤šæ¨¡æ€æ¨¡å‹æ˜¯å¦ä¿ç•™åŸå§‹å›¾ç‰‡")
        always_process_images_before_summary: bool = Field(default=True, description="ğŸ“ æ‘˜è¦å‰æ€»æ˜¯å…ˆå¤„ç†å›¾ç‰‡")

        # ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†é…ç½®
        enable_context_maximization: bool = Field(default=True, description="ğŸ“š å¯ç”¨ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†")
        context_max_direct_preserve_ratio: float = Field(default=0.40, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ç›´æ¥ä¿ç•™æ¯”ä¾‹(40%)")
        context_max_processing_ratio: float = Field(default=0.45, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†é¢„ç®—æ¯”ä¾‹(45%)")
        context_max_fallback_ratio: float = Field(default=0.15, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å®¹é”™é¢„ç®—æ¯”ä¾‹(15%)")
        context_max_skip_rag: bool = Field(default=True, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–è·³è¿‡RAGå¤„ç†")
        context_max_prioritize_recent: bool = Field(default=True, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¼˜å…ˆä¿ç•™æœ€è¿‘å†…å®¹")

        # å®¹é”™æœºåˆ¶é…ç½®
        enable_fallback_preservation: bool = Field(default=True, description="ğŸ›¡ï¸ å¯ç”¨å®¹é”™ä¿æŠ¤æœºåˆ¶")
        fallback_preserve_ratio: float = Field(default=0.25, description="ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤é¢„ç•™æ¯”ä¾‹(25%)")
        min_history_messages: int = Field(default=8, description="ğŸ›¡ï¸ æœ€å°‘å†å²æ¶ˆæ¯æ•°é‡")
        force_preserve_recent_user_exchanges: int = Field(default=3, description="ğŸ›¡ï¸ å¼ºåˆ¶ä¿ç•™æœ€è¿‘ç”¨æˆ·å¯¹è¯è½®æ¬¡")

        # åŠŸèƒ½å¼€å…³
        enable_multimodal: bool = Field(default=True, description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å¤„ç†")
        enable_vision_preprocessing: bool = Field(default=True, description="ğŸ‘ï¸ å¯ç”¨å›¾ç‰‡é¢„å¤„ç†")
        enable_vector_retrieval: bool = Field(default=True, description="ğŸ” å¯ç”¨å‘é‡æ£€ç´¢")
        enable_intelligent_chunking: bool = Field(default=True, description="ğŸ§© å¯ç”¨æ™ºèƒ½åˆ†ç‰‡")
        enable_recursive_summarization: bool = Field(default=True, description="ğŸ”„ å¯ç”¨é€’å½’æ‘˜è¦")
        enable_reranking: bool = Field(default=True, description="ğŸ”„ å¯ç”¨é‡æ’åº")

        # æ™ºèƒ½å…³é”®å­—ç”Ÿæˆå’Œä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹
        enable_keyword_generation: bool = Field(default=True, description="ğŸ”‘ å¯ç”¨æ™ºèƒ½å…³é”®å­—ç”Ÿæˆ")
        enable_ai_context_max_detection: bool = Field(default=True, description="ğŸ§  å¯ç”¨AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹")
        keyword_generation_for_context_max: bool = Field(default=True, description="ğŸ”‘ å¯¹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¯ç”¨å…³é”®å­—ç”Ÿæˆ")

        # ç»Ÿè®¡å’Œè°ƒè¯•
        enable_detailed_stats: bool = Field(default=True, description="ğŸ“Š å¯ç”¨è¯¦ç»†ç»Ÿè®¡")
        enable_detailed_progress: bool = Field(default=True, description="ğŸ“± å¯ç”¨è¯¦ç»†è¿›åº¦æ˜¾ç¤º")
        debug_level: int = Field(default=2, description="ğŸ› è°ƒè¯•çº§åˆ« 0-3")
        show_frontend_progress: bool = Field(default=True, description="ğŸ“± æ˜¾ç¤ºå¤„ç†è¿›åº¦")

        # APIé…ç½®
        api_error_retry_times: int = Field(default=2, description="ğŸ”„ APIé”™è¯¯é‡è¯•æ¬¡æ•°")
        api_error_retry_delay: float = Field(default=1.0, description="â±ï¸ APIé”™è¯¯é‡è¯•å»¶è¿Ÿ(ç§’)")

        # Tokenç®¡ç†
        default_token_limit: int = Field(default=200000, description="âš–ï¸ é»˜è®¤tokené™åˆ¶")
        token_safety_ratio: float = Field(default=0.92, description="ğŸ›¡ï¸ Tokenå®‰å…¨æ¯”ä¾‹(92%)")
        target_window_usage: float = Field(default=0.85, description="ğŸªŸ ç›®æ ‡çª—å£ä½¿ç”¨ç‡(85%)")
        max_processing_iterations: int = Field(default=5, description="ğŸ”„ æœ€å¤§å¤„ç†è¿­ä»£æ¬¡æ•°")

        # ä¿æŠ¤ç­–ç•¥
        force_preserve_current_user_message: bool = Field(default=True, description="ğŸ”’ å¼ºåˆ¶ä¿ç•™å½“å‰ç”¨æˆ·æ¶ˆæ¯(æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯)")
        preserve_recent_exchanges: int = Field(default=4, description="ğŸ’¬ ä¿æŠ¤æœ€è¿‘å®Œæ•´å¯¹è¯è½®æ¬¡")
        max_preserve_ratio: float = Field(default=0.3, description="ğŸ”’ ä¿æŠ¤æ¶ˆæ¯æœ€å¤§tokenæ¯”ä¾‹")
        max_single_message_tokens: int = Field(default=20000, description="ğŸ“ å•æ¡æ¶ˆæ¯æœ€å¤§token")

        # æ™ºèƒ½åˆ†ç‰‡é…ç½®
        enable_smart_chunking: bool = Field(default=True, description="ğŸ§© å¯ç”¨æ™ºèƒ½åˆ†ç‰‡")
        chunk_target_tokens: int = Field(default=4000, description="ğŸ§© åˆ†ç‰‡ç›®æ ‡tokenæ•°")
        chunk_overlap_tokens: int = Field(default=300, description="ğŸ”— åˆ†ç‰‡é‡å tokenæ•°")
        chunk_min_tokens: int = Field(default=1000, description="ğŸ“ åˆ†ç‰‡æœ€å°tokenæ•°")
        chunk_max_tokens: int = Field(default=4000, description="ğŸ“ åˆ†ç‰‡æœ€å¤§tokenæ•°")
        large_message_threshold: int = Field(default=10000, description="ğŸ“ å¤§æ¶ˆæ¯åˆ†ç‰‡é˜ˆå€¼")
        preserve_paragraph_integrity: bool = Field(default=True, description="ğŸ“ ä¿æŒæ®µè½å®Œæ•´æ€§")
        preserve_sentence_integrity: bool = Field(default=True, description="ğŸ“ ä¿æŒå¥å­å®Œæ•´æ€§")
        preserve_code_blocks: bool = Field(default=True, description="ğŸ’» ä¿æŒä»£ç å—å®Œæ•´æ€§")

        # å†…å®¹ä¼˜å…ˆçº§è®¾ç½®
        high_priority_content: str = Field(
            default="ä»£ç ,é…ç½®,å‚æ•°,æ•°æ®,é”™è¯¯,è§£å†³æ–¹æ¡ˆ,æ­¥éª¤,æ–¹æ³•,æŠ€æœ¯ç»†èŠ‚,API,å‡½æ•°,ç±»,å˜é‡,é—®é¢˜,bug,ä¿®å¤,å®ç°,ç®—æ³•,æ¶æ„,ç”¨æˆ·é—®é¢˜,å…³é”®å›ç­”",
            description="ğŸ¯ é«˜ä¼˜å…ˆçº§å†…å®¹å…³é”®è¯(é€—å·åˆ†éš”)"
        )

        # ç»Ÿä¸€çš„APIé…ç½®
        api_base: str = Field(default="https://ark.cn-beijing.volces.com/api/v3", description="ğŸ”— APIåŸºç¡€åœ°å€")
        api_key: str = Field(default="", description="ğŸ”‘ APIå¯†é’¥")

        # å¤šæ¨¡æ€æ¨¡å‹é…ç½®
        multimodal_model: str = Field(default="doubao-1.5-vision-pro-250328", description="ğŸ–¼ï¸ å¤šæ¨¡æ€æ¨¡å‹")

        # æ–‡æœ¬æ¨¡å‹é…ç½®
        text_model: str = Field(default="doubao-1-5-lite-32k-250115", description="ğŸ“ æ–‡æœ¬å¤„ç†æ¨¡å‹")

        # å‘é‡æ¨¡å‹é…ç½®
        text_vector_model: str = Field(default="doubao-embedding-large-text-250515", description="ğŸ§  æ–‡æœ¬å‘é‡æ¨¡å‹")
        multimodal_vector_model: str = Field(default="doubao-embedding-vision-250615", description="ğŸ§  å¤šæ¨¡æ€å‘é‡æ¨¡å‹")

        # Visionç›¸å…³é…ç½®
        vision_prompt_template: str = Field(
            default="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€æ–‡å­—ã€é¢œè‰²ã€å¸ƒå±€ç­‰æ‰€æœ‰å¯è§ä¿¡æ¯ã€‚ç‰¹åˆ«æ³¨æ„ä»£ç ã€é…ç½®ã€æ•°æ®ç­‰æŠ€æœ¯ä¿¡æ¯ã€‚ä¿æŒå®¢è§‚å‡†ç¡®ï¼Œé‡ç‚¹çªå‡ºå…³é”®ä¿¡æ¯ã€‚å¦‚æœå›¾ç‰‡åŒ…å«æ–‡å­—å†…å®¹ï¼Œè¯·å®Œæ•´è½¬å½•å‡ºæ¥ã€‚",
            description="ğŸ‘ï¸ Visionæç¤ºè¯"
        )
        vision_max_tokens: int = Field(default=2500, description="ğŸ‘ï¸ Visionæœ€å¤§è¾“å‡ºtokens")

        # å…³é”®å­—ç”Ÿæˆé…ç½®
        keyword_generation_prompt: str = Field(
            default="""ä½ æ˜¯ä¸“ä¸šçš„æœç´¢å…³é”®å­—ç”ŸæˆåŠ©æ‰‹ã€‚ç”¨æˆ·è¾“å…¥äº†ä¸€ä¸ªæŸ¥è¯¢ï¼Œä½ éœ€è¦ç”Ÿæˆå¤šä¸ªç›¸å…³çš„æœç´¢å…³é”®å­—æ¥å¸®åŠ©åœ¨å¯¹è¯å†å²ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚

ğŸ“‹ ä»»åŠ¡è¦æ±‚ï¼š
1. åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾å’Œä¸»é¢˜
2. ç”Ÿæˆ5-10ä¸ªç›¸å…³çš„æœç´¢å…³é”®å­—
3. åŒ…å«åŒä¹‰è¯ã€ç›¸å…³è¯ã€æŠ€æœ¯æœ¯è¯­
4. å¯¹äºå®½æ³›æŸ¥è¯¢ï¼ˆå¦‚"èŠäº†ä»€ä¹ˆ"ã€"è¯´äº†ä»€ä¹ˆ"ï¼‰ï¼Œç”Ÿæˆé€šç”¨ä½†æœ‰æ•ˆçš„å…³é”®å­—
5. å…³é”®å­—åº”è¯¥èƒ½è¦†ç›–å¯èƒ½çš„å¯¹è¯ä¸»é¢˜

ğŸ“ è¾“å‡ºæ ¼å¼ï¼š
ç›´æ¥è¾“å‡ºå…³é”®å­—ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

ç°åœ¨è¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”Ÿæˆå…³é”®å­—ï¼š""",
            description="ğŸ”‘ å…³é”®å­—ç”Ÿæˆæç¤ºè¯"
        )

        # ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹é…ç½®
        context_max_detection_prompt: str = Field(
            default="""ä½ æ˜¯ä¸“ä¸šçš„æŸ¥è¯¢æ„å›¾åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢æ˜¯å¦éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†ã€‚

ğŸ“‹ åˆ¤æ–­æ ‡å‡†ï¼š
éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„æŸ¥è¯¢ç‰¹å¾ï¼š
- è¯¢é—®"èŠäº†ä»€ä¹ˆ"ã€"è¯´äº†ä»€ä¹ˆ"ã€"è®¨è®ºäº†ä»€ä¹ˆ"ç­‰å®½æ³›å†…å®¹
- è¯¢é—®"ä¹‹å‰çš„å†…å®¹"ã€"å†å²è®°å½•"ã€"å¯¹è¯å†å²"ç­‰
- ç¼ºä¹å…·ä½“çš„ä¸»é¢˜ã€å…³é”®è¯æˆ–æ˜ç¡®çš„æœç´¢æ„å›¾
- æŸ¥è¯¢è¯æ±‡å°‘äº3ä¸ªæœ‰æ•ˆè¯æ±‡

ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„æŸ¥è¯¢ç‰¹å¾ï¼š
- åŒ…å«æ˜ç¡®çš„ä¸»é¢˜ã€æŠ€æœ¯æœ¯è¯­ã€äº§å“åç§°ç­‰
- æœ‰å…·ä½“çš„é—®é¢˜æŒ‡å‘
- åŒ…å«è¯¦ç»†çš„æè¿°æˆ–èƒŒæ™¯ä¿¡æ¯

ğŸ“ è¾“å‡ºæ ¼å¼ï¼š
åªè¾“å‡º "éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–" æˆ– "ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–"ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

ç°åœ¨è¯·åˆ†æä»¥ä¸‹æŸ¥è¯¢ï¼š""",
            description="ğŸ§  ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹æç¤ºè¯"
        )

        # å‘é‡æ£€ç´¢é…ç½®
        vector_similarity_threshold: float = Field(default=0.06, description="ğŸ¯ åŸºç¡€ç›¸ä¼¼åº¦é˜ˆå€¼")
        multimodal_similarity_threshold: float = Field(default=0.04, description="ğŸ–¼ï¸ å¤šæ¨¡æ€ç›¸ä¼¼åº¦é˜ˆå€¼")
        text_similarity_threshold: float = Field(default=0.08, description="ğŸ“ æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼")
        vector_top_k: int = Field(default=150, description="ğŸ” å‘é‡æ£€ç´¢Top-Kæ•°é‡")

        # é‡æ’åºAPIé…ç½®
        rerank_api_base: str = Field(default="https://api.bochaai.com", description="ğŸ”„ é‡æ’åºAPI")
        rerank_api_key: str = Field(default="", description="ğŸ”‘ é‡æ’åºå¯†é’¥")
        rerank_model: str = Field(default="gte-rerank", description="ğŸ§  é‡æ’åºæ¨¡å‹")
        rerank_top_k: int = Field(default=100, description="ğŸ” é‡æ’åºè¿”å›æ•°é‡")

        # æ‘˜è¦é…ç½®
        max_summary_length: int = Field(default=25000, description="ğŸ“ æ‘˜è¦æœ€å¤§é•¿åº¦")
        min_summary_ratio: float = Field(default=0.30, description="ğŸ“ æ‘˜è¦æœ€å°é•¿åº¦æ¯”ä¾‹")
        summary_compression_ratio: float = Field(default=0.40, description="ğŸ“Š æ‘˜è¦å‹ç¼©æ¯”ä¾‹")
        max_recursion_depth: int = Field(default=3, description="ğŸ”„ æœ€å¤§é€’å½’æ·±åº¦")

        # æ€§èƒ½é…ç½®
        max_concurrent_requests: int = Field(default=6, description="âš¡ æœ€å¤§å¹¶å‘æ•°")
        request_timeout: int = Field(default=90, description="â±ï¸ è¯·æ±‚è¶…æ—¶(ç§’) - åŠ é•¿åˆ°90s")

    def __init__(self):
        print("\n" + "=" * 70)
        print("ğŸš€ Advanced Context Manager v2.4.4 - å®Œæ•´ä¿®å¤ç‰ˆæœ¬")
        print("ğŸ“ æ’ä»¶æ­£åœ¨åˆå§‹åŒ–...")
        
        self.valves = self.Valves()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model_matcher = ModelMatcher()
        self.token_calculator = TokenCalculator()
        self.input_cleaner = InputCleaner()
        self.message_chunker = MessageChunker(self.token_calculator, self.valves)
        self.coverage_planner = CoveragePlanner(self.token_calculator, self.valves)
        
        # å¤„ç†ç»Ÿè®¡
        self.stats = ProcessingStats()
        
        # æ¶ˆæ¯é¡ºåºç®¡ç†å™¨
        self.message_order = None
        self.current_processing_id = None
        self.current_user_message = None
        self.current_model_info = None
        
        # è§£æé…ç½®
        self._parse_configurations()
        
        print(f"âœ… v2.4.4 å®Œæ•´ä¿®å¤ç‰ˆæœ¬åˆå§‹åŒ–å®Œæˆ:")
        print(f"ğŸ”§ è¯­æ³•ä¿®å¤: æ‰€æœ‰__init__é”™è¯¯ã€å¼•å·æ–­è£‚ã€è¿ç®—ç¬¦ä¸¢å¤±å·²ä¿®å¤")
        print(f"ğŸ”§ æ–¹æ³•åç»Ÿä¸€: æ¶ˆé™¤æ‰€æœ‰ä¸‹åˆ’çº¿ä¸ä¸€è‡´é—®é¢˜")
        print(f"ğŸ”§ å±æ€§åŒ¹é…: æ‰€æœ‰è°ƒç”¨åä¸å®šä¹‰åä¿æŒä¸€è‡´")
        print(f"ğŸ†• GPT-5ç³»åˆ—: å®Œæ•´æ”¯æŒgpt-5/mini/nano (200k + å¤šæ¨¡æ€)")
        print(f"ğŸ›¡ï¸ åŒé‡æŠ¤æ : ç»„è£…å‰æ ¡éªŒ + æœªè½åœ°å¾®æ‘˜è¦å›é€€")
        print(f"ğŸ§© è‡ªé€‚åº”åˆ†å—: æŒ‰åŸæ–‡é‡({self.valves.raw_block_target:,}t)åˆ‡å—")
        print(f"âš–ï¸ ä¸€æ¬¡æ€§ç¼©æ”¾: Î±ç²¾ç¡®è®¡ç®—ï¼Œè¯¯å·®æŠ¹å¹³")
        print(f"â¬†ï¸ å‡çº§æ± ä¿æŠ¤: é¢„ç•™{self.valves.upgrade_min_pct:.1%}é˜²è¢«åƒå…‰")
        print(f"ğŸ¯ Coverage-First: 100%è¦†ç›– + é›¶ä¸¢å¤±ä¿éšœ")
        print(f"ğŸªŸ æœ€å¤§çª—å£åˆ©ç”¨: {self.valves.max_window_utilization:.1%}")
        print(f"ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–: {self.valves.enable_context_maximization}")
        print(f"ğŸ”‘ æ™ºèƒ½å…³é”®å­—: {self.valves.enable_keyword_generation}")
        print(f"ğŸ§  AIæ£€æµ‹: {self.valves.enable_ai_context_max_detection}")
        print("=" * 70 + "\n")

    def _parse_configurations(self):
        """è§£æé…ç½®é¡¹"""
        # è§£æé«˜ä¼˜å…ˆçº§å†…å®¹å…³é”®è¯
        self.high_priority_keywords = set()
        if self.valves.high_priority_content:
            self.high_priority_keywords = {
                keyword.strip().lower()
                for keyword in self.valves.high_priority_content.split(",")
                if keyword.strip()
            }

    def reset_processing_state(self):
        """é‡ç½®å¤„ç†çŠ¶æ€"""
        self.current_processing_id = None
        self.message_order = None
        self.current_user_message = None
        self.current_model_info = None
        self.stats = ProcessingStats()

    def debug_log(self, level: int, message: str, emoji: str = "ğŸ”§"):
        """åˆ†çº§è°ƒè¯•æ—¥å¿—"""
        if self.valves.debug_level >= level:
            prefix = ["", "ğŸ›[DEBUG]", "ğŸ”[DETAIL]", "ğŸ“‹[VERBOSE]"][min(level, 3)]
            # åŸºæœ¬æ¸…ç†
            message = self.input_cleaner.clean_text_for_regex(message)
            print(f"{prefix} {emoji} {message}")

    def is_model_excluded(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«æ’é™¤"""
        if not self.valves.excluded_models or not model_name:
            return False
        
        excluded_list = [
            model.strip().lower()
            for model in self.valves.excluded_models.split(",")
            if model.strip()
        ]
        
        if not excluded_list:
            return False
        
        model_lower = model_name.lower()
        for excluded_model in excluded_list:
            if excluded_model in model_lower:
                self.debug_log(1, f"æ¨¡å‹ {model_name} åœ¨æ’é™¤åˆ—è¡¨ä¸­", "ğŸš«")
                return True
        return False

    def analyze_model(self, model_name: str) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹ä¿¡æ¯"""
        model_info = self.model_matcher.match_model(model_name)
        self.debug_log(2, f"æ¨¡å‹åˆ†æ: {model_name} -> {model_info['family']} "
                          f"({'å¤šæ¨¡æ€' if model_info['multimodal'] else 'æ–‡æœ¬'}) "
                          f"{model_info['limit']:,}tokens "
                          f"[{model_info['match_type']}åŒ¹é…]", "ğŸ¯")
        
        if model_info.get("special") == "thinking":
            self.debug_log(1, f"æ£€æµ‹åˆ°Thinkingæ¨¡å‹: {model_name}", "ğŸ§ ")
        
        # ç‰¹åˆ«æ ‡è®°GPT-5ç³»åˆ—
        if model_info.get("family") == "gpt" and "gpt-5" in model_name.lower():
            self.debug_log(1, f"æ£€æµ‹åˆ°GPT-5ç³»åˆ—æ¨¡å‹: {model_name} (200k tokens + å¤šæ¨¡æ€)", "ğŸ†•")
        
        return model_info

    def count_tokens(self, text: str) -> int:
        """ç®€åŒ–çš„tokenè®¡ç®—"""
        if not text:
            return 0
        return self.token_calculator.count_tokens(text)

    def count_message_tokens(self, message: dict) -> int:
        """è®¡ç®—å•æ¡æ¶ˆæ¯çš„tokenæ•°é‡"""
        if not message:
            return 0
        
        content = message.get("content", "")
        role = message.get("role", "")
        total_tokens = 0
        
        # å¤„ç†å¤šæ¨¡æ€å†…å®¹
        if isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    text = item.get("text", "")
                    total_tokens += self.count_tokens(text)
                elif item.get("type") == "image_url":
                    total_tokens += self.token_calculator.calculate_image_tokens("")
        else:
            total_tokens = self.count_tokens(content)
        
        # åŠ ä¸Šè§’è‰²å’Œæ ¼å¼å¼€é”€
        total_tokens += self.count_tokens(role) + 20
        return total_tokens

    def count_messages_tokens(self, messages: List[dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€»tokenæ•°é‡"""
        if not messages:
            return 0
        
        total_tokens = sum(self.count_message_tokens(msg) for msg in messages)
        self.debug_log(2, f"æ¶ˆæ¯åˆ—è¡¨tokenè®¡ç®—: {len(messages)}æ¡æ¶ˆæ¯ -> {total_tokens:,}tokens", "ğŸ“Š")
        return total_tokens

    def get_model_token_limit(self, model_name: str) -> int:
        """è·å–æ¨¡å‹çš„tokené™åˆ¶ï¼ˆåº”ç”¨å®‰å…¨ç³»æ•°ï¼‰"""
        model_info = self.analyze_model(model_name)
        limit = model_info.get("limit", self.valves.default_token_limit)
        safe_limit = int(limit * self.valves.token_safety_ratio)
        self.debug_log(2, f"æ¨¡å‹tokené™åˆ¶: {model_name} -> {limit} -> {safe_limit}", "âš–ï¸")
        return safe_limit

    def is_multimodal_model(self, model_name: str) -> bool:
        """åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€è¾“å…¥"""
        model_info = self.analyze_model(model_name)
        return model_info.get("multimodal", False)

    def find_current_user_message(self, messages: List[dict]) -> Optional[dict]:
        """æŸ¥æ‰¾å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€æ–°çš„ç”¨æˆ·è¾“å…¥ï¼‰"""
        if not messages:
            return None
        
        for msg in reversed(messages):
            if msg.get("role") == "user":
                self.debug_log(2, f"æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯: {len(self.extract_text_from_content(msg.get('content', '')))}å­—ç¬¦", "ğŸ’¬")
                return msg
        return None

    def separate_current_and_history_messages(self, messages: List[dict]) -> Tuple[Optional[dict], List[dict]]:
        """åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯"""
        if not messages:
            return None, []
        
        # æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€æ–°çš„ç”¨æˆ·è¾“å…¥ï¼‰
        current_user_message = None
        current_user_index = -1
        
        # ä»åå¾€å‰æŸ¥æ‰¾æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if msg.get("role") == "user":
                current_user_message = msg
                current_user_index = i
                break
        
        if not current_user_message:
            self.debug_log(1, "æœªæ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼Œæ‰€æœ‰æ¶ˆæ¯ä½œä¸ºå†å²æ¶ˆæ¯å¤„ç†", "âš ï¸")
            return None, messages
        
        # åˆ†ç¦»å†å²æ¶ˆæ¯ï¼ˆå½“å‰ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰çš„æ‰€æœ‰æ¶ˆæ¯ï¼‰
        history_messages = messages[:current_user_index]
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.history_message_separation_count += 1
        
        self.debug_log(1, f"æ¶ˆæ¯åˆ†ç¦»å®Œæˆ: å½“å‰ç”¨æˆ·æ¶ˆæ¯1æ¡({self.count_message_tokens(current_user_message)}tokens), "
                          f"å†å²æ¶ˆæ¯{len(history_messages)}æ¡({self.count_messages_tokens(history_messages):,}tokens), "
                          f"åˆ†ç¦»ç´¢å¼•:{current_user_index}", "ğŸ“‹")
        
        return current_user_message, history_messages

    def calculate_target_tokens(self, model_name: str, current_user_tokens: int) -> int:
        """è®¡ç®—ç›®æ ‡tokenæ•°ï¼šæ¨¡å‹é™åˆ¶ - å½“å‰ç”¨æˆ·æ¶ˆæ¯ - å“åº”ç©ºé—´"""
        model_token_limit = self.get_model_token_limit(model_name)
        
        # è®¡ç®—å“åº”ç©ºé—´
        response_buffer = min(
            self.valves.response_buffer_max,
            max(
                self.valves.response_buffer_min,
                int(model_token_limit * self.valves.response_buffer_ratio)
            )
        )
        
        # è®¡ç®—ç›®æ ‡ï¼šæ€»é™åˆ¶ - å½“å‰ç”¨æˆ·æ¶ˆæ¯ - å“åº”ç¼“å†²åŒº
        target_tokens = model_token_limit - current_user_tokens - response_buffer
        
        # ç¡®ä¿ä¸å°äºåŸºç¡€å€¼
        min_target = max(10000, model_token_limit * 0.3)
        target_tokens = max(target_tokens, min_target)
        
        self.debug_log(1, f"ğŸ¯ ç›®æ ‡tokenè®¡ç®—: {model_token_limit} - {current_user_tokens} - {response_buffer} = {target_tokens}", "ğŸ¯")
        return int(target_tokens)

    # ========== å¤šæ¨¡æ€å¤„ç† ==========
    def has_images_in_content(self, content) -> bool:
        """æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        if isinstance(content, list):
            return any(item.get("type") == "image_url" for item in content)
        return False

    def has_images_in_messages(self, messages: List[dict]) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯åˆ—è¡¨ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        return any(self.has_images_in_content(msg.get("content")) for msg in messages)

    def extract_text_from_content(self, content) -> str:
        """ä»å†…å®¹ä¸­æå–æ–‡æœ¬"""
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if item.get("type") == "text":
                    text = item.get("text", "")
                    text_parts.append(text)
            return " ".join(text_parts)
        else:
            return str(content) if content else ""

    def extract_images_from_content(self, content) -> List[dict]:
        """ä»å†…å®¹ä¸­æå–å›¾ç‰‡ä¿¡æ¯"""
        if isinstance(content, list):
            images = []
            for item in content:
                if item.get("type") == "image_url":
                    images.append(item)
            return images
        return []

    def is_high_priority_content(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜ä¼˜å…ˆçº§å†…å®¹"""
        if not text or not self.high_priority_keywords:
            return False
        
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.high_priority_keywords)

    # ========== APIå®¢æˆ·ç«¯ç®¡ç† ==========
    def get_api_client(self, client_type: str = "default"):
        """è·å–APIå®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE:
            return None
        
        if self.valves.api_key:
            return AsyncOpenAI(
                base_url=self.valves.api_base,
                api_key=self.valves.api_key,
                timeout=self.valves.request_timeout
            )
        return None

    # ========== å®‰å…¨APIè°ƒç”¨ ==========
    async def safe_api_call(self, call_func, call_name: str, *args, **kwargs):
        """å®‰å…¨çš„APIè°ƒç”¨åŒ…è£…å™¨"""
        for attempt in range(self.valves.api_error_retry_times + 1):
            try:
                result = await call_func(*args, **kwargs)
                return result
            except Exception as e:
                error_msg = str(e)
                # æ•è·è¯­æ³•é”™è¯¯å¹¶æ‰“å°ä¸Šä¸‹æ–‡
                if "SyntaxError" in error_msg or "did not match the expected pattern" in error_msg:
                    print(f"âŒ {call_name} è¯­æ³•é”™è¯¯: {error_msg}")
                    # æ‰“å°è°ƒç”¨å‚æ•°çš„å‰100å­—ç¬¦
                    if args:
                        context = str(args[0])[:100] if len(str(args[0])) > 100 else str(args[0])
                        print(f"âŒ ä¸Šä¸‹æ–‡: {context}")
                    self.stats.syntax_errors_fixed += 1
                    return None
                
                if attempt < self.valves.api_error_retry_times:
                    self.debug_log(1, f"{call_name} ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{self.valves.api_error_retry_delay}ç§’åé‡è¯•: {error_msg}", "ğŸ”„")
                    await asyncio.sleep(self.valves.api_error_retry_delay)
                else:
                    self.debug_log(1, f"{call_name} æœ€ç»ˆå¤±è´¥: {error_msg}", "âŒ")
                    return None
        return None

    # ========== ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹ ==========
    async def detect_context_max_need_impl(self, query_text: str, event_emitter):
        """å®é™…çš„ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹å®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        # ä½¿ç”¨è¾“å…¥æ¸…æ´—
        cleaned_query = self.input_cleaner.clean_text_for_regex(query_text)
        
        # æ„å»ºæç¤º
        prompt = f"{self.valves.context_max_detection_prompt}\n\n{cleaned_query}"
        
        response = await client.chat.completions.create(
            model=self.valves.text_model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=50,
            temperature=0.1,
            timeout=self.valves.request_timeout
        )
        
        if response.choices and response.choices[0].message.content:
            result = response.choices[0].message.content.strip()
            result = self.input_cleaner.clean_text_for_regex(result)
            need_context_max = "éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–" in result
            self.debug_log(2, f"AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹ç»“æœ: {result} -> {need_context_max}", "ğŸ§ ")
            return need_context_max
        return None

    async def detect_context_max_need(self, query_text: str, event_emitter) -> bool:
        """ä½¿ç”¨AIæ£€æµ‹æ˜¯å¦éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–"""
        if not self.valves.enable_ai_context_max_detection:
            return self.is_context_max_need_simple(query_text)
        
        self.debug_log(1, f"ğŸ§  AIæ£€æµ‹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚: {query_text[:50]}...", "ğŸ§ ")
        
        need_context_max = await self.safe_api_call(
            self.detect_context_max_need_impl, "ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹", query_text, event_emitter
        )
        
        if need_context_max is not None:
            self.stats.context_maximization_detections += 1
            self.debug_log(1, f"ğŸ§  AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹å®Œæˆ: {'éœ€è¦' if need_context_max else 'ä¸éœ€è¦'}", "ğŸ§ ")
            return need_context_max
        else:
            # AIæ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•
            self.debug_log(1, f"ğŸ§  AIæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•", "âš ï¸")
            return self.is_context_max_need_simple(query_text)

    def is_context_max_need_simple(self, query_text: str) -> bool:
        """ç®€å•çš„ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚åˆ¤æ–­ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        if not query_text:
            return True
        
        query_text = self.input_cleaner.clean_text_for_regex(query_text)
        
        # éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„ç‰¹å¾
        context_max_patterns = [
            r".*èŠ.*ä»€ä¹ˆ.*",
            r".*è¯´.*ä»€ä¹ˆ.*",
            r".*è®¨è®º.*ä»€ä¹ˆ.*",
            r".*è°ˆ.*ä»€ä¹ˆ.*",
            r".*å†…å®¹.*",
            r".*è¯é¢˜.*",
            r".*å†å².*",
            r".*è®°å½•.*",
            r".*ä¹‹å‰.*",
            r"what.*discuss.*",
            r"what.*talk.*",
            r"what.*chat.*",
            r".*conversation.*",
            r".*history.*",
        ]
        
        query_lower = query_text.lower()
        for pattern in context_max_patterns:
            if self.input_cleaner.safe_regex_match(pattern, query_lower):
                return True
        
        return len(query_text.split()) <= 3

    # ========== å…³é”®å­—ç”Ÿæˆ ==========
    async def generate_keywords_impl(self, query_text: str, event_emitter):
        """å®é™…çš„å…³é”®å­—ç”Ÿæˆå®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        cleaned_query = self.input_cleaner.clean_text_for_regex(query_text)
        prompt = f"{self.valves.keyword_generation_prompt}\n\n{cleaned_query}"
        
        response = await client.chat.completions.create(
            model=self.valves.text_model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.3,
            timeout=self.valves.request_timeout
        )
        
        if response.choices and response.choices[0].message.content:
            keywords_text = response.choices[0].message.content.strip()
            keywords_text = self.input_cleaner.clean_text_for_regex(keywords_text)
            keywords = [kw.strip() for kw in keywords_text.split(",") if kw.strip()]
            keywords = [kw for kw in keywords if len(kw) >= 2]
            self.debug_log(2, f"ç”Ÿæˆå…³é”®å­—: {keywords[:5]}...", "ğŸ”‘")
            return keywords
        return None

    async def generate_search_keywords(self, query_text: str, event_emitter) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®å­—"""
        if not self.valves.enable_keyword_generation:
            return [query_text]
        
        need_context_max = await self.detect_context_max_need(query_text, event_emitter)
        if not need_context_max and not self.valves.keyword_generation_for_context_max:
            self.debug_log(2, f"å…·ä½“æŸ¥è¯¢ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬: {query_text[:50]}...", "ğŸ”‘")
            return [query_text]
        
        self.debug_log(1, f"ğŸ”‘ ç”Ÿæˆæœç´¢å…³é”®å­—: {query_text[:50]}...", "ğŸ”‘")
        
        keywords = await self.safe_api_call(
            self.generate_keywords_impl, "å…³é”®å­—ç”Ÿæˆ", query_text, event_emitter
        )
        
        if keywords:
            final_keywords = [query_text] + keywords
            final_keywords = list(dict.fromkeys(final_keywords))
            self.stats.keyword_generations += 1
            self.debug_log(1, f"ğŸ”‘ å…³é”®å­—ç”Ÿæˆå®Œæˆ: {len(final_keywords)}ä¸ª", "ğŸ”‘")
            return final_keywords
        else:
            self.debug_log(1, f"ğŸ”‘ å…³é”®å­—ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢", "âš ï¸")
            return [query_text]

    # ========== å‘é‡å¤„ç† ==========
    async def get_text_embedding_impl(self, text: str, event_emitter):
        """å®é™…çš„æ–‡æœ¬å‘é‡è·å–å®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        cleaned_text = self.input_cleaner.clean_text_for_regex(text)
        
        response = await client.embeddings.create(
            model=self.valves.text_vector_model,
            input=[cleaned_text[:8000]],
            encoding_format="float"
        )
        
        if response and response.data and len(response.data) > 0 and response.data[0].embedding:
            return response.data[0].embedding
        return None

    async def get_text_embedding(self, text: str, event_emitter) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬å‘é‡"""
        if not text:
            return None
        
        embedding = await self.safe_api_call(
            self.get_text_embedding_impl, "æ–‡æœ¬å‘é‡", text, event_emitter
        )
        
        if embedding:
            self.debug_log(3, f"æ–‡æœ¬å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ“")
        return embedding

    async def get_multimodal_embedding_impl(self, content, event_emitter):
        """å®é™…çš„å¤šæ¨¡æ€å‘é‡è·å–å®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        # å¤„ç†è¾“å…¥æ ¼å¼
        if isinstance(content, list):
            cleaned_content = []
            for item in content:
                if item.get("type") == "text":
                    cleaned_item = item.copy()
                    text = item.get("text", "")
                    cleaned_text = self.input_cleaner.clean_text_for_regex(text)
                    cleaned_item["text"] = cleaned_text
                    cleaned_content.append(cleaned_item)
                elif item.get("type") == "image_url":
                    # éªŒè¯å¹¶æ¸…æ´—å›¾ç‰‡æ•°æ®
                    image_url = item.get("image_url", {}).get("url", "")
                    is_valid, cleaned_url = self.input_cleaner.validate_and_clean_data_uri(image_url)
                    if is_valid:
                        cleaned_item = copy.deepcopy(item)
                        cleaned_item["image_url"]["url"] = cleaned_url
                        cleaned_content.append(cleaned_item)
                else:
                    cleaned_content.append(item)
            input_data = cleaned_content
        else:
            text = str(content)
            cleaned_text = self.input_cleaner.clean_text_for_regex(text)
            input_data = [{"type": "text", "text": cleaned_text[:8000]}]
        
        try:
            response = await client.embeddings.create(
                model=self.valves.multimodal_vector_model,
                input=input_data,
                encoding_format="float"
            )
            
            if hasattr(response, "data") and hasattr(response.data, "embedding"):
                return response.data.embedding
            elif hasattr(response, "data") and isinstance(response.data, list) and len(response.data) > 0:
                return response.data[0].embedding
            else:
                self.debug_log(1, f"å¤šæ¨¡æ€å‘é‡å“åº”æ ¼å¼å¼‚å¸¸", "âš ï¸")
                return None
        except Exception as e:
            self.debug_log(1, f"å¤šæ¨¡æ€å‘é‡è°ƒç”¨å¤±è´¥: {str(e)[:100]}", "âŒ")
            raise

    async def get_multimodal_embedding(self, content, event_emitter) -> Optional[List[float]]:
        """è·å–å¤šæ¨¡æ€å‘é‡"""
        if not content:
            return None
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ¨¡æ€å†…å®¹
        has_multimodal_content = False
        if isinstance(content, list):
            has_multimodal_content = any(item.get("type") in ["image_url", "video_url"] for item in content)
        
        if not has_multimodal_content:
            self.debug_log(3, "å†…å®¹ä¸åŒ…å«å¤šæ¨¡æ€å…ƒç´ ï¼Œä¸ä½¿ç”¨å¤šæ¨¡æ€å‘é‡", "ğŸ“")
            return None
        
        embedding = await self.safe_api_call(
            self.get_multimodal_embedding_impl, "å¤šæ¨¡æ€å‘é‡", content, event_emitter
        )
        
        if embedding:
            self.debug_log(3, f"å¤šæ¨¡æ€å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ–¼ï¸")
        return embedding

    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)

    # ========== ç›¸å…³åº¦è®¡ç®—ï¼ˆå¹¶å‘ä¼˜åŒ–ï¼‰ ==========
    async def compute_relevance_scores(self, query_msg: dict, history_msgs: List[dict], progress: ProgressTracker) -> List[dict]:
        """è®¡ç®—æ‰€æœ‰å†å²æ¶ˆæ¯çš„ç›¸å…³åº¦åˆ†æ•°ï¼ˆå¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if not history_msgs:
            return []
        
        self.debug_log(1, f"ğŸ¯ å¼€å§‹è®¡ç®—ç›¸å…³åº¦åˆ†æ•°: æŸ¥è¯¢1æ¡ï¼Œå†å²{len(history_msgs)}æ¡", "ğŸ¯")
        
        # è·å–æŸ¥è¯¢å‘é‡
        query_content = query_msg.get("content", "")
        query_text = self.extract_text_from_content(query_content)
        
        # æ™ºèƒ½å‘é‡åŒ–ç­–ç•¥
        if self.has_images_in_content(query_content):
            query_vector = await self.get_multimodal_embedding(query_content, progress.event_emitter)
            if not query_vector:
                query_vector = await self.get_text_embedding(query_text, progress.event_emitter)
        else:
            query_vector = await self.get_text_embedding(query_text, progress.event_emitter)
        
        # å¹¶å‘è·å–å†å²æ¶ˆæ¯å‘é‡ï¼ˆä¿®å¤å¤§æ•°æ®é›†è¶…æ—¶é—®é¢˜ï¼‰
        if len(history_msgs) > 80:
            # å¤§æ•°æ®é›†ï¼šè·³è¿‡å‘é‡è®¡ç®—ï¼Œä½¿ç”¨è½»é‡çº§è¯„åˆ†
            self.debug_log(1, f"ğŸ¯ å¤§æ•°æ®é›†({len(history_msgs)}æ¡)ï¼Œè·³è¿‡å‘é‡è®¡ç®—ä½¿ç”¨è½»é‡çº§è¯„åˆ†", "âš¡")
            scored = self._compute_lightweight_scores(query_text, history_msgs)
        else:
            # å°æ•°æ®é›†ï¼šå¹¶å‘è·å–å‘é‡
            scored = await self._compute_vector_scores_concurrent(query_vector, history_msgs, progress)
        
        self.debug_log(1, f"ğŸ¯ ç›¸å…³åº¦è®¡ç®—å®Œæˆ: {len(scored)}æ¡æ¶ˆæ¯å…¨éƒ¨è¯„åˆ†", "ğŸ¯")
        
        # æ‰“å°Top5åˆ†æ•°ç”¨äºè°ƒè¯•
        top5 = sorted(scored, key=lambda x: x["score"], reverse=True)[:5]
        for i, item in enumerate(top5):
            self.debug_log(2, f"Top{i+1}: score={item['score']:.3f}(sim={item.get('sim',0):.3f}+rec={item['recency']:.3f}+role={item['role_weight']:.3f}+kw={item['kw_bonus']:.3f}), {item['tokens']}tokens", "ğŸ“Š")
        
        return scored

    def _compute_lightweight_scores(self, query_text: str, history_msgs: List[dict]) -> List[dict]:
        """è½»é‡çº§è¯„åˆ†ï¼ˆä¸ä½¿ç”¨å‘é‡ï¼‰"""
        scored = []
        query_lower = query_text.lower()
        
        for idx, msg in enumerate(history_msgs):
            msg_content = msg.get("content", "")
            msg_text = self.extract_text_from_content(msg_content)
            msg_lower = msg_text.lower()
            
            # åŸºäºæ–‡æœ¬åŒ¹é…çš„ç®€å•ç›¸ä¼¼åº¦
            common_words = set(query_lower.split()) & set(msg_lower.split())
            text_sim = len(common_words) / max(1, len(set(query_lower.split())))
            
            # é¢å¤–æƒé‡è®¡ç®—
            recency = idx / max(1, len(history_msgs) - 1)
            role = msg.get("role", "")
            role_weight = 1.0 if role == "user" else (0.8 if role == "assistant" else 0.6)
            kw_bonus = 1.0 if self.is_high_priority_content(msg_text) else 0.0
            
            # ç»¼åˆåˆ†æ•°ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦60% + æ—¶é—´æƒé‡20% +è§’è‰²æƒé‡10% + å…³é”®è¯æƒé‡10%
            score = 0.6 * text_sim + 0.2 * recency + 0.1 * role_weight + 0.1 * kw_bonus
            
            scored.append({
                "msg": msg,
                "score": score,
                "tokens": self.count_message_tokens(msg),
                "idx": idx,
                "sim": text_sim,
                "recency": recency,
                "role_weight": role_weight,
                "kw_bonus": kw_bonus
            })
        
        return scored

    async def _compute_vector_scores_concurrent(self, query_vector: List[float], history_msgs: List[dict], progress: ProgressTracker) -> List[dict]:
        """å¹¶å‘è®¡ç®—å‘é‡åˆ†æ•°"""
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.valves.max_concurrent_requests)
        
        async def get_msg_embedding(msg, idx):
            async with semaphore:
                msg_content = msg.get("content", "")
                msg_text = self.extract_text_from_content(msg_content)
                
                if self.has_images_in_content(msg_content):
                    msg_vector = await self.get_multimodal_embedding(msg_content, progress.event_emitter)
                    if not msg_vector:
                        msg_vector = await self.get_text_embedding(msg_text, progress.event_emitter)
                else:
                    msg_vector = await self.get_text_embedding(msg_text, progress.event_emitter)
                
                return idx, msg_vector
        
        # å¹¶å‘è·å–æ‰€æœ‰å‘é‡
        embedding_tasks = [get_msg_embedding(msg, idx) for idx, msg in enumerate(history_msgs)]
        embedding_results = await asyncio.gather(*embedding_tasks)
        
        # è®¡ç®—åˆ†æ•°
        scored = []
        for idx, msg in enumerate(history_msgs):
            # æ‰¾åˆ°å¯¹åº”çš„å‘é‡ç»“æœ
            msg_vector = None
            for result_idx, vector in embedding_results:
                if result_idx == idx:
                    msg_vector = vector
                    break
            
            msg_text = self.extract_text_from_content(msg.get("content", ""))
            
            # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
            sim = self.cosine_similarity(query_vector, msg_vector) if (query_vector and msg_vector) else 0.0
            
            # é¢å¤–æƒé‡è®¡ç®—
            recency = idx / max(1, len(history_msgs) - 1)
            role = msg.get("role", "")
            role_weight = 1.0 if role == "user" else (0.8 if role == "assistant" else 0.6)
            kw_bonus = 1.0 if self.is_high_priority_content(msg_text) else 0.0
            
            # ç»¼åˆåˆ†æ•°ï¼šå‘é‡ç›¸ä¼¼åº¦60% + æ—¶é—´æƒé‡20% + è§’è‰²æƒé‡10% + å…³é”®è¯æƒé‡10%
            score = 0.6 * sim + 0.2 * recency + 0.1 * role_weight + 0.1 * kw_bonus
            
            scored.append({
                "msg": msg,
                "score": score,
                "tokens": self.count_message_tokens(msg),
                "idx": idx,
                "sim": sim,
                "recency": recency,
                "role_weight": role_weight,
                "kw_bonus": kw_bonus
            })
        
        return scored

    # ========== å‡çº§ç­–ç•¥ï¼ˆé˜²é¢„ç®—è¢«åƒå…‰ï¼‰ ==========
    def select_preserve_upgrades_with_protection(self, scored_msgs: List[dict], coverage_entries: List[dict], total_budget: int) -> Tuple[set, int]:
        """é€‰æ‹©å‡çº§çš„æ¶ˆæ¯ï¼ˆé˜²é¢„ç®—è¢«åƒå…‰ç‰ˆæœ¬ï¼‰"""
        # å…ˆé¢„ç•™å‡çº§æ± 
        upgrade_pool = int(total_budget * self.valves.upgrade_min_pct)
        if upgrade_pool <= 0 or not scored_msgs:
            return set(), 0
        
        self.debug_log(1, f"â¬†ï¸ å‡çº§æ± ä¿æŠ¤: é¢„ç•™{upgrade_pool:,}tokens({self.valves.upgrade_min_pct:.1%})ç»™å‡çº§", "â¬†ï¸")
        
        # å»ºç«‹æ¶ˆæ¯IDåˆ°æ‘˜è¦æˆæœ¬çš„æ˜ å°„
        summary_cost_map = defaultdict(int)
        for entry in coverage_entries:
            if entry["type"] == "micro":
                summary_cost_map[entry["msg_id"]] = entry.get("budget", entry.get("ideal_budget", 0))
        
        # æ„å»ºå‡çº§å€™é€‰åˆ—è¡¨
        candidates = []
        for item in scored_msgs:
            msg = item["msg"]
            msg_id = msg.get("_order_id", f"msg_{item['idx']}")
            original_tokens = item["tokens"]
            summary_cost = summary_cost_map.get(msg_id, 0)
            
            if summary_cost > 0:
                upgrade_cost = max(0, original_tokens - summary_cost)
            else:
                upgrade_cost = original_tokens
            
            if upgrade_cost <= 0:
                continue
            
            score = item["score"]
            # æœ€è¿‘æ€§æƒé‡ï¼Œä½†è®¾ä¸Šé™é˜²æ­¢æé•¿æ¶ˆæ¯æŒ¤çˆ†æ± å­
            if item["recency"] > 0.8:
                recency_boost = min(1.2, 1.0 + 0.2 * (2000 / max(upgrade_cost, 1)))  # æˆæœ¬è¶Šé«˜ï¼ŒåŠ æƒè¶Šå°‘
                score *= recency_boost
            
            density = score / upgrade_cost
            candidates.append({
                "density": density,
                "score": score,
                "upgrade_cost": upgrade_cost,
                "item": item,
                "msg_id": msg_id
            })
        
        # æŒ‰ä»·å€¼å¯†åº¦æ’åº
        candidates.sort(key=lambda x: (-x["density"], -x["score"]))
        
        # è´ªå¿ƒé€‰æ‹©ï¼Œä½¿ç”¨å‡çº§æ± é¢„ç®—
        preserve_set = set()
        consumed = 0
        
        self.debug_log(2, f"â¬†ï¸ å‡çº§å€™é€‰: {len(candidates)}ä¸ªï¼Œå‡çº§æ± é¢„ç®—{upgrade_pool:,}tokens", "â¬†ï¸")
        
        for cand in candidates:
            if consumed + cand["upgrade_cost"] > upgrade_pool:
                continue
            preserve_set.add(cand["msg_id"])
            consumed += cand["upgrade_cost"]
            self.debug_log(3, f"â¬†ï¸ å‡çº§é€‰ä¸­: ID={cand['msg_id'][:8]}, å¯†åº¦={cand['density']:.4f}, æˆæœ¬={cand['upgrade_cost']}tokens", "â¬†ï¸")
        
        self.debug_log(1, f"â¬†ï¸ å‡çº§é€‰æ‹©å®Œæˆ: {len(preserve_set)}æ¡æ¶ˆæ¯å‡çº§, æ¶ˆè€—{consumed:,}/{upgrade_pool:,}tokens", "â¬†ï¸")
        
        return preserve_set, consumed

    # ========== æ‘˜è¦ç”Ÿæˆï¼ˆä½¿ç”¨ç¼©æ”¾åé¢„ç®—ï¼‰ ==========
    async def generate_micro_summary_with_budget_impl(self, msg: dict, budget: int, event_emitter):
        """ç”Ÿæˆå•æ¡æ¶ˆæ¯çš„å¾®æ‘˜è¦ï¼ˆä½¿ç”¨ç¼©æ”¾åçš„é¢„ç®—ï¼‰"""
        client = self.get_api_client()
        if not client:
            return None
        
        content = self.extract_text_from_content(msg.get("content", ""))
        role = msg.get("role", "")
        cleaned_content = self.input_cleaner.clean_text_for_regex(content)
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ¶ˆæ¯ç”Ÿæˆç®€æ´æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚è¦æ±‚ï¼š
1. ä¸¥æ ¼åœ¨{budget}ä¸ªtokensä»¥å†…
2. ä¿ç•™æ—¶é—´ã€ä¸»ä½“ã€åŠ¨ä½œã€æ•°æ®/ä»£ç å…³é”®è¡Œç­‰æ ¸å¿ƒè¦ç´ 
3. å¦‚æœæ˜¯æŠ€æœ¯å†…å®¹ï¼Œä¿ç•™æŠ€æœ¯æœ¯è¯­å’Œå…³é”®å‚æ•°
4. ä¿æŒå®¢è§‚ç®€æ´

æ¶ˆæ¯è§’è‰²: {role}
æ¶ˆæ¯å†…å®¹: {cleaned_content[:2000]}

æ‘˜è¦ï¼š"""
        
        has_multimodal = self.has_images_in_content(msg.get("content"))
        model_to_use = self.valves.multimodal_model if has_multimodal else self.valves.text_model
        
        response = await client.chat.completions.create(
            model=model_to_use,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=budget,  # ä½¿ç”¨ç¼©æ”¾åçš„é¢„ç®—
            temperature=0.2,
            timeout=self.valves.request_timeout
        )
        
        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            summary = self.input_cleaner.clean_text_for_regex(summary)
            return summary
        return None

    async def generate_adaptive_block_summary_impl(self, msgs: List[dict], idx_range: Tuple[int, int], budget: int, event_emitter):
        """ç”Ÿæˆè‡ªé€‚åº”å—æ‘˜è¦ï¼ˆä½¿ç”¨ç¼©æ”¾åçš„é¢„ç®—ï¼‰"""
        client = self.get_api_client()
        if not client:
            return None
        
        # åˆå¹¶æ¶ˆæ¯å†…å®¹
        combined_content = ""
        has_multimodal = False
        for i, msg in enumerate(msgs):
            role = msg.get("role", "")
            content = self.extract_text_from_content(msg.get("content", ""))
            combined_content += f"[æ¶ˆæ¯{idx_range[0] + i}:{role}] {content}\n\n"
            if self.has_images_in_content(msg.get("content")):
                has_multimodal = True
        
        cleaned_content = self.input_cleaner.clean_text_for_regex(combined_content)
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹è¿ç»­æ¶ˆæ¯å—(ç¬¬{idx_range[0]}åˆ°{idx_range[1]}æ¡)ç”Ÿæˆç»¼åˆæ‘˜è¦ã€‚è¦æ±‚ï¼š
1. ä¸¥æ ¼åœ¨{budget}ä¸ªtokensä»¥å†…
2. è¦†ç›–æ‰€æœ‰è¦ç‚¹ï¼Œä¿æŒé€»è¾‘é¡ºåº
3. æŒ‡æ˜æ¶ˆæ¯ç¼–å·èŒƒå›´å’Œä¸»è¦è§’è‰²
4. ä¿ç•™å…³é”®æŠ€æœ¯ç»†èŠ‚ã€æ•°æ®ã€å‚æ•°ç­‰

æ¶ˆæ¯å—å†…å®¹ï¼š
{cleaned_content[:4000]}

å—æ‘˜è¦ï¼š"""
        
        model_to_use = self.valves.multimodal_model if has_multimodal else self.valves.text_model
        
        response = await client.chat.completions.create(
            model=model_to_use,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=budget,  # ä½¿ç”¨ç¼©æ”¾åçš„é¢„ç®—
            temperature=0.2,
            timeout=self.valves.request_timeout
        )
        
        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            summary = self.input_cleaner.clean_text_for_regex(summary)
            return summary
        return None

    async def generate_global_block_summary_impl(self, msgs: List[dict], idx_range: Tuple[int, int], budget: int, event_emitter):
        """ç”Ÿæˆå…¨å±€å—æ‘˜è¦"""
        client = self.get_api_client()
        if not client:
            return None
        
        # é‡‡æ ·å…³é”®æ¶ˆæ¯ï¼Œé¿å…å†…å®¹è¿‡é•¿
        sampled_msgs = msgs[::max(1, len(msgs) // 10)]  # æœ€å¤šé‡‡æ ·10æ¡
        combined_content = ""
        has_multimodal = False
        
        for i, msg in enumerate(sampled_msgs):
            role = msg.get("role", "")
            content = self.extract_text_from_content(msg.get("content", ""))
            combined_content += f"[æ¶ˆæ¯æ ·æœ¬{i}:{role}] {content[:200]}...\n\n"
            if self.has_images_in_content(msg.get("content")):
                has_multimodal = True
        
        cleaned_content = self.input_cleaner.clean_text_for_regex(combined_content)
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹å¯¹è¯å†å²ç”Ÿæˆå…¨å±€æ‘˜è¦ã€‚è¦æ±‚ï¼š
1. ä¸¥æ ¼åœ¨{budget}ä¸ªtokensä»¥å†…
2. æ¦‚æ‹¬ä¸»è¦è¯é¢˜å’Œè®¨è®ºè¦ç‚¹
3. ä¿ç•™é‡è¦çš„æŠ€æœ¯ç»†èŠ‚å’Œç»“è®º
4. æ€»å…±æ¶µç›–{len(msgs)}æ¡å†å²æ¶ˆæ¯

å¯¹è¯å†å²æ ·æœ¬ï¼š
{cleaned_content[:5000]}

å…¨å±€æ‘˜è¦ï¼š"""
        
        model_to_use = self.valves.multimodal_model if has_multimodal else self.valves.text_model
        
        response = await client.chat.completions.create(
            model=model_to_use,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=budget,
            temperature=0.3,
            timeout=self.valves.request_timeout
        )
        
        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            summary = self.input_cleaner.clean_text_for_regex(summary)
            return summary
        return None

    async def generate_coverage_summaries_with_budgets(self, coverage_entries: List[dict], progress: ProgressTracker) -> Dict[str, str]:
        """ç”Ÿæˆè¦†ç›–æ‘˜è¦ï¼ˆä½¿ç”¨ç¼©æ”¾åçš„é¢„ç®—ï¼‰"""
        if not coverage_entries:
            return {}
        
        self.debug_log(1, f"ğŸ“ å¼€å§‹ç”Ÿæˆè¦†ç›–æ‘˜è¦: {len(coverage_entries)}ä¸ªæ¡ç›®", "ğŸ“")
        
        summaries = {}
        
        # åˆ†ç±»å¤„ç†
        micro_entries = [e for e in coverage_entries if e["type"] == "micro"]
        adaptive_block_entries = [e for e in coverage_entries if e["type"] == "adaptive_block"]
        global_block_entries = [e for e in coverage_entries if e["type"] == "global_block"]
        
        # ç”Ÿæˆå¾®æ‘˜è¦
        for entry in micro_entries:
            msg = entry["msg"]
            budget = entry.get("budget", entry.get("ideal_budget", self.valves.coverage_high_summary_tokens))
            msg_id = entry["msg_id"]
            
            summary = await self.safe_api_call(
                self.generate_micro_summary_with_budget_impl, "å¾®æ‘˜è¦ç”Ÿæˆ",
                msg, budget, progress.event_emitter
            )
            
            if summary:
                summaries[msg_id] = summary
                self.stats.coverage_micro_summaries += 1
                self.debug_log(3, f"ğŸ“ å¾®æ‘˜è¦ç”Ÿæˆ: {msg_id[:8]} -> {len(summary)}å­—ç¬¦ (é¢„ç®—{budget})", "ğŸ“")
            else:
                content = self.extract_text_from_content(msg.get("content", ""))
                fallback_summary = content[:budget*3] + "..." if len(content) > budget*3 else content
                summaries[msg_id] = f"[ç®€åŒ–æ‘˜è¦] {fallback_summary}"
                self.stats.guard_b_fallbacks += 1
                self.debug_log(2, f"ğŸ“ å¾®æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰: {msg_id[:8]}", "âš ï¸")
        
        # ç”Ÿæˆè‡ªé€‚åº”å—æ‘˜è¦
        for entry in adaptive_block_entries:
            msgs = entry["msgs"]
            idx_range = entry["idx_range"]
            budget = entry.get("budget", entry.get("ideal_budget", self.valves.coverage_block_summary_tokens))
            block_key = entry["block_key"]
            
            summary = await self.safe_api_call(
                self.generate_adaptive_block_summary_impl, "è‡ªé€‚åº”å—æ‘˜è¦ç”Ÿæˆ",
                msgs, idx_range, budget, progress.event_emitter
            )
            
            if summary:
                summaries[block_key] = summary
                self.stats.coverage_block_summaries += 1
                self.stats.adaptive_blocks_created += 1
                self.debug_log(3, f"ğŸ“ è‡ªé€‚åº”å—æ‘˜è¦ç”Ÿæˆ: ç¬¬{idx_range[0]}-{idx_range[1]}æ¡ -> {len(summary)}å­—ç¬¦ (é¢„ç®—{budget})", "ğŸ“")
            else:
                combined = " ".join([
                    f"[{msg.get('role','')}]{self.extract_text_from_content(msg.get('content',''))[:100]}..."
                    for msg in msgs
                ])
                summaries[block_key] = f"[ç®€åŒ–å—æ‘˜è¦] ç¬¬{idx_range[0]}-{idx_range[1]}æ¡: {combined}"
                self.stats.guard_b_fallbacks += 1
                self.debug_log(2, f"ğŸ“ è‡ªé€‚åº”å—æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰: ç¬¬{idx_range[0]}-{idx_range[1]}æ¡", "âš ï¸")
        
        # ç”Ÿæˆå…¨å±€å—æ‘˜è¦
        for entry in global_block_entries:
            msgs = entry["msgs"]
            idx_range = entry["idx_range"]
            budget = entry.get("budget", self.valves.min_block_summary_tokens)
            block_key = entry["block_key"]
            
            summary = await self.safe_api_call(
                self.generate_global_block_summary_impl, "å…¨å±€å—æ‘˜è¦ç”Ÿæˆ",
                msgs, idx_range, budget, progress.event_emitter
            )
            
            if summary:
                summaries[block_key] = summary
                self.stats.coverage_block_summaries += 1
                self.debug_log(3, f"ğŸ“ å…¨å±€å—æ‘˜è¦ç”Ÿæˆ: å…¨å±€æ‘˜è¦ -> {len(summary)}å­—ç¬¦ (é¢„ç®—{budget})", "ğŸ“")
            else:
                summaries[block_key] = f"[å…¨å±€ç®€åŒ–æ‘˜è¦] åŒ…å«{len(msgs)}æ¡å†å²æ¶ˆæ¯çš„å¯¹è¯å†…å®¹"
                self.stats.guard_b_fallbacks += 1
                self.debug_log(2, f"ğŸ“ å…¨å±€å—æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰", "âš ï¸")
        
        self.debug_log(1, f"ğŸ“ è¦†ç›–æ‘˜è¦ç”Ÿæˆå®Œæˆ: {len(summaries)}ä¸ªæ‘˜è¦", "ğŸ“")
        return summaries

    # ========== ç»„è£…é˜¶æ®µåŒé‡æŠ¤æ  ==========
    async def assemble_coverage_output_with_guards(
        self,
        history_messages: List[dict],
        preserve_set: set,
        coverage_entries: List[dict],
        summaries: Dict[str, str],
        progress: ProgressTracker
    ) -> List[dict]:
        """ç»„è£…æœ€ç»ˆè¾“å‡ºï¼ˆåŒé‡æŠ¤æ ç‰ˆæœ¬ï¼‰"""
        if not history_messages:
            return []
        
        self.debug_log(1, f"ğŸ”§ å¼€å§‹ç»„è£…æœ€ç»ˆè¾“å‡º: {len(history_messages)}æ¡å†å²æ¶ˆæ¯", "ğŸ”§")
        
        # æŠ¤æ Aï¼šæ ¡éªŒå¹¶æ—¥å¿—æ‰“å°å„ç§æ¡ç›®æ•°é‡
        micro_entries = [e for e in coverage_entries if e["type"] == "micro"]
        adaptive_block_entries = [e for e in coverage_entries if e["type"] == "adaptive_block"]
        global_block_entries = [e for e in coverage_entries if e["type"] == "global_block"]
        
        print(f"ğŸ›¡ï¸ æŠ¤æ Aç»Ÿè®¡:")
        print(f"    â”œâ”€ åŸæ–‡ä¿ç•™é›†åˆ: {len(preserve_set)}æ¡")
        print(f"    â”œâ”€ å¾®æ‘˜è¦æ¡ç›®: {len(micro_entries)}æ¡")
        print(f"    â”œâ”€ è‡ªé€‚åº”å—æ¡ç›®: {len(adaptive_block_entries)}æ¡")
        print(f"    â”œâ”€ å…¨å±€å—æ¡ç›®: {len(global_block_entries)}æ¡")
        print(f"    â”œâ”€ ç”Ÿæˆæ‘˜è¦æ€»æ•°: {len(summaries)}ä¸ª")
        print(f"    â””â”€ å†å²æ¶ˆæ¯æ€»æ•°: {len(history_messages)}æ¡")
        
        # æ‰“å°å‰å‡ ä¸ªæœªå‘½ä¸­microçš„msg_id
        all_micro_msg_ids = {e["msg_id"] for e in micro_entries}
        all_msg_ids = {msg.get("_order_id", f"msg_{i}") for i, msg in enumerate(history_messages)}
        unmapped_msg_ids = all_msg_ids - all_micro_msg_ids
        
        if unmapped_msg_ids:
            unmapped_sample = list(unmapped_msg_ids)[:3]
            print(f"ğŸ›¡ï¸ æŠ¤æ Aè­¦å‘Š: {len(unmapped_msg_ids)}æ¡æ¶ˆæ¯æœªæ˜ å°„åˆ°å¾®æ‘˜è¦: {unmapped_sample}...")
            self.stats.guard_a_warnings += 1
        
        # å»ºç«‹æ˜ å°„
        msg_id_to_msg = {msg.get("_order_id", f"msg_{i}"): msg for i, msg in enumerate(history_messages)}
        
        # å»ºç«‹å—æ‘˜è¦æ˜ å°„
        block_summaries = {}
        block_ranges = {}
        entry_idx_ranges = {}  # å­˜å‚¨æ¯ä¸ªblock_keyçš„idx_range
        
        for entry in adaptive_block_entries + global_block_entries:
            idx_range = entry["idx_range"]
            block_key = entry.get("block_key", f"block_{idx_range[0]}_{idx_range[1]}")
            entry_idx_ranges[block_key] = idx_range
            
            if block_key in summaries:
                block_summaries[block_key] = summaries[block_key]
                # è®°å½•è¿™ä¸ªèŒƒå›´å†…çš„æ‰€æœ‰æ¶ˆæ¯ç´¢å¼•
                for idx in range(idx_range[0], idx_range[1] + 1):
                    if idx < len(history_messages):
                        block_ranges[idx] = block_key
        
        # è®¡ç®—å·²è¢«micro/preserveè¦†ç›–çš„ç´¢å¼•
        covered_by_micro_or_preserve = set()
        for i, msg in enumerate(history_messages):
            mid = msg.get("_order_id", f"msg_{i}")
            if mid in preserve_set or mid in summaries:  # micro å·²ç”Ÿæˆ
                covered_by_micro_or_preserve.add(i)
        
        final_messages = []
        processed_block_keys = set()
        
        # æŒ‰åŸå§‹é¡ºåºéå†å†å²æ¶ˆæ¯
        for idx, msg in enumerate(history_messages):
            msg_id = msg.get("_order_id", f"msg_{idx}")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨preserveé›†åˆä¸­ï¼ˆå‡çº§ä¸ºåŸæ–‡ï¼‰
            if msg_id in preserve_set:
                final_messages.append(msg)
                self.stats.coverage_preserved_count += 1
                self.stats.coverage_preserved_tokens += self.count_message_tokens(msg)
                self.debug_log(3, f"ğŸ”§ ä½¿ç”¨åŸæ–‡: {msg_id[:8]}", "ğŸ“„")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¾®æ‘˜è¦
            elif msg_id in summaries:
                summary_msg = {
                    "role": "assistant",
                    "content": summaries[msg_id],
                    "_is_summary": True,
                    "_original_msg_id": msg_id,
                    "_summary_type": "micro"
                }
                final_messages.append(summary_msg)
                self.stats.coverage_summary_count += 1
                self.stats.coverage_summary_tokens += self.count_message_tokens(summary_msg)
                self.debug_log(3, f"ğŸ”§ ä½¿ç”¨å¾®æ‘˜è¦: {msg_id[:8]}", "ğŸ“„")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æŸä¸ªå—æ‘˜è¦ä¸­
            elif idx in block_ranges:
                block_key = block_ranges[idx]
                if block_key not in processed_block_keys and block_key in block_summaries:
                    # æ£€æŸ¥è¿™ä¸ªå—èŒƒå›´é‡Œæ˜¯å¦è¿˜æœ‰æœªè¢«è¦†ç›–çš„ç´¢å¼•
                    idx0, idx1 = entry_idx_ranges[block_key]
                    has_uncovered = any(j not in covered_by_micro_or_preserve for j in range(idx0, idx1 + 1) if j < len(history_messages))
                    
                    if has_uncovered:
                        block_summary_msg = {
                            "role": "assistant",
                            "content": block_summaries[block_key],
                            "_is_summary": True,
                            "_block_key": block_key,
                            "_summary_type": "adaptive_block" if "global" not in block_key else "global_block"
                        }
                        final_messages.append(block_summary_msg)
                        processed_block_keys.add(block_key)
                        self.stats.coverage_summary_count += 1
                        self.stats.coverage_summary_tokens += self.count_message_tokens(block_summary_msg)
                        self.debug_log(3, f"ğŸ”§ ä½¿ç”¨å—æ‘˜è¦: {block_key}", "ğŸ“„")
                # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªæˆ–æ²¡æœ‰æœªè¦†ç›–ç´¢å¼•ï¼Œè·³è¿‡ï¼ˆå·²ç»ç”±å—æ‘˜è¦è¦†ç›–ï¼‰
            else:
                # æŠ¤æ Bï¼šè®¡åˆ’microä½†æœ€ç»ˆæ²¡è½åœ°çš„æ¡ç›®ï¼Œå›é€€æ”¾ç®€åŒ–æ‘˜è¦
                self.debug_log(1, f"ğŸ›¡ï¸ æŠ¤æ Bè§¦å‘ï¼šæ¶ˆæ¯{msg_id[:8]}æ—¢ä¸åœ¨preserveä¹Ÿä¸åœ¨coverageä¸­", "ğŸ›¡ï¸")
                content = self.extract_text_from_content(msg.get("content", ""))
                fallback_msg = {
                    "role": "assistant",
                    "content": f"[æŠ¤æ Bç®€åŒ–æ‘˜è¦] {content[:200]}...",
                    "_is_summary": True,
                    "_original_msg_id": msg_id,
                    "_summary_type": "guard_b_fallback"
                }
                final_messages.append(fallback_msg)
                self.stats.guard_b_fallbacks += 1
                self.stats.coverage_summary_count += 1
                self.stats.coverage_summary_tokens += self.count_message_tokens(fallback_msg)
        
        # ç¡®ä¿æ¶ˆæ¯é¡ºåºæ­£ç¡®
        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )
        
        # æ›´æ–°coverageç»Ÿè®¡
        self.stats.coverage_total_messages = len(history_messages)
        self.stats.coverage_rate = 1.0  # åº”è¯¥æ˜¯100%è¦†ç›–
        
        # æŠ¤æ Aæœ€ç»ˆéªŒè¯
        final_tokens = self.count_messages_tokens(final_messages)
        cu_id = self.current_user_message.get("_order_id") if self.current_user_message else None
        user_position_check = "åœ¨æœ€åä½ç½®" if final_messages and final_messages[-1].get("_order_id") == cu_id else "ä½ç½®éªŒè¯å¤±è´¥"
        
        print(f"ğŸ›¡ï¸ æŠ¤æ Aæœ€ç»ˆéªŒè¯:")
        print(f"    â”œâ”€ æœ€ç»ˆæ¶ˆæ¯æ•°: åŸæ–‡{self.stats.coverage_preserved_count}æ¡ + æ‘˜è¦{self.stats.coverage_summary_count}æ¡ = {len(final_messages)}æ¡")
        print(f"    â”œâ”€ è¦†ç›–ç‡éªŒè¯: {self.stats.coverage_rate:.1%} (åº”è¯¥=100%)")
        print(f"    â”œâ”€ å½“å‰ç”¨æˆ·æ¶ˆæ¯: {user_position_check}")
        print(f"    â””â”€ æœ€ç»ˆtokenç»Ÿè®¡: {final_tokens:,}tokens")
        
        self.debug_log(1, f"ğŸ”§ åŒé‡æŠ¤æ ç»„è£…å®Œæˆ: {len(history_messages)} -> {len(final_messages)}æ¡æ¶ˆæ¯({final_tokens:,}tokens)", "âœ…")
        
        return final_messages

    # ========== Top-upçª—å£å¡«å……å™¨ï¼ˆä¿®å¤ç»Ÿè®¡é—®é¢˜ï¼‰ ==========
    def topup_fill_window(self, final_messages: List[dict], scored_msgs: List[dict], available_tokens: int, summaries: Dict[str, str], preserve_set: set) -> List[dict]:
        """Top-upå¡«å……å™¨ï¼šæŠŠçª—å£åˆ©ç”¨ç‡æå‡åˆ°ç›®æ ‡å€¼ - ä¿®å¤ç»Ÿè®¡é—®é¢˜"""
        # ä¸€å¼€å¤´å°±å­˜åŸºçº¿
        initial_tokens = self.count_messages_tokens(final_messages)
        current_tokens = initial_tokens
        target_tokens = int(available_tokens * self.valves.target_window_usage)  # 85%
        
        if current_tokens >= target_tokens:
            self.debug_log(1, f"ğŸ”¥ çª—å£åˆ©ç”¨ç‡å·²è¾¾æ ‡: {current_tokens:,}/{target_tokens:,} tokens", "ğŸ”¥")
            return final_messages
        
        self.debug_log(1, f"ğŸ”¥ å¼€å§‹Top-upå¡«å……: {current_tokens:,} -> {target_tokens:,} tokens", "ğŸ”¥")
        self.stats.topup_applied += 1
        
        # 1) å…ˆæŠŠå·²æœ‰ micro å‡çº§ä¸ºåŸæ–‡ï¼ˆæ›¿æ¢æ‰ microï¼‰
        # æŒ‰ä»·å€¼å¯†åº¦ï¼ˆscore/tokensï¼‰ä»é«˜åˆ°ä½
        taken_micro = {m.get("_original_msg_id") for m in final_messages if m.get("_summary_type") == "micro"}
        id2msg = {item["msg"].get("_order_id", f"msg_{item['idx']}"): item for item in scored_msgs}
        
        micro_ids_sorted = sorted(
            [mid for mid in taken_micro if mid in id2msg],
            key=lambda mid: id2msg[mid]["score"] / max(1, id2msg[mid]["tokens"]),
            reverse=True
        )
        
        upgraded_count = 0
        for mid in micro_ids_sorted:
            item = id2msg[mid]
            raw_msg = item["msg"]
            raw_tokens = self.count_message_tokens(raw_msg)
            
            # æ‰¾åˆ°å¯¹åº”çš„microæ‘˜è¦æ¶ˆæ¯
            micro_msg = None
            for i, msg in enumerate(final_messages):
                if msg.get("_original_msg_id") == mid:
                    micro_msg = msg
                    break
            
            if not micro_msg:
                continue
            
            micro_tokens = self.count_message_tokens(micro_msg)
            token_diff = raw_tokens - micro_tokens
            
            if current_tokens + token_diff > available_tokens:
                continue
            
            # åˆ é™¤è¯¥æ¡ microï¼ŒåŠ å…¥åŸæ–‡
            final_messages = [m for m in final_messages if m.get("_original_msg_id") != mid]
            final_messages.append(raw_msg)
            current_tokens += token_diff
            upgraded_count += 1
            self.stats.topup_micro_upgraded += 1
            self.debug_log(3, f"ğŸ”¥ å¾®æ‘˜è¦å‡çº§ä¸ºåŸæ–‡: {mid[:8]}, å¢åŠ {token_diff}tokens", "â¬†ï¸")
            
            if current_tokens >= target_tokens:
                break
        
        if upgraded_count > 0:
            self.debug_log(1, f"ğŸ”¥ å¾®æ‘˜è¦å‡çº§å®Œæˆ: {upgraded_count}æ¡å‡çº§", "â¬†ï¸")
        
        # 2) å†ä»æœªè½åœ°çš„æ¶ˆæ¯é‡Œï¼ŒæŒ‰ä»·å€¼å¯†åº¦è´ªå¿ƒåŠ å…¥åŸæ–‡
        landed_ids = {
            m.get("_order_id") or m.get("_original_msg_id")
            for m in final_messages
        }
        
        candidates = [it for it in scored_msgs if it["msg"].get("_order_id") not in landed_ids]
        candidates.sort(key=lambda it: it["score"]/max(1,it["tokens"]), reverse=True)
        
        added_count = 0
        for item in candidates:
            tokens = item["tokens"]
            if current_tokens + tokens > available_tokens:
                continue
            
            final_messages.append(item["msg"])
            current_tokens += tokens
            added_count += 1
            self.stats.topup_raw_added += 1
            self.debug_log(3, f"ğŸ”¥ æ·»åŠ æœªè½åœ°åŸæ–‡: {item['msg'].get('_order_id', 'unknown')[:8]}, å¢åŠ {tokens}tokens", "ğŸ“")
            
            if current_tokens >= target_tokens:
                break
        
        if added_count > 0:
            self.debug_log(1, f"ğŸ”¥ æœªè½åœ°åŸæ–‡æ·»åŠ å®Œæˆ: {added_count}æ¡æ·»åŠ ", "ğŸ“")
        
        # 3) é‡æ–°æ’åº
        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )
        
        # ä¿®å¤ç»Ÿè®¡è®¡ç®—ï¼šç”¨åŸºçº¿åšå·®
        final_tokens = self.count_messages_tokens(final_messages)
        tokens_added = max(0, final_tokens - initial_tokens)
        self.stats.topup_tokens_added += tokens_added
        
        utilization = final_tokens / available_tokens if available_tokens > 0 else 0
        self.debug_log(1, f"ğŸ”¥ Top-upå¡«å……å®Œæˆ: {final_tokens:,}tokens, åˆ©ç”¨ç‡{utilization:.1%}, æ–°å¢{tokens_added:,}tokens", "âœ…")
        
        return final_messages

    # ========== Coverage-Firstä¸»æµç¨‹ï¼ˆé‡æ„ç‰ˆï¼‰ ==========
    async def process_coverage_first_context_maximization_v2(
        self, history_messages: List[dict], available_tokens: int, progress: ProgressTracker, query_message: dict
    ) -> List[dict]:
        """Coverage-Firstä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†ä¸»æµç¨‹ v2.4.4"""
        if not history_messages or not self.valves.enable_coverage_first:
            return history_messages
        
        await progress.start_phase("Coverage-First v2.4.4å¤„ç†", len(history_messages))
        self.debug_log(1, f"ğŸ¯ Coverage-First v2.4.4å¼€å§‹: {len(history_messages)}æ¡æ¶ˆæ¯, å¯ç”¨é¢„ç®—: {available_tokens:,}tokens", "ğŸ¯")
        
        # Step 0: æ¶ˆæ¯åˆ†ç‰‡é¢„å¤„ç†
        if self.valves.enable_smart_chunking:
            await progress.update_progress(0, 8, "æ¶ˆæ¯åˆ†ç‰‡é¢„å¤„ç†")
            processed_history = self.message_chunker.preprocess_messages_with_chunking(
                history_messages, self.message_order
            )
            self.stats.chunked_messages_count = len([msg for msg in processed_history if msg.get("_is_chunk")])
            self.stats.total_chunks_created = len(processed_history) - len(history_messages) + self.stats.chunked_messages_count
            self.debug_log(1, f"ğŸ§© æ¶ˆæ¯åˆ†ç‰‡é¢„å¤„ç†: {len(history_messages)} -> {len(processed_history)}æ¡ "
                              f"({self.stats.chunked_messages_count}æ¡è¢«åˆ†ç‰‡)", "ğŸ§©")
        else:
            processed_history = history_messages
        
        # Step 1: è®¡ç®—ç›¸å…³åº¦åˆ†æ•°ï¼ˆå¹¶å‘ä¼˜åŒ–ï¼‰
        await progress.update_progress(1, 8, "è®¡ç®—ç›¸å…³åº¦åˆ†æ•°")
        scored_msgs = await self.compute_relevance_scores(query_message, processed_history, progress)
        if not scored_msgs:
            self.debug_log(1, "ç›¸å…³åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯", "âš ï¸")
            return processed_history
        
        # Step 2: è‡ªé€‚åº”Coverageè§„åˆ’ï¼ˆæŒ‰åŸæ–‡tokené‡åˆ†å— + ä¸€æ¬¡æ€§ç¼©æ”¾ï¼‰
        await progress.update_progress(2, 8, "è‡ªé€‚åº”Coverageè§„åˆ’")
        # ä¸ºè¦†ç›–åˆ†é…é¢„ç®—ï¼ˆå…ˆé¢„ç•™å‡çº§æ± ï¼‰
        upgrade_pool = int(available_tokens * self.valves.upgrade_min_pct)
        coverage_budget = available_tokens - upgrade_pool
        
        coverage_entries, coverage_cost = self.coverage_planner.plan_adaptive_coverage_summaries(
            scored_msgs, coverage_budget
        )
        
        # è®°å½•ç¼©æ”¾ç»Ÿè®¡
        if coverage_cost < coverage_budget:
            # æœ‰ä½™é¢ï¼Œå¢åŠ å‡çº§æ± 
            actual_upgrade_pool = upgrade_pool + (coverage_budget - coverage_cost)
        else:
            actual_upgrade_pool = upgrade_pool
        
        if coverage_cost != coverage_budget:
            self.stats.budget_scaling_applied += 1
            self.stats.scaling_factor = coverage_cost / coverage_budget if coverage_budget > 0 else 1.0
        
        self.debug_log(1, f"ğŸ“„ è‡ªé€‚åº”Coverageè§„åˆ’: {len(coverage_entries)}ä¸ªæ¡ç›®, æˆæœ¬{coverage_cost:,}tokens "
                          f"(å‡çº§æ± {actual_upgrade_pool:,}tokens)", "ğŸ“„")
        
        # Step 3: å‡çº§ç­–ç•¥ï¼ˆé˜²é¢„ç®—è¢«åƒå…‰ï¼‰
        await progress.update_progress(3, 8, "å‡çº§ç­–ç•¥é€‰æ‹©")
        preserve_set, upgrade_consumed = self.select_preserve_upgrades_with_protection(
            scored_msgs, coverage_entries, actual_upgrade_pool
        )
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.coverage_upgrade_count = len(preserve_set)
        self.stats.coverage_upgrade_tokens_saved = upgrade_consumed
        
        # Step 4: ç”Ÿæˆæ‘˜è¦å†…å®¹ï¼ˆä½¿ç”¨ç¼©æ”¾åé¢„ç®—ï¼‰
        await progress.update_progress(4, 8, "ç”Ÿæˆæ‘˜è¦å†…å®¹")
        summaries = await self.generate_coverage_summaries_with_budgets(coverage_entries, progress)
        
        # Step 5: åŒé‡æŠ¤æ ç»„è£…
        await progress.update_progress(5, 8, "åŒé‡æŠ¤æ ç»„è£…")
        final_messages = await self.assemble_coverage_output_with_guards(
            processed_history, preserve_set, coverage_entries, summaries, progress
        )
        
        # Step 6: Top-upçª—å£å¡«å……ï¼ˆä¿®å¤ç»Ÿè®¡ï¼‰
        await progress.update_progress(6, 8, "Top-upçª—å£å¡«å……")
        final_messages = self.topup_fill_window(
            final_messages, scored_msgs, available_tokens, summaries, preserve_set
        )
        
        # Step 7: æœ€ç»ˆç»Ÿè®¡
        await progress.update_progress(7, 8, "æœ€ç»ˆç»Ÿè®¡è®¡ç®—")
        final_tokens = self.count_messages_tokens(final_messages)
        self.stats.coverage_budget_usage = final_tokens / available_tokens if available_tokens > 0 else 0
        
        # ç¡®ä¿æ¶ˆæ¯é¡ºåºæ­£ç¡®
        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )
        
        await progress.update_progress(8, 8, "å¤„ç†å®Œæˆ")
        
        self.debug_log(1, f"ğŸ¯ Coverage-First v2.4.4å®Œæˆ: {len(processed_history)} -> {len(final_messages)}æ¡æ¶ˆæ¯", "âœ…")
        self.debug_log(1, f"ğŸ¯ ç»Ÿè®¡: åŸæ–‡{self.stats.coverage_preserved_count}æ¡ + æ‘˜è¦{self.stats.coverage_summary_count}æ¡", "âœ…")
        self.debug_log(1, f"ğŸ¯ é¢„ç®—: {final_tokens:,}/{available_tokens:,}tokens ({self.stats.coverage_budget_usage:.1%})", "âœ…")
        
        await progress.complete_phase(f"Coverage-First v2.4.4å®Œæˆ è¦†ç›–ç‡100% é¢„ç®—{self.stats.coverage_budget_usage:.1%}")
        return final_messages

    # ========== è§†è§‰å¤„ç† ==========
    def validate_base64_image_data(self, image_data: str) -> bool:
        """éªŒè¯base64å›¾ç‰‡æ•°æ®çš„æœ‰æ•ˆæ€§"""
        return self.input_cleaner.validate_and_clean_data_uri(image_data)[0]

    async def describe_image_impl(self, image_data: str, event_emitter):
        """å®é™…çš„å›¾ç‰‡æè¿°å®ç°"""
        client = self.get_api_client()
        if not client:
            return None
        
        # ä½¿ç”¨InputCleaneréªŒè¯
        is_valid, cleaned_data = self.input_cleaner.validate_and_clean_data_uri(image_data)
        if not is_valid:
            self.debug_log(1, "å›¾ç‰‡æ•°æ®éªŒè¯å¤±è´¥", "âš ï¸")
            self.stats.image_processing_errors += 1
            return "å›¾ç‰‡æ ¼å¼é”™è¯¯ï¼šä¸æ˜¯æœ‰æ•ˆçš„base64æ•°æ®"
        
        try:
            response = await client.chat.completions.create(
                model=self.valves.multimodal_model,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self.valves.vision_prompt_template},
                        {"type": "image_url", "image_url": {"url": cleaned_data}}
                    ]
                }],
                max_tokens=self.valves.vision_max_tokens,
                temperature=0.2,
                timeout=self.valves.request_timeout
            )
            
            if response.choices and response.choices[0].message.content:
                description = response.choices[0].message.content.strip()
                description = self.input_cleaner.clean_text_for_regex(description)
                return description
            else:
                self.stats.image_processing_errors += 1
                return "å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼šAPIè¿”å›ç©ºå“åº”"
        except Exception as e:
            self.debug_log(1, f"å›¾ç‰‡è¯†åˆ«å¼‚å¸¸: {str(e)[:100]}", "âŒ")
            self.stats.image_processing_errors += 1
            return f"å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼š{str(e)[:100]}"

    async def describe_image(self, image_data: str, event_emitter) -> str:
        """æè¿°å•å¼ å›¾ç‰‡"""
        if not image_data:
            return "å›¾ç‰‡æ•°æ®ä¸ºç©º"
        
        description = await self.safe_api_call(
            self.describe_image_impl, "å›¾ç‰‡è¯†åˆ«", image_data, event_emitter
        )
        
        if description:
            if len(description) > 3000:
                description = description[:3000] + "..."
            return description
        else:
            self.stats.image_processing_errors += 1
            return "å›¾ç‰‡å¤„ç†å¤±è´¥ï¼šæ— æ³•è·å–æè¿°"

    async def process_message_images(self, message: dict, progress: ProgressTracker) -> dict:
        """å¤„ç†å•æ¡æ¶ˆæ¯ä¸­çš„å›¾ç‰‡"""
        content = message.get("content", "")
        if not isinstance(content, list):
            return message
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        images = [item for item in content if item.get("type") == "image_url"]
        if not images:
            return message
        
        self.debug_log(2, f"å¤„ç†æ¶ˆæ¯ä¸­çš„å›¾ç‰‡: {len(images)}å¼ ", "ğŸ–¼ï¸")
        
        # å¤„ç†å›¾ç‰‡
        processed_content = []
        image_count = 0
        
        for item in content:
            if item.get("type") == "text":
                text = item.get("text", "")
                if text.strip():
                    processed_content.append(text)
            elif item.get("type") == "image_url":
                image_count += 1
                image_data = item.get("image_url", {}).get("url", "")
                if image_data:
                    if progress:
                        await progress.update_progress(
                            image_count, len(images), f"å¤„ç†å›¾ç‰‡ {image_count}/{len(images)}"
                        )
                    description = await self.describe_image(
                        image_data, progress.event_emitter if progress else None
                    )
                    image_description = f"[å›¾ç‰‡{image_count}æè¿°] {description}"
                    processed_content.append(image_description)
        
        # åˆ›å»ºæ–°æ¶ˆæ¯
        processed_message = copy.deepcopy(message)
        processed_message["content"] = "\n".join(processed_content) if processed_content else ""
        processed_message["_images_processed"] = image_count
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.multimodal_processed += image_count
        
        return processed_message

    # ========== å¤šæ¨¡æ€å¤„ç†ç­–ç•¥ ==========
    def calculate_multimodal_budget_sufficient(self, messages: List[dict], target_tokens: int) -> bool:
        """è®¡ç®—å¤šæ¨¡æ€æ¨¡å‹çš„Tokené¢„ç®—æ˜¯å¦å……è¶³"""
        current_tokens = self.count_messages_tokens(messages)
        usage_ratio = current_tokens / target_tokens if target_tokens > 0 else 1.0
        threshold = self.valves.multimodal_direct_threshold
        is_sufficient = usage_ratio <= threshold
        
        self.debug_log(1, f"ğŸ¯ å¤šæ¨¡æ€é¢„ç®—æ£€æŸ¥: {current_tokens:,}/{target_tokens:,} = {usage_ratio:.2%} "
                          f"{'â‰¤' if is_sufficient else '>'} {threshold:.1%}", "ğŸ’°")
        return is_sufficient

    async def determine_multimodal_processing_strategy(
        self, messages: List[dict], model_name: str, target_tokens: int
    ) -> Tuple[str, str]:
        """ç¡®å®šå¤šæ¨¡æ€å¤„ç†ç­–ç•¥"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return "text_only", "æ— å›¾ç‰‡å†…å®¹ï¼ŒæŒ‰æ–‡æœ¬å¤„ç†"
        
        # åˆ¤æ–­æ¨¡å‹ç±»å‹
        is_multimodal = self.is_multimodal_model(model_name)
        self.debug_log(1, f"ğŸ¤– æ¨¡å‹åˆ†æ: {model_name} | å¤šæ¨¡æ€æ”¯æŒ: {is_multimodal}", "ğŸ¤–")
        
        if is_multimodal:
            budget_sufficient = self.calculate_multimodal_budget_sufficient(messages, target_tokens)
            if budget_sufficient:
                return "direct_multimodal", "å¤šæ¨¡æ€æ¨¡å‹ï¼ŒTokené¢„ç®—å……è¶³ï¼Œç›´æ¥è¾“å…¥"
            else:
                return "multimodal_rag", "å¤šæ¨¡æ€æ¨¡å‹ï¼ŒTokené¢„ç®—ä¸è¶³ï¼Œä½¿ç”¨å¤šæ¨¡æ€å‘é‡RAG"
        else:
            return "vision_to_text", "çº¯æ–‡æœ¬æ¨¡å‹ï¼Œå…ˆè¯†åˆ«å›¾ç‰‡å†å¤„ç†"

    async def process_multimodal_content(
        self, messages: List[dict], model_name: str, target_tokens: int, progress: ProgressTracker
    ) -> List[dict]:
        """å¤šæ¨¡æ€å†…å®¹å¤„ç†"""
        if not self.valves.enable_multimodal:
            return messages
        
        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return messages
        
        # ç¡®å®šç­–ç•¥
        strategy, strategy_desc = await self.determine_multimodal_processing_strategy(
            messages, model_name, target_tokens
        )
        
        self.debug_log(1, f"ğŸ¯ å¤šæ¨¡æ€ç­–ç•¥: {strategy} - {strategy_desc}", "ğŸ¯")
        
        if strategy == "text_only":
            return messages
        elif strategy == "direct_multimodal":
            return messages
        elif strategy == "vision_to_text":
            await progress.start_phase("è§†è§‰è¯†åˆ«è½¬æ–‡æœ¬", len(messages))
            # å¹¶å‘å¤„ç†
            semaphore = asyncio.Semaphore(self.valves.max_concurrent_requests)
            
            async def process_single_message(i, message):
                if self.has_images_in_content(message.get("content")):
                    async with semaphore:
                        processed_message = await self.process_message_images(message, progress)
                        return processed_message
                else:
                    return message
            
            process_tasks = []
            for i, message in enumerate(messages):
                task = process_single_message(i, message)
                process_tasks.append(task)
            
            processed_messages = await asyncio.gather(*process_tasks)
            
            if self.message_order:
                processed_messages = self.message_order.sort_messages_preserve_user(
                    processed_messages, self.current_user_message
                )
            
            await progress.complete_phase("è§†è§‰è¯†åˆ«å®Œæˆ")
            return processed_messages
        else:
            return messages

    # ========== æ™ºèƒ½æˆªæ–­ ==========
    def smart_truncate_messages(
        self, messages: List[dict], target_tokens: int, preserve_priority: bool = True
    ) -> List[dict]:
        """æ™ºèƒ½æˆªæ–­ç®—æ³• - å…¨é¢ä¿®å¤count_message_tokené”™è¯¯"""
        if not messages:
            return messages
        
        current_tokens = self.count_messages_tokens(messages)
        if current_tokens <= target_tokens:
            return messages
        
        self.debug_log(1, f"âœ‚ï¸ å¼€å§‹æ™ºèƒ½æˆªæ–­: {current_tokens:,} -> {target_tokens:,}tokens", "âœ‚ï¸")
        self.stats.smart_truncation_applied += 1
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        if preserve_priority:
            message_priorities = []
            for i, msg in enumerate(messages):
                priority_score = self._calculate_message_priority(msg, i, len(messages))
                message_priorities.append((i, msg, priority_score))
            message_priorities.sort(key=lambda x: x[2], reverse=True)
        else:
            message_priorities = [(i, msg, 1.0) for i, msg in enumerate(messages)]
        
        # æ™ºèƒ½é€‰æ‹©
        selected_messages = []
        used_tokens = 0
        skipped_messages = []
        
        for original_idx, msg, priority in message_priorities:
            # ä¿®å¤é”™åˆ«å­—ï¼šç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
            msg_tokens = self.count_message_tokens(msg)
            if used_tokens + msg_tokens <= target_tokens:
                selected_messages.append((original_idx, msg, priority))
                used_tokens += msg_tokens
            else:
                skipped_messages.append((original_idx, msg, priority, msg_tokens))
                self.stats.truncation_skip_count += 1
        
        # å¡«è¡¥ç©ºéš™
        remaining_budget = target_tokens - used_tokens
        if remaining_budget > 100 and skipped_messages:
            skipped_messages.sort(key=lambda x: x[3])
            recovered_count = 0
            for original_idx, msg, priority, msg_tokens in skipped_messages:
                if msg_tokens <= remaining_budget:
                    selected_messages.append((original_idx, msg, priority))
                    used_tokens += msg_tokens
                    remaining_budget -= msg_tokens
                    recovered_count += 1
                    if remaining_budget < 100:
                        break
            self.stats.truncation_recovered_messages += recovered_count
        
        # æŒ‰åŸå§‹ç´¢å¼•æ’åº
        selected_messages.sort(key=lambda x: x[0])
        final_messages = [msg for _, msg, _ in selected_messages]
        
        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )
        
        final_tokens = self.count_messages_tokens(final_messages)
        retention_ratio = len(final_messages) / len(messages) if messages else 0
        
        self.debug_log(1, f"âœ‚ï¸ æ™ºèƒ½æˆªæ–­å®Œæˆ: {len(messages)} -> {len(final_messages)}æ¡æ¶ˆæ¯ "
                          f"ä¿ç•™ç‡{retention_ratio:.1%}", "âœ…")
        
        return final_messages

    def _calculate_message_priority(self, msg: dict, index: int, total_count: int) -> float:
        """è®¡ç®—æ¶ˆæ¯ä¼˜å…ˆçº§åˆ†æ•°"""
        priority = 1.0
        
        # è§’è‰²ä¼˜å…ˆçº§
        role = msg.get("role", "")
        if role == "user":
            priority += 2.0
        elif role == "assistant":
            priority += 1.5
        elif role == "system":
            priority += 3.0
        
        # ä½ç½®ä¼˜å…ˆçº§
        position_score = index / total_count if total_count > 0 else 0
        priority += position_score * 2.0
        
        # å†…å®¹ä¼˜å…ˆçº§
        content_text = self.extract_text_from_content(msg.get("content", ""))
        if self.is_high_priority_content(content_text):
            priority += 1.5
        
        # é•¿åº¦å› å­
        content_length = len(content_text)
        if 100 < content_length < 2000:
            priority += 0.5
        elif content_length > 5000:
            priority -= 0.5
        
        # å¤šæ¨¡æ€å†…å®¹ä¼˜å…ˆçº§
        if self.has_images_in_content(msg.get("content")):
            priority += 1.0
        
        # æ‘˜è¦æ¶ˆæ¯çš„ä¼˜å…ˆçº§
        if msg.get("_is_summary"):
            priority += 0.8
        
        # åˆ†ç‰‡æ¶ˆæ¯çš„ä¼˜å…ˆçº§
        if msg.get("_is_chunk"):
            priority += 0.3
        
        return priority

    # ========== ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤ ==========
    def ensure_current_user_message_preserved(self, final_messages: List[dict]) -> List[dict]:
        """ç¡®ä¿å½“å‰ç”¨æˆ·æ¶ˆæ¯è¢«æ­£ç¡®ä¿ç•™åœ¨æœ€åä½ç½®"""
        if not self.current_user_message:
            return final_messages
        
        # æ£€æŸ¥å½“å‰ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦åœ¨æœ€åä½ç½®
        if final_messages and final_messages[-1].get("role") == "user":
            current_id = self.current_user_message.get("_order_id")
            last_id = final_messages[-1].get("_order_id")
            if current_id == last_id:
                return final_messages
        
        # ä¿®å¤ä½ç½®
        self.debug_log(1, "ğŸ›¡ï¸ æ£€æµ‹åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½ç½®é”™è¯¯ï¼Œå¼€å§‹ä¿®å¤", "ğŸ›¡ï¸")
        current_id = self.current_user_message.get("_order_id")
        
        filtered_messages = []
        for msg in final_messages:
            if msg.get("_order_id") != current_id:
                filtered_messages.append(msg)
        
        filtered_messages.append(self.current_user_message)
        self.stats.user_message_recovery_count += 1
        
        self.debug_log(1, "ğŸ›¡ï¸ å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½ç½®ä¿®å¤å®Œæˆ", "ğŸ›¡ï¸")
        return filtered_messages

    # ========== ä¸»è¦å¤„ç†é€»è¾‘ ==========
    def should_force_maximize_content(self, messages: List[dict], target_tokens: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼ºåˆ¶è¿›è¡Œå†…å®¹æœ€å¤§åŒ–å¤„ç†"""
        current_tokens = self.count_messages_tokens(messages)
        utilization = current_tokens / target_tokens if target_tokens > 0 else 0
        
        should_maximize = (
            utilization < self.valves.max_window_utilization
            or current_tokens > target_tokens
        )
        
        self.debug_log(1, f"ğŸ”¥ å†…å®¹æœ€å¤§åŒ–åˆ¤æ–­: {current_tokens:,}tokens / {target_tokens:,}tokens = {utilization:.1%}", "ğŸ”¥")
        self.debug_log(1, f"ğŸ”¥ éœ€è¦æœ€å¤§åŒ–: {should_maximize}", "ğŸ”¥")
        return should_maximize

    async def maximize_content_comprehensive_processing_v2(
        self, messages: List[dict], target_tokens: int, progress: ProgressTracker
    ) -> List[dict]:
        """å†…å®¹æœ€å¤§åŒ–ç»¼åˆå¤„ç† v2.4.4"""
        start_time = time.time()
        
        # è·å–çœŸå®æ¨¡å‹é™åˆ¶
        current_model_name = getattr(self, '_current_model_name', 'unknown')
        if hasattr(self, 'current_model_info') and self.current_model_info:
            model_limit = self.current_model_info.get('limit', self.valves.default_token_limit)
            safe_limit = int(model_limit * self.valves.token_safety_ratio)
        else:
            safe_limit = self.get_model_token_limit(current_model_name)
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        self.stats.original_tokens = self.count_messages_tokens(messages)
        self.stats.original_messages = len(messages)
        self.stats.token_limit = safe_limit
        self.stats.target_tokens = target_tokens
        
        current_tokens = self.stats.original_tokens
        self.debug_log(1, f"ğŸ¯ Coverage-First v2.4.4å¤„ç†å¼€å§‹: {current_tokens:,} tokens, {len(messages)} æ¡æ¶ˆæ¯", "ğŸ¯")
        
        await progress.start_phase("Coverage-First v2.4.4å¤„ç†", 10)
        
        # 1. æ¶ˆæ¯åˆ†ç¦»
        await progress.update_progress(1, 10, "åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯")
        current_user_message, history_messages = self.separate_current_and_history_messages(messages)
        self.current_user_message = current_user_message
        
        # ç³»ç»Ÿæ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        
        if current_user_message:
            self.stats.current_user_tokens = self.count_message_tokens(current_user_message)
        
        # 2. æ£€æµ‹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚
        need_context_max = False
        if current_user_message and self.valves.enable_context_maximization:
            query_text = self.extract_text_from_content(current_user_message.get("content", ""))
            need_context_max = await self.detect_context_max_need(query_text, progress.event_emitter)
            if need_context_max:
                self.debug_log(1, f"ğŸ“š æ£€æµ‹åˆ°éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ï¼Œå¯ç”¨Coverage-First v2.4.4ç­–ç•¥", "ğŸ“š")
        
        # 3. è®¡ç®—ä¿æŠ¤æ¶ˆæ¯çš„token
        protected_messages = system_messages[:]
        protected_tokens = self.count_messages_tokens(protected_messages)
        available_for_processing = target_tokens - protected_tokens - self.stats.current_user_tokens
        
        self.debug_log(1, f"ğŸ’° å†å²æ¶ˆæ¯å¯ç”¨å¤„ç†ç©ºé—´: {available_for_processing:,}tokens", "ğŸ’°")
        
        # 4. å¤„ç†å†å²æ¶ˆæ¯
        if not history_messages:
            final_messages = system_messages[:]
            if current_user_message:
                final_messages.append(current_user_message)
            await progress.complete_phase("æ— å†å²æ¶ˆæ¯éœ€è¦å¤„ç†")
            return final_messages
        
        # 5. ä½¿ç”¨Coverage-First v2.4.4ç­–ç•¥
        if need_context_max and self.valves.enable_context_maximization and self.valves.enable_coverage_first:
            await progress.update_progress(2, 10, "Coverage-First v2.4.4ä¸“ç”¨å¤„ç†")
            processed_history = await self.process_coverage_first_context_maximization_v2(
                history_messages, available_for_processing, progress, current_user_message
            )
        else:
            await progress.update_progress(2, 10, "æ ‡å‡†æˆªæ–­å¤„ç†")
            if available_for_processing > 0:
                processed_history = self.smart_truncate_messages(
                    history_messages, available_for_processing, True
                )
            else:
                processed_history = []
        
        # 6. é›¶ä¸¢å¤±ä¿éšœæ£€æŸ¥  
        await progress.update_progress(6, 10, "é›¶ä¸¢å¤±ä¿éšœæ£€æŸ¥")
        final_history = processed_history
        final_tokens = self.count_messages_tokens(final_history)
        
        if final_tokens > available_for_processing and self.valves.disable_insurance_truncation:
            self.debug_log(1, f"ğŸ›¡ï¸ é¢„ç®—è¶…é™ä½†ç¦ç”¨æˆªæ–­ï¼Œä¿è¯é›¶ä¸¢å¤±", "ğŸ›¡ï¸")
            self.stats.insurance_truncation_avoided += 1
        elif final_tokens > available_for_processing:
            self.debug_log(1, f"âœ‚ï¸ è¶…å‡ºé¢„ç®—ï¼Œå¯ç”¨ä¿é™©æˆªæ–­", "âœ‚ï¸")
            final_history = self.smart_truncate_messages(final_history, available_for_processing, True)
            final_tokens = self.count_messages_tokens(final_history)
            self.stats.zero_loss_guarantee = False
        
        # 7. ç»„åˆæœ€ç»ˆç»“æœ
        await progress.update_progress(8, 10, "ç»„åˆæœ€ç»ˆç»“æœ")
        current_result = system_messages + final_history
        
        if self.message_order:
            current_result = self.message_order.sort_messages_preserve_user(
                current_result, self.current_user_message
            )
        
        final_messages = []
        for msg in current_result:
            final_messages.append(msg)
        
        if current_user_message:
            final_messages.append(current_user_message)
        
        # 8. ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤
        await progress.update_progress(9, 10, "ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤éªŒè¯")
        final_messages = self.ensure_current_user_message_preserved(final_messages)
        
        # 9. æ›´æ–°ç»Ÿè®¡
        await progress.update_progress(10, 10, "æ›´æ–°ç»Ÿè®¡")
        self.stats.final_tokens = self.count_messages_tokens(final_messages)
        self.stats.final_messages = len(final_messages)
        self.stats.processing_time = time.time() - start_time
        self.stats.iterations = 1
        
        if self.stats.original_tokens > 0:
            self.stats.content_loss_ratio = max(
                0, (self.stats.original_tokens - self.stats.final_tokens) / self.stats.original_tokens
            )
        
        if target_tokens > 0:
            self.stats.window_utilization = self.stats.final_tokens / target_tokens
        
        if current_user_message:
            self.stats.current_user_preserved = any(
                msg.get("_order_id") == current_user_message.get("_order_id")
                for msg in final_messages
            )
        
        retention_ratio = self.stats.calculate_retention_ratio()
        window_usage = self.stats.calculate_window_usage_ratio()
        
        self.debug_log(1, f"ğŸ¯ Coverage-First v2.4.4å¤„ç†å®Œæˆ: "
                          f"ä¿ç•™{retention_ratio:.1%} çª—å£ä½¿ç”¨{window_usage:.1%} "
                          f"é›¶ä¸¢å¤±{'ä¿éšœæˆåŠŸ' if self.stats.zero_loss_guarantee else 'éƒ¨åˆ†å¤±æ•ˆ'}", "ğŸ¯")
        
        await progress.complete_phase(
            f"Coverage-First v2.4.4å®Œæˆ è¦†ç›–ç‡{self.stats.coverage_rate:.1%} é¢„ç®—ä½¿ç”¨{window_usage:.1%} "
            f"é›¶ä¸¢å¤±ä¿éšœ{'æˆåŠŸ' if self.stats.zero_loss_guarantee else 'éƒ¨åˆ†å¤±æ•ˆ'} "
            f"{'[ä¸Šä¸‹æ–‡æœ€å¤§åŒ–]' if need_context_max else '[å…·ä½“æŸ¥è¯¢]'}"
        )
        
        return final_messages

    def print_detailed_stats(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.valves.enable_detailed_stats:
            return
        
        print("\n" + "=" * 80)
        print(self.stats.get_summary())
        print("=" * 80)

    # ========== ä¸»è¦å…¥å£å‡½æ•° ==========
    async def inlet(
        self, body: dict, user: Optional[dict] = None, __event_emitter__: Optional[Callable] = None
    ) -> dict:
        """å…¥å£å‡½æ•° v2.4.4 - å®Œæ•´ä¿®å¤ç‰ˆæœ¬"""
        print("\nğŸš€ ===== INLET CALLED (Coverage-First v2.4.4 - å®Œæ•´ä¿®å¤ç‰ˆæœ¬) =====")
        print(f"ğŸ“¨ æ”¶åˆ°è¯·æ±‚: {list(body.keys())}")
        
        if not self.valves.enable_processing:
            print("âŒ å¤„ç†åŠŸèƒ½å·²ç¦ç”¨")
            return body
        
        messages = body.get("messages", [])
        if not messages:
            print("âŒ æ— æ¶ˆæ¯å†…å®¹")
            return body
        
        model_name = body.get("model", "æœªçŸ¥")
        print(f"ğŸ“‹ æ¨¡å‹: {model_name}, æ¶ˆæ¯æ•°: {len(messages)}")
        
        if self.is_model_excluded(model_name):
            print(f"ğŸš« æ¨¡å‹å·²æ’é™¤")
            return body
        
        # é‡ç½®å¤„ç†çŠ¶æ€
        self.reset_processing_state()
        
        # ä¿å­˜å½“å‰æ¨¡å‹å
        self._current_model_name = model_name
        
        # åˆ†ææ¨¡å‹ä¿¡æ¯
        self.current_model_info = self.analyze_model(model_name)
        
        # åˆ›å»ºè¿›åº¦è¿½è¸ªå™¨
        progress = ProgressTracker(__event_emitter__)
        
        # åˆå§‹åŒ–æ¶ˆæ¯é¡ºåºç®¡ç†å™¨ï¼ˆä¸å†deepcopyï¼Œç›´æ¥åœ¨åŸæ¶ˆæ¯ä¸Šæ‰“æ ‡ç­¾ï¼‰
        self.message_order = MessageOrder(messages)
        
        # ã€å…³é”®ä¿®å¤ã€‘ï¼šä½¿ç”¨å¸¦_order_idçš„æ¶ˆæ¯åˆ—è¡¨
        messages = self.message_order.original_messages
        
        # æ¶ˆæ¯åˆ†ç¦»ï¼ˆä½¿ç”¨å¸¦IDçš„æ¶ˆæ¯ï¼‰
        current_user_message, history_messages = self.separate_current_and_history_messages(messages)
        self.current_user_message = current_user_message
        
        # Tokenåˆ†æ
        original_tokens = self.count_messages_tokens(messages)
        model_token_limit = self.get_model_token_limit(model_name)
        current_user_tokens = self.count_message_tokens(current_user_message) if current_user_message else 0
        target_tokens = self.calculate_target_tokens(model_name, current_user_tokens)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.token_limit = model_token_limit
        self.stats.target_tokens = target_tokens
        self.stats.current_user_tokens = current_user_tokens
        
        print(f"ğŸ¯ Coverage-First v2.4.4ç»Ÿè®¡: {original_tokens:,}/{model_token_limit:,} (ç›®æ ‡:{target_tokens:,})")
        print(f"ğŸ¯ æ¨¡å‹ä¿¡æ¯: {self.current_model_info['family']}å®¶æ— | "
              f"{'å¤šæ¨¡æ€' if self.current_model_info['multimodal'] else 'æ–‡æœ¬'} | "
              f"{self.current_model_info['match_type']}åŒ¹é…")
        
        print(f"âœ… v2.4.4 å®Œæ•´ä¿®å¤ç‰ˆæœ¬:")
        print(f"ğŸ”§ è¯­æ³•ä¿®å¤: æ‰€æœ‰__init__é”™è¯¯ã€å¼•å·æ–­è£‚ã€è¿ç®—ç¬¦ä¸¢å¤±å·²ä¿®å¤")
        print(f"ğŸ”§ æ–¹æ³•åç»Ÿä¸€: æ¶ˆé™¤æ‰€æœ‰ä¸‹åˆ’çº¿ä¸ä¸€è‡´é—®é¢˜")
        print(f"ğŸ”§ å±æ€§åŒ¹é…: æ‰€æœ‰è°ƒç”¨åä¸å®šä¹‰åä¿æŒä¸€è‡´")
        print(f"ğŸ†• GPT-5ç³»åˆ—: å®Œæ•´æ”¯æŒgpt-5/mini/nano (200k + å¤šæ¨¡æ€)")
        print(f"ğŸ›¡ï¸ åŒé‡æŠ¤æ : ç»„è£…å‰æ ¡éªŒ + æœªè½åœ°å¾®æ‘˜è¦å›é€€")
        print(f"ğŸ§© è‡ªé€‚åº”åˆ†å—: æŒ‰åŸæ–‡é‡({self.valves.raw_block_target:,}t)åˆ‡å—")
        print(f"âš–ï¸ ä¸€æ¬¡æ€§ç¼©æ”¾: Î±ç²¾ç¡®è®¡ç®—ï¼Œè¯¯å·®æŠ¹å¹³")
        print(f"â¬†ï¸ å‡çº§æ± ä¿æŠ¤: é¢„ç•™{self.valves.upgrade_min_pct:.1%}é˜²è¢«åƒå…‰")
        print(f"ğŸ¯ Coverage-First: 100%è¦†ç›– + é›¶ä¸¢å¤±ä¿éšœ")
        print(f"ğŸªŸ æœ€å¤§çª—å£åˆ©ç”¨: {self.valves.max_window_utilization:.1%}")
        print(f"ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–: {self.valves.enable_context_maximization}")
        print(f"ğŸ”‘ æ™ºèƒ½å…³é”®å­—: {self.valves.enable_keyword_generation}")
        print(f"ğŸ§  AIæ£€æµ‹: {self.valves.enable_ai_context_max_detection}")
        
        # ç”Ÿæˆå¤„ç†ID
        if current_user_message:
            content_preview = self.message_order.get_message_preview(current_user_message)
            processing_id = hashlib.md5(f"{current_user_message.get('_order_id', '')}{content_preview}{time.time()}".encode()).hexdigest()[:8]
            self.current_processing_id = processing_id
            print(f"ğŸ’¬ å½“å‰ç”¨æˆ·æ¶ˆæ¯ [ID:{processing_id}]: {current_user_tokens}tokens")
            print(f"ğŸ“œ å†å²æ¶ˆæ¯: {len(history_messages)}æ¡ ({self.count_messages_tokens(history_messages):,}tokens)")
            
            # AIæ£€æµ‹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–
            query_text = self.extract_text_from_content(current_user_message.get("content", ""))
            if self.valves.enable_ai_context_max_detection:
                try:
                    need_context_max = await self.detect_context_max_need(query_text, __event_emitter__)
                    print(f"ğŸ§  AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹: {'éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–' if need_context_max else 'ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–'}")
                    if need_context_max:
                        print(f"ğŸ¯ Coverage-First v2.4.4ç­–ç•¥è¯¦æƒ…:")
                        print(f"ğŸ¯ Step 0: å¤§æ¶ˆæ¯åˆ†ç‰‡é¢„å¤„ç† - ä¿æŒå†…å®¹å®Œæ•´æ€§")
                        print(f"ğŸ¯ Step 1: å¹¶å‘ç›¸å…³åº¦åˆ†æ•°è®¡ç®— - 100%è¦†ç›–ä¸è¿‡æ»¤")
                        print(f"ğŸ¯ Step 2: è‡ªé€‚åº”åˆ†å— - æŒ‰åŸæ–‡é‡{self.valves.raw_block_target:,}tåˆ‡å—")
                        print(f"ğŸ¯ Step 3: ä¸€æ¬¡æ€§ç¼©æ”¾/å‘ä¸Šæ‰©å¼  - æ•°å­¦ç²¾ç¡®Î±è®¡ç®—")
                        print(f"ğŸ¯ Step 4: å‡çº§æ± ä¿æŠ¤ - é¢„ç•™{self.valves.upgrade_min_pct:.1%}é˜²åƒå…‰")
                        print(f"ğŸ¯ Step 5: ç”Ÿæˆæ‘˜è¦ - ä½¿ç”¨ç¼©æ”¾åé¢„ç®—")
                        print(f"ğŸ¯ Step 6: åŒé‡æŠ¤æ ç»„è£… - éªŒè¯+å›é€€æœºåˆ¶")
                        print(f"ğŸ¯ Step 7: Top-upçª—å£å¡«å……(ä¿®å¤ç‰ˆ) - å†²åˆº{self.valves.target_window_usage:.1%}åˆ©ç”¨ç‡")
                        print(f"ğŸ¯ å®Œæ•´ä¿®å¤ä¿éšœ: è¯­æ³•é”™è¯¯+æ–¹æ³•å+å±æ€§åŒ¹é… = é›¶é”™è¯¯è¿è¡Œ")
                except Exception as e:
                    print(f"ğŸ§  AIæ£€æµ‹å¤±è´¥: {e}")
                    need_context_max = self.is_context_max_need_simple(query_text)
                    print(f"ğŸ§  ç®€å•æ–¹æ³•æ£€æµ‹: {'éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–' if need_context_max else 'ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–'}")
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æœ€å¤§åŒ–
        should_maximize = self.should_force_maximize_content(messages, target_tokens)
        print(f"ğŸ”¥ æ˜¯å¦éœ€è¦æœ€å¤§åŒ–: {should_maximize}")
        
        try:
            # 1. å¤šæ¨¡æ€å¤„ç†
            if self.valves.enable_detailed_progress:
                await progress.start_phase("å¤šæ¨¡æ€å¤„ç†", 1)
            
            processed_messages = await self.process_multimodal_content(
                messages, model_name, target_tokens, progress
            )
            processed_tokens = self.count_messages_tokens(processed_messages)
            print(f"ğŸ“Š å¤šæ¨¡æ€å¤„ç†å: {processed_tokens:,} tokens")
            
            # 2. Coverage-First v2.4.4å†…å®¹æœ€å¤§åŒ–å¤„ç†
            if should_maximize:
                print(f"ğŸ¯ å¯åŠ¨Coverage-First v2.4.4å†…å®¹æœ€å¤§åŒ–å¤„ç†...")
                final_messages = await self.maximize_content_comprehensive_processing_v2(
                    processed_messages, target_tokens, progress
                )
                
                # æ‰“å°è¯¦ç»†ç»Ÿè®¡
                self.print_detailed_stats()
                
                body["messages"] = copy.deepcopy(final_messages)
                
                # æœ€ç»ˆç»Ÿè®¡
                final_tokens = self.count_messages_tokens(final_messages)
                window_utilization = final_tokens / target_tokens if target_tokens > 0 else 0
                
                print(f"ğŸ¯ Coverage-First v2.4.4å¤„ç†å®Œæˆ [ID:{self.current_processing_id}]")
                print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {len(final_messages)}æ¡æ¶ˆæ¯, {final_tokens:,}tokens")
                print(f"ğŸªŸ çª—å£åˆ©ç”¨ç‡: {window_utilization:.1%}")
                print(f"ğŸ“ˆ å†…å®¹ä¿ç•™ç‡: {self.stats.calculate_retention_ratio():.1%}")
                print(f"ğŸ›¡ï¸ é›¶ä¸¢å¤±ä¿éšœ: {'æˆåŠŸ' if self.stats.zero_loss_guarantee else 'éƒ¨åˆ†å¤±æ•ˆ'}")
                
                print(f"âœ… v2.4.4å®Œæ•´ä¿®å¤æˆæœ:")
                print(f"    â”œâ”€ è¯­æ³•é”™è¯¯ä¿®å¤: {self.stats.syntax_errors_fixed}æ¬¡")
                print(f"    â”œâ”€ GPT-5æ¨¡å‹è¯†åˆ«: {'æ”¯æŒ' if 'gpt-5' in model_name.lower() else 'ç­‰å¾…æ£€æµ‹'}")
                print(f"    â”œâ”€ IDä¼ é€’ä¿®å¤: MessageOrderç›´æ¥æ‰“æ ‡ç­¾ï¼Œæ¶ˆæ¯æµæ°´çº¿IDä¸€è‡´")
                print(f"    â”œâ”€ çª—å£å¡«å……: Top-upåº”ç”¨{self.stats.topup_applied}æ¬¡")
                print(f"    â”œâ”€ å¾®æ‘˜è¦å‡çº§: {self.stats.topup_micro_upgraded}æ¡ -> åŸæ–‡")
                print(f"    â”œâ”€ åŸæ–‡æ·»åŠ : {self.stats.topup_raw_added}æ¡æœªè½åœ°æ¶ˆæ¯")
                print(f"    â”œâ”€ æ–°å¢tokens: {self.stats.topup_tokens_added:,}tokens (åŸºçº¿å·®å€¼ä¿®æ­£)")
                print(f"    â”œâ”€ è‡ªé€‚åº”åˆ†å—: {self.stats.adaptive_blocks_created}ä¸ªå—, {self.stats.block_merge_operations}æ¬¡åˆå¹¶")
                print(f"    â”œâ”€ æ¶ˆæ¯åˆ†ç‰‡: {self.stats.chunked_messages_count}æ¡æ¶ˆæ¯åˆ†ä¸º{self.stats.total_chunks_created}ç‰‡")
                print(f"    â”œâ”€ é¢„ç®—ç¼©æ”¾: åº”ç”¨{self.stats.budget_scaling_applied}æ¬¡, å› å­{self.stats.scaling_factor:.3f}")
                print(f"    â”œâ”€ åŒé‡æŠ¤æ : Aè­¦å‘Š{self.stats.guard_a_warnings}æ¬¡, Bå›é€€{self.stats.guard_b_fallbacks}æ¬¡")
                print(f"    â”œâ”€ Coverageç»Ÿè®¡: åŸæ–‡{self.stats.coverage_preserved_count}æ¡ + æ‘˜è¦{self.stats.coverage_summary_count}æ¡")
                print(f"    â”œâ”€ å¾®æ‘˜è¦: {self.stats.coverage_micro_summaries}æ¡, å—æ‘˜è¦: {self.stats.coverage_block_summaries}å—")
                print(f"    â”œâ”€ å‡çº§æˆåŠŸ: {self.stats.coverage_upgrade_count}æ¡ (èŠ‚çº¦{self.stats.coverage_upgrade_tokens_saved:,}tokens)")
                print(f"    â””â”€ é¢„ç®—ä½¿ç”¨: {self.stats.coverage_budget_usage:.1%}")
                
                print(f"ğŸ›¡ï¸ é›¶ä¸¢å¤±ä¿éšœç»Ÿè®¡:")
                print(f"    â”œâ”€ ä¿éšœçŠ¶æ€: {'æˆåŠŸ' if self.stats.zero_loss_guarantee else 'éƒ¨åˆ†å¤±æ•ˆ'}")
                print(f"    â”œâ”€ é¢„ç®—è°ƒæ•´: {self.stats.budget_adjustments}è½®")
                print(f"    â”œâ”€ æœ€å°é¢„ç®—åº”ç”¨: {self.stats.min_budget_applied}æ¬¡")
                print(f"    â””â”€ é¿å…ä¿é™©æˆªæ–­: {self.stats.insurance_truncation_avoided}æ¬¡")
                
                print(f"ğŸ”‘ å…³é”®å­—ç”Ÿæˆ: {self.stats.keyword_generations}æ¬¡")
                print(f"ğŸ§  ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹: {self.stats.context_maximization_detections}æ¬¡")
                print(f"ğŸ–¼ï¸ å¤šæ¨¡æ€å¤„ç†: {self.stats.multimodal_processed}å¼ å›¾ç‰‡, é”™è¯¯{self.stats.image_processing_errors}æ¬¡")
                print(f"âœ‚ï¸ æ™ºèƒ½æˆªæ–­: åº”ç”¨{self.stats.smart_truncation_applied}æ¬¡, "
                      f"è·³è¿‡{self.stats.truncation_skip_count}æ¡, æ¢å¤{self.stats.truncation_recovered_messages}æ¡")
                print(f"ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤: åå¤‡ä¿ç•™{self.stats.fallback_preserve_applied}æ¬¡, "
                      f"ç”¨æˆ·æ¶ˆæ¯æ¢å¤{self.stats.user_message_recovery_count}æ¬¡")
                
                # éªŒè¯å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤
                if current_user_message and final_messages:
                    last_msg = final_messages[-1]
                    if last_msg.get("role") == "user":
                        print(f"âœ… å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤æˆåŠŸï¼")
                    else:
                        print(f"âŒ æœ€åä¸€æ¡æ¶ˆæ¯ä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼")
            else:
                # ç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯
                self.stats.original_tokens = self.count_messages_tokens(messages)
                self.stats.original_messages = len(messages)
                self.stats.final_tokens = processed_tokens
                self.stats.final_messages = len(processed_messages)
                
                if self.valves.enable_detailed_progress:
                    await progress.complete_phase("æ— éœ€æœ€å¤§åŒ–å¤„ç†")
                
                body["messages"] = copy.deepcopy(processed_messages)
                print(f"âœ… ç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯ [ID:{self.current_processing_id}]")
                
        except Exception as e:
            print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
            if "SyntaxError" in str(e) or "did not match the expected pattern" in str(e):
                print(f"âŒ è¯­æ³•é”™è¯¯è¯¦æƒ…: {str(e)[:200]}")
                print(f"âŒ è¿™ç±»é”™è¯¯å·²åœ¨v2.4.4ä¸­ä¿®å¤ï¼")
                self.stats.syntax_errors_fixed += 1
            import traceback
            traceback.print_exc()
            
            if self.valves.enable_detailed_progress:
                await progress.update_status(f"å¤„ç†å¤±è´¥: {str(e)[:50]}", True)
        
        print(f"ğŸ ===== INLET DONE (Coverage-First v2.4.4 - å®Œæ•´ä¿®å¤ç‰ˆæœ¬) [ID:{self.current_processing_id}] =====\n")
        return body

    async def outlet(self, body: dict, user: Optional[dict] = None, __event_emitter__: Optional[Callable] = None) -> dict:
        """å‡ºå£å‡½æ•°"""
        return body

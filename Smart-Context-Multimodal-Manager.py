"""
title: ğŸ§  Smart Context & Multimodal Manager
author: Advanced AI Assistant
version: 1.0.0
license: MIT
required_open_webui_version: 0.6.0
description: æ™ºèƒ½é•¿ä¸Šä¸‹æ–‡å’Œå¤šæ¨¡æ€å†…å®¹å¤„ç†å™¨ï¼Œæ”¯æŒå‘é‡åŒ–æ£€ç´¢ã€è¯­ä¹‰é‡æ’åºå’Œå¤šæ¨¡æ€ç†è§£
"""

import json
import hashlib
import asyncio
import re
import base64
from typing import Optional, List, Dict, Callable, Any, Tuple, Union
from pydantic import BaseModel, Field
from datetime import datetime
import urllib.parse

# å¯¼å…¥æ‰€éœ€åº“
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    tiktoken = None

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None


class Filter:
    class Valves(BaseModel):
        # ğŸ”§ åŸºç¡€é…ç½®
        enable_context_management: bool = Field(
            default=True, 
            description="ğŸ§  å¯ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†"
        )
        
        enable_multimodal_processing: bool = Field(
            default=True, 
            description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å†…å®¹å¤„ç†"
        )
        
        # ğŸ“Š Tokenç®¡ç†é…ç½®
        default_token_limit: int = Field(
            default=32000, 
            description="ğŸ¯ é»˜è®¤tokené™åˆ¶ï¼ˆå»ºè®®è®¾ç½®ä¸ºæ¨¡å‹ä¸Šä¸‹æ–‡çš„70-80%ï¼‰"
        )
        
        model_token_limits: str = Field(
            default='{"gpt-4": 32000, "gpt-3.5-turbo": 16000, "claude-3": 200000, "doubao": 32000}',
            description="ğŸ“ æ¨¡å‹ç‰¹å®štokené™åˆ¶é…ç½®ï¼ˆJSONæ ¼å¼ï¼‰"
        )
        
        preserve_last_messages_count: int = Field(
            default=2,
            description="ğŸ”’ å¼ºåˆ¶ä¿ç•™æœ€åNæ¡æ¶ˆæ¯çš„å®Œæ•´æ€§"
        )
        
        context_preserve_ratio: float = Field(
            default=0.6,
            description="ğŸ“Š ä¸Šä¸‹æ–‡ä¿ç•™æ¯”ä¾‹ï¼ˆ0.6è¡¨ç¤ºä¿ç•™60%åŸæ–‡ï¼Œ40%å‘é‡åŒ–æ£€ç´¢ï¼‰"
        )
        
        # ğŸ” å‘é‡åŒ–é…ç½®
        enable_vectorization: bool = Field(
            default=True,
            description="ğŸ” å¯ç”¨å‘é‡åŒ–æ£€ç´¢åŠŸèƒ½"
        )
        
        vector_api_base_url: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸŒ å‘é‡åŒ–APIåŸºç¡€URL"
        )
        
        vector_api_key: str = Field(
            default="",
            description="ğŸ”‘ å‘é‡åŒ–APIå¯†é’¥"
        )
        
        vector_model: str = Field(
            default="doubao-embedding-large-text-250515",
            description="ğŸ¤– å‘é‡åŒ–æ¨¡å‹åç§°"
        )
        
        multimodal_vector_model: str = Field(
            default="doubao-embedding-vision-250615",
            description="ğŸ¨ å¤šæ¨¡æ€å‘é‡åŒ–æ¨¡å‹åç§°"
        )
        
        # ğŸ¯ æ£€ç´¢é…ç½®
        retrieval_top_k: int = Field(
            default=10,
            description="ğŸ¯ æ£€ç´¢Top-Kå€™é€‰æ•°é‡"
        )
        
        chunk_size: int = Field(
            default=1000,
            description="ğŸ“¦ æ–‡æœ¬åˆ†ç‰‡å¤§å°ï¼ˆtokenæ•°ï¼‰"
        )
        
        chunk_overlap: int = Field(
            default=200,
            description="ğŸ”„ åˆ†ç‰‡é‡å å¤§å°ï¼ˆtokenæ•°ï¼‰"
        )
        
        # ğŸ”„ é‡æ’åºé…ç½®
        enable_rerank: bool = Field(
            default=True,
            description="ğŸ”„ å¯ç”¨è¯­ä¹‰é‡æ’åº"
        )
        
        rerank_api_url: str = Field(
            default="https://api.bochaai.com/v1/rerank",
            description="ğŸ”„ é‡æ’åºAPIåœ°å€"
        )
        
        rerank_api_key: str = Field(
            default="",
            description="ğŸ”‘ é‡æ’åºAPIå¯†é’¥"
        )
        
        rerank_model: str = Field(
            default="gte-rerank",
            description="ğŸ¤– é‡æ’åºæ¨¡å‹åç§°"
        )
        
        rerank_top_n: int = Field(
            default=5,
            description="ğŸ¯ é‡æ’åºè¿”å›æ•°é‡"
        )
        
        # ğŸ–¼ï¸ å¤šæ¨¡æ€é…ç½®
        vision_model: str = Field(
            default="doubao-1.5-vision-pro-250328",
            description="ğŸ‘ï¸ è§†è§‰ç†è§£æ¨¡å‹"
        )
        
        vision_api_base_url: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸŒ è§†è§‰æ¨¡å‹APIåŸºç¡€URL"
        )
        
        vision_api_key: str = Field(
            default="",
            description="ğŸ”‘ è§†è§‰æ¨¡å‹APIå¯†é’¥"
        )
        
        models_with_native_vision: str = Field(
            default='["gpt-4-vision", "gpt-4o", "claude-3", "doubao-1.5-vision"]',
            description="ğŸ‘ï¸ åŸç”Ÿæ”¯æŒè§†è§‰çš„æ¨¡å‹åˆ—è¡¨ï¼ˆJSONæ ¼å¼ï¼‰"
        )
        
        # ğŸ› è°ƒè¯•é…ç½®
        debug_level: int = Field(
            default=1,
            description="ğŸ› è°ƒè¯•çº§åˆ«ï¼š0=å…³é—­ï¼Œ1=åŸºç¡€ï¼Œ2=è¯¦ç»†ï¼Œ3=å®Œæ•´"
        )
        
        show_processing_status: bool = Field(
            default=True,
            description="ğŸ“Š æ˜¾ç¤ºå¤„ç†çŠ¶æ€ä¿¡æ¯"
        )
        
        # âš™ï¸ é«˜çº§é…ç½®
        max_concurrent_requests: int = Field(
            default=3,
            description="ğŸš€ æœ€å¤§å¹¶å‘è¯·æ±‚æ•°"
        )
        
        request_timeout: int = Field(
            default=60,
            description="â±ï¸ APIè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"
        )
        
        intelligent_chunking: bool = Field(
            default=True,
            description="ğŸ§  å¯ç”¨æ™ºèƒ½åˆ†ç‰‡ï¼ˆæŒ‰å¥å­ã€æ®µè½åˆ†ç‰‡ï¼‰"
        )

    def __init__(self):
        self.valves = self.Valves()
        self.toggle = True  # UIå¼€å…³
        self.icon = """xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGZpbGw9Im5vbmUiIHZpZXdCb3g9IjAgMCAyNCAyNCIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZT0iY3VycmVudENvbG9yIj4KICA8cGF0aCBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiIGQ9Ik0xMiAxOHYtNS4yNW0wIDBhNi4wMSA2LjAxIDAgMCAwIDEuNS0uMTg5bS0xLjUuMTg5YTYuMDEgNi4wMSAwIDAgMS0xLjUtLjE4OW0zLjc1IDcuNDc4YTEyLjA2IDEyLjA2IDAgMCAxLTQuNSAwbTMuNzUgMi4zODNhMTQuNDA2IDE0LjQwNiAwIDAgMS0zIDBNMTQuMjUgMTh2LS4xOTJjMC0uOTgzLjY1OC0xLjgyMyAxLjUwOC0yLjMxNmE3LjUgNy41IDAgMSAwLTcuNTE3IDBjLjg1LjQ5MyAxLjUwOSAxLjMzMyAxLjUwOSAyLjMxNlYxOCIgLz4KPC9zdmc+"""
        
        # ç¼“å­˜å’ŒçŠ¶æ€
        self._encoding = None
        self._vector_client = None
        self._vision_client = None
        self._vector_store = {}  # ç®€å•å†…å­˜å‘é‡å­˜å‚¨
        self._processing_stats = {}

    def debug_log(self, level: int, message: str, emoji: str = "ğŸ”"):
        """åˆ†çº§debugæ—¥å¿—"""
        if self.valves.debug_level >= level:
            prefix = ["", f"{emoji}[DEBUG]", f"{emoji}[DETAIL]", f"{emoji}[VERBOSE]"][min(level, 3)]
            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"{prefix} [{timestamp}] {message}")

    def get_encoding(self):
        """è·å–tiktokenç¼–ç å™¨"""
        if not TIKTOKEN_AVAILABLE:
            return None
        if self._encoding is None:
            self._encoding = tiktoken.get_encoding("cl100k_base")
        return self._encoding

    def count_tokens(self, text: str) -> int:
        """ç²¾ç¡®è®¡ç®—tokenæ•°é‡"""
        if not text:
            return 0
        encoding = self.get_encoding()
        if encoding is None:
            return len(text) // 4  # ç²—ç•¥ä¼°ç®—
        try:
            return len(encoding.encode(text))
        except:
            return len(text) // 4

    def count_message_tokens(self, message: dict) -> int:
        """è®¡ç®—å•ä¸ªæ¶ˆæ¯çš„tokenæ•°"""
        content = message.get("content", "")
        role = message.get("role", "")
        total_tokens = 0

        if isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    text = item.get("text", "")
                    total_tokens += self.count_tokens(text)
                elif item.get("type") == "image_url":
                    total_tokens += 1000  # å›¾åƒå¤§æ¦‚ä¼°ç®—
        elif isinstance(content, str):
            total_tokens = self.count_tokens(content)

        total_tokens += self.count_tokens(role) + 4
        return total_tokens

    def count_messages_tokens(self, messages: List[dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€»tokenæ•°"""
        return sum(self.count_message_tokens(msg) for msg in messages)

    def get_model_token_limit(self, model_name: str) -> int:
        """è·å–æ¨¡å‹ç‰¹å®šçš„tokené™åˆ¶"""
        try:
            model_limits = json.loads(self.valves.model_token_limits)
            for model_key in model_limits:
                if model_key.lower() in model_name.lower():
                    return model_limits[model_key]
        except:
            pass
        return self.valves.default_token_limit

    def has_native_vision(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŸç”Ÿæ”¯æŒè§†è§‰"""
        try:
            native_models = json.loads(self.valves.models_with_native_vision)
            return any(model.lower() in model_name.lower() for model in native_models)
        except:
            return False

    def extract_images_from_messages(self, messages: List[dict]) -> List[Tuple[int, dict]]:
        """æå–æ¶ˆæ¯ä¸­çš„å›¾åƒ"""
        images = []
        for msg_idx, message in enumerate(messages):
            content = message.get("content", [])
            if isinstance(content, list):
                for item in content:
                    if item.get("type") == "image_url":
                        images.append((msg_idx, item))
        return images

    def smart_chunk_text(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†ç‰‡"""
        if not self.valves.intelligent_chunking:
            # ç®€å•åˆ†ç‰‡
            chunks = []
            for i in range(0, len(text), chunk_size - overlap):
                chunks.append(text[i:i + chunk_size])
            return chunks
        
        # æ™ºèƒ½åˆ†ç‰‡ - æŒ‰å¥å­å’Œæ®µè½
        chunks = []
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', text)
        
        current_chunk = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = self.count_tokens(sentence)
            
            if current_tokens + sentence_tokens > chunk_size and current_chunk:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                chunks.append(current_chunk.strip())
                # ä¿ç•™é‡å 
                if overlap > 0:
                    words = current_chunk.split()
                    overlap_text = " ".join(words[-overlap//4:])  # ç²—ç•¥é‡å 
                    current_chunk = overlap_text + " " + sentence
                    current_tokens = self.count_tokens(current_chunk)
                else:
                    current_chunk = sentence
                    current_tokens = sentence_tokens
            else:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
                current_tokens += sentence_tokens
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    async def get_vector_client(self):
        """è·å–å‘é‡åŒ–å®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE:
            raise Exception("OpenAIåº“æœªå®‰è£…")
        if not self.valves.vector_api_key:
            raise Exception("å‘é‡åŒ–APIå¯†é’¥æœªé…ç½®")
        
        if self._vector_client is None:
            self._vector_client = AsyncOpenAI(
                base_url=self.valves.vector_api_base_url,
                api_key=self.valves.vector_api_key,
                timeout=self.valves.request_timeout,
            )
        return self._vector_client

    async def get_vision_client(self):
        """è·å–è§†è§‰ç†è§£å®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE:
            raise Exception("OpenAIåº“æœªå®‰è£…")
        if not self.valves.vision_api_key:
            raise Exception("è§†è§‰APIå¯†é’¥æœªé…ç½®")
        
        if self._vision_client is None:
            self._vision_client = AsyncOpenAI(
                base_url=self.valves.vision_api_base_url,
                api_key=self.valves.vision_api_key,
                timeout=self.valves.request_timeout,
            )
        return self._vision_client

    async def vectorize_text(self, text: str, is_multimodal: bool = False) -> List[float]:
        """æ–‡æœ¬å‘é‡åŒ–"""
        try:
            client = await self.get_vector_client()
            model = self.valves.multimodal_vector_model if is_multimodal else self.valves.vector_model
            
            response = await client.embeddings.create(
                model=model,
                input=text,
                encoding_format="float"
            )
            
            return response.data[0].embedding
        except Exception as e:
            self.debug_log(1, f"âŒ å‘é‡åŒ–å¤±è´¥: {str(e)}")
            return []

    async def vectorize_multimodal_content(self, content_list: List[dict]) -> List[float]:
        """å¤šæ¨¡æ€å†…å®¹å‘é‡åŒ–"""
        try:
            client = await self.get_vector_client()
            
            # æ„å»ºå¤šæ¨¡æ€è¾“å…¥
            multimodal_input = []
            for item in content_list:
                if item.get("type") == "text":
                    multimodal_input.append({
                        "type": "text",
                        "text": item.get("text", "")
                    })
                elif item.get("type") == "image_url":
                    multimodal_input.append({
                        "type": "image_url",
                        "image_url": item["image_url"]
                    })
            
            response = await client.embeddings.create(
                model=self.valves.multimodal_vector_model,
                input=multimodal_input,
                encoding_format="float"
            )
            
            return response.data[0].embedding
        except Exception as e:
            self.debug_log(1, f"âŒ å¤šæ¨¡æ€å‘é‡åŒ–å¤±è´¥: {str(e)}")
            return []

    async def describe_image(self, image_item: dict) -> str:
        """å›¾åƒæè¿°ç”Ÿæˆ"""
        try:
            client = await self.get_vision_client()
            
            response = await client.chat.completions.create(
                model=self.valves.vision_model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€æ–‡å­—ã€é¢œè‰²ã€é£æ ¼ç­‰é‡è¦ä¿¡æ¯ã€‚"
                            },
                            {
                                "type": "image_url",
                                "image_url": image_item["image_url"]
                            }
                        ]
                    }
                ],
                max_tokens=1000,
                temperature=0.1
            )
            
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.debug_log(1, f"âŒ å›¾åƒæè¿°ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"å›¾åƒæè¿°ç”Ÿæˆå¤±è´¥: {str(e)[:100]}"

    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)

    async def retrieve_similar_chunks(self, query_vector: List[float], chat_id: str) -> List[Tuple[str, float]]:
        """æ£€ç´¢ç›¸ä¼¼æ–‡æœ¬å—"""
        if not query_vector or chat_id not in self._vector_store:
            return []
        
        similarities = []
        chunks_data = self._vector_store[chat_id]
        
        for chunk_text, chunk_vector in chunks_data:
            similarity = self.cosine_similarity(query_vector, chunk_vector)
            similarities.append((chunk_text, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:self.valves.retrieval_top_k]

    async def rerank_documents(self, query: str, documents: List[str]) -> List[Tuple[str, float]]:
        """è¯­ä¹‰é‡æ’åº"""
        if not self.valves.enable_rerank or not self.valves.rerank_api_key:
            return [(doc, 1.0) for doc in documents]
        
        if not HTTPX_AVAILABLE:
            self.debug_log(1, "âŒ httpxåº“æœªå®‰è£…ï¼Œè·³è¿‡é‡æ’åº")
            return [(doc, 1.0) for doc in documents]
        
        try:
            headers = {
                "Authorization": f"Bearer {self.valves.rerank_api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.valves.rerank_model,
                "query": query,
                "documents": documents,
                "top_n": min(self.valves.rerank_top_n, len(documents)),
                "return_documents": True
            }
            
            async with httpx.AsyncClient(timeout=self.valves.request_timeout) as client:
                response = await client.post(
                    self.valves.rerank_api_url,
                    headers=headers,
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get("code") == 200:
                        reranked = []
                        for item in result["data"]["results"]:
                            doc_text = item["document"]["text"]
                            score = item["relevance_score"]
                            reranked.append((doc_text, score))
                        return reranked
                    else:
                        self.debug_log(1, f"âŒ é‡æ’åºAPIé”™è¯¯: {result.get('msg', 'Unknown error')}")
                else:
                    self.debug_log(1, f"âŒ é‡æ’åºHTTPé”™è¯¯: {response.status_code}")
        
        except Exception as e:
            self.debug_log(1, f"âŒ é‡æ’åºå¤±è´¥: {str(e)}")
        
        return [(doc, 1.0) for doc in documents]

    async def process_multimodal_content(self, messages: List[dict], model_name: str) -> List[dict]:
        """å¤„ç†å¤šæ¨¡æ€å†…å®¹"""
        if not self.valves.enable_multimodal_processing:
            return messages
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
        if self.has_native_vision(model_name):
            self.debug_log(2, f"ğŸ‘ï¸ æ¨¡å‹ {model_name} åŸç”Ÿæ”¯æŒè§†è§‰ï¼Œè·³è¿‡å¤šæ¨¡æ€å¤„ç†")
            return messages
        
        # æå–å›¾åƒ
        images = self.extract_images_from_messages(messages)
        if not images:
            return messages
        
        self.debug_log(1, f"ğŸ–¼ï¸ æ£€æµ‹åˆ° {len(images)} å¼ å›¾åƒï¼Œå¼€å§‹å¤„ç†...")
        
        # å¤„ç†æ¯å¼ å›¾åƒ
        processed_messages = []
        for i, message in enumerate(messages):
            processed_message = message.copy()
            content = message.get("content", [])
            
            if isinstance(content, list):
                new_content = []
                has_images = False
                
                for item in content:
                    if item.get("type") == "image_url":
                        has_images = True
                        # ç”Ÿæˆå›¾åƒæè¿°
                        try:
                            description = await self.describe_image(item)
                            new_content.append({
                                "type": "text",
                                "text": f"[å›¾åƒæè¿°]: {description}"
                            })
                            self.debug_log(2, f"ğŸ¨ å›¾åƒæè¿°ç”ŸæˆæˆåŠŸ: {description[:100]}...")
                        except Exception as e:
                            self.debug_log(1, f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {str(e)}")
                            new_content.append({
                                "type": "text", 
                                "text": f"[å›¾åƒå¤„ç†å¤±è´¥]: {str(e)[:100]}"
                            })
                    else:
                        new_content.append(item)
                
                if has_images:
                    processed_message["content"] = new_content
            
            processed_messages.append(processed_message)
        
        return processed_messages

    async def store_conversation_chunks(self, messages: List[dict], chat_id: str):
        """å­˜å‚¨å¯¹è¯å—åˆ°å‘é‡åº“"""
        if not self.valves.enable_vectorization:
            return
        
        chunks_data = []
        
        for message in messages:
            content = message.get("content", "")
            if isinstance(content, str) and content.strip():
                # åˆ†ç‰‡
                chunks = self.smart_chunk_text(
                    content,
                    self.valves.chunk_size,
                    self.valves.chunk_overlap
                )
                
                # å‘é‡åŒ–æ¯ä¸ªåˆ†ç‰‡
                for chunk in chunks:
                    if chunk.strip():
                        try:
                            vector = await self.vectorize_text(chunk)
                            if vector:
                                chunks_data.append((chunk, vector))
                        except Exception as e:
                            self.debug_log(2, f"âŒ åˆ†ç‰‡å‘é‡åŒ–å¤±è´¥: {str(e)}")
        
        # å­˜å‚¨åˆ°å‘é‡åº“
        if chunks_data:
            if chat_id not in self._vector_store:
                self._vector_store[chat_id] = []
            self._vector_store[chat_id].extend(chunks_data)
            
            # é™åˆ¶å­˜å‚¨å¤§å°ï¼ˆä¿ç•™æœ€æ–°çš„1000ä¸ªå—ï¼‰
            if len(self._vector_store[chat_id]) > 1000:
                self._vector_store[chat_id] = self._vector_store[chat_id][-1000:]
            
            self.debug_log(1, f"ğŸ“¦ å­˜å‚¨äº† {len(chunks_data)} ä¸ªæ–‡æœ¬å—åˆ°å‘é‡åº“")

    async def smart_context_compression(self, messages: List[dict], token_limit: int, chat_id: str) -> List[dict]:
        """æ™ºèƒ½ä¸Šä¸‹æ–‡å‹ç¼©"""
        current_tokens = self.count_messages_tokens(messages)
        
        if current_tokens <= token_limit:
            self.debug_log(2, f"âœ… å½“å‰tokenæ•° {current_tokens} æœªè¶…é™åˆ¶ {token_limit}")
            return messages
        
        self.debug_log(1, f"âš ï¸ éœ€è¦å‹ç¼©: {current_tokens} -> {token_limit} tokens")
        
        # åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå¯¹è¯æ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        conversation_messages = [msg for msg in messages if msg.get("role") != "system"]
        
        # ä¿ç•™æœ€åå‡ æ¡æ¶ˆæ¯
        preserve_count = self.valves.preserve_last_messages_count
        if len(conversation_messages) <= preserve_count:
            return messages
        
        preserved_messages = conversation_messages[-preserve_count:]
        to_compress_messages = conversation_messages[:-preserve_count]
        
        # è®¡ç®—ä¿ç•™ç©ºé—´
        system_tokens = self.count_messages_tokens(system_messages)
        preserved_tokens = self.count_messages_tokens(preserved_messages)
        available_tokens = token_limit - system_tokens - preserved_tokens
        
        if available_tokens <= 0:
            self.debug_log(1, "âš ï¸ ä¿ç•™æ¶ˆæ¯å·²è¶…é™ï¼Œä»…è¿”å›å¿…è¦å†…å®¹")
            return system_messages + preserved_messages
        
        # è®¡ç®—å‹ç¼©ç­–ç•¥
        preserve_ratio = self.valves.context_preserve_ratio
        preserve_tokens = int(available_tokens * preserve_ratio)
        retrieval_tokens = available_tokens - preserve_tokens
        
        # 1. ä¿ç•™éƒ¨åˆ†åŸå§‹æ¶ˆæ¯
        preserved_original = []
        current_preserve_tokens = 0
        
        for msg in reversed(to_compress_messages):
            msg_tokens = self.count_message_tokens(msg)
            if current_preserve_tokens + msg_tokens <= preserve_tokens:
                preserved_original.insert(0, msg)
                current_preserve_tokens += msg_tokens
            else:
                break
        
        # 2. ä¸ºå‰©ä½™æ¶ˆæ¯è¿›è¡Œå‘é‡æ£€ç´¢
        remaining_messages = to_compress_messages[:-len(preserved_original)] if preserved_original else to_compress_messages
        
        if remaining_messages and self.valves.enable_vectorization:
            # å­˜å‚¨åˆ°å‘é‡åº“
            await self.store_conversation_chunks(remaining_messages, chat_id)
            
            # æ„å»ºæŸ¥è¯¢ï¼ˆä½¿ç”¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
            query_text = ""
            for msg in reversed(conversation_messages):
                if msg.get("role") == "user":
                    content = msg.get("content", "")
                    if isinstance(content, str):
                        query_text = content
                    elif isinstance(content, list):
                        for item in content:
                            if item.get("type") == "text":
                                query_text = item.get("text", "")
                                break
                    break
            
            if query_text:
                # å‘é‡æ£€ç´¢
                query_vector = await self.vectorize_text(query_text)
                if query_vector:
                    similar_chunks = await self.retrieve_similar_chunks(query_vector, chat_id)
                    
                    if similar_chunks:
                        # é‡æ’åº
                        documents = [chunk for chunk, _ in similar_chunks]
                        reranked = await self.rerank_documents(query_text, documents)
                        
                        # æ„å»ºæ£€ç´¢å†…å®¹
                        retrieval_content = ""
                        current_retrieval_tokens = 0
                        
                        for doc, score in reranked:
                            doc_tokens = self.count_tokens(doc)
                            if current_retrieval_tokens + doc_tokens <= retrieval_tokens:
                                retrieval_content += f"\n[ç›¸å…³å†…å®¹ - ç›¸ä¼¼åº¦: {score:.3f}]\n{doc}\n"
                                current_retrieval_tokens += doc_tokens
                            else:
                                break
                        
                        if retrieval_content:
                            # æ·»åŠ æ£€ç´¢æ‘˜è¦åˆ°ç³»ç»Ÿæ¶ˆæ¯
                            retrieval_summary = {
                                "role": "system",
                                "content": f"=== ğŸ“š æ™ºèƒ½æ£€ç´¢å†…å®¹ ===\nåŸºäºå½“å‰æŸ¥è¯¢æ£€ç´¢åˆ°çš„ç›¸å…³å†å²å¯¹è¯å†…å®¹ï¼š\n{retrieval_content}"
                            }
                            system_messages.append(retrieval_summary)
                            
                            self.debug_log(1, f"ğŸ” æ·»åŠ äº† {current_retrieval_tokens} tokens çš„æ£€ç´¢å†…å®¹")
        
        # ç»„è£…æœ€ç»ˆç»“æœ
        final_messages = system_messages + preserved_original + preserved_messages
        final_tokens = self.count_messages_tokens(final_messages)
        
        self.debug_log(1, f"âœ… å‹ç¼©å®Œæˆ: {current_tokens} -> {final_tokens} tokens")
        return final_messages

    async def send_status(self, __event_emitter__, message: str, done: bool = True):
        """å‘é€çŠ¶æ€æ¶ˆæ¯"""
        if __event_emitter__ and self.valves.show_processing_status:
            try:
                await __event_emitter__({
                    "type": "status",
                    "data": {
                        "description": message,
                        "done": done,
                    },
                })
            except Exception as e:
                self.debug_log(2, f"âŒ çŠ¶æ€å‘é€å¤±è´¥: {str(e)}")

    def get_chat_id(self, __event_emitter__) -> str:
        """æå–èŠå¤©ID"""
        try:
            if hasattr(__event_emitter__, "__closure__") and __event_emitter__.__closure__:
                info = __event_emitter__.__closure__[0].cell_contents
                chat_id = info.get("chat_id")
                if chat_id:
                    return chat_id
        except:
            pass
        
        # ç”Ÿæˆfallback ID
        return f"chat_{hashlib.md5(str(datetime.now()).encode()).hexdigest()[:8]}"

    async def inlet(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __event_emitter__: Optional[Callable] = None,
    ) -> dict:
        """ä¸»è¦å¤„ç†é€»è¾‘"""
        if not self.toggle:
            return body
        
        messages = body.get("messages", [])
        model = body.get("model", "")
        
        if not messages:
            return body
        
        chat_id = self.get_chat_id(__event_emitter__)
        start_time = datetime.now()
        
        try:
            # è·å–æ¨¡å‹tokené™åˆ¶
            token_limit = self.get_model_token_limit(model)
            self.debug_log(1, f"ğŸ¯ æ¨¡å‹ {model} tokené™åˆ¶: {token_limit}")
            
            await self.send_status(__event_emitter__, f"ğŸ§  å¯åŠ¨æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç† (æ¨¡å‹: {model})", False)
            
            # 1. å¤šæ¨¡æ€å¤„ç†
            if self.valves.enable_multimodal_processing:
                await self.send_status(__event_emitter__, "ğŸ–¼ï¸ å¤„ç†å¤šæ¨¡æ€å†…å®¹...", False)
                messages = await self.process_multimodal_content(messages, model)
                body["messages"] = messages
            
            # 2. ä¸Šä¸‹æ–‡ç®¡ç†
            if self.valves.enable_context_management:
                await self.send_status(__event_emitter__, "ğŸ“Š æ™ºèƒ½ä¸Šä¸‹æ–‡å‹ç¼©...", False)
                messages = await self.smart_context_compression(messages, token_limit, chat_id)
                body["messages"] = messages
            
            # å¤„ç†ç»Ÿè®¡
            processing_time = (datetime.now() - start_time).total_seconds()
            final_tokens = self.count_messages_tokens(messages)
            
            self._processing_stats[chat_id] = {
                "last_processing_time": processing_time,
                "final_tokens": final_tokens,
                "timestamp": datetime.now()
            }
            
            await self.send_status(
                __event_emitter__, 
                f"âœ… å¤„ç†å®Œæˆ | ğŸ¯ {final_tokens} tokens | â±ï¸ {processing_time:.2f}s"
            )
            
            self.debug_log(1, f"âœ… å¤„ç†å®Œæˆ: {len(messages)} æ¡æ¶ˆæ¯, {final_tokens} tokens, è€—æ—¶ {processing_time:.2f}s")
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            await self.send_status(__event_emitter__, error_msg)
            self.debug_log(1, error_msg)
            
            if self.valves.debug_level >= 2:
                import traceback
                traceback.print_exc()
        
        return body

    async def outlet(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __event_emitter__: Optional[Callable] = None,
    ) -> dict:
        """è¾“å‡ºåå¤„ç†"""
        if not self.toggle:
            return body
        
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è¾“å‡ºå¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚æ·»åŠ æº¯æºæ ‡è®°ç­‰
        return body
